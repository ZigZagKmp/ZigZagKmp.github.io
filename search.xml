<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Algorithm Design 4</title>
    <url>/2023/10/09/Algorithm-Design-4/</url>
    <content><![CDATA[<h2 id="Chapter-2-Greedy-Algorithm"><a href="#Chapter-2-Greedy-Algorithm" class="headerlink" title="Chapter 2 Greedy Algorithm"></a>Chapter 2 Greedy Algorithm</h2><h3 id="2-7-Optimal-Caching"><a href="#2-7-Optimal-Caching" class="headerlink" title="2.7 Optimal Caching"></a>2.7 Optimal Caching</h3><ol>
<li><p>Description</p>
<ul>
<li><p>U : $n$ pieces of distinct elements stored in main memory</p>
</li>
<li><p>cache : can hold $k&lt;n$ pieces</p>
<p>init : cache holds $k$ elements </p>
</li>
<li><p>need to process a sequence of elements $d_1,\cdots,d_m$ ( $m$ can $\ge n$ , not distinct)</p>
</li>
<li><p>when processing $d_i$ : </p>
<ul>
<li>If $d_i$ already in cache : cache hit , do nothing to the cache</li>
<li>If $d_i$ not in cache : cache miss , evict some other element from the cache</li>
</ul>
</li>
<li><p>Goal : minimize # of cache miss .</p>
</li>
</ul>
</li>
<li><p>Greedy Algorithm</p>
<ol>
<li><p>FF Strategy : evict the element that will be used the furthest in the future .</p>
</li>
<li><p>Proof [Exchange Argument] :</p>
<p>See BOOK</p>
<blockquote>
<p>Suppose our solution $S_{FF}$ differs with the optimal solution $S$ firstly at $d_i$ , which is a cache miss , and our solution evicts $a$ and the optimal solution evicts $b$ , where the future used time : $t(a)&gt;t(b)$ . Let’s prove that we can construct a new solution $S’$ that is same with our solution at $d_i$ as well , and is not worse than the optimal solution $S$.</p>
<p>We do not need to care about other caches and other elements . We only need to care about $a,b$ . $S=S’$ except the following cases :</p>
<p>Case 0 : At $d_i$ , $S’$ should evict $a$ rather than $b$ </p>
<p>Case 1 : $d_j$ let $a$ evicted in $S$ before the first future used time $t(a)$ . Then let $S’$ evicts $b$ at the same time , $S$ is the same as $S’$ .</p>
<p>Case 2 : At $t(b)$ . If at this time , $S$ evicts $a$ , then $S’$ can do nothing and save one cache miss . If at this time , $S$ evicts $c\neq a$ , then we can let $S’$ evicts $c$ as well , and then let $a$ “waiting” until it is used at $t(a)$ or evicted later .</p>
</blockquote>
</li>
</ol>
</li>
<li><p>In reality : we cannot know the sequence ahead</p>
<p>LRU : least recently used </p>
<p>using locality of reference</p>
<p>[ Slater , Tarjan ] : LRU is the earliest online solution (elements come one by one)</p>
<p>$LRU\le k(FF+1)$ , $k$ is the cache size .</p>
</li>
</ol>
<h3 id="2-8-Minimum-Cost-Arborescence"><a href="#2-8-Minimum-Cost-Arborescence" class="headerlink" title="2.8 Minimum Cost Arborescence"></a>2.8 Minimum Cost Arborescence</h3><p>a.k.a 最小树形图</p>
<ol>
<li><p>Description</p>
<p>Find a min-cost directed spanning tree rooted at $r$ in a directed graph $G$ .</p>
<p>Assumption : $\forall e\in E,w(e)\ge 0$</p>
</li>
<li><p>Algorithm</p>
<ul>
<li><p>For $v\in V\backslash \{r\}$ , choose the in-edge with minimum weight $e_v$ .</p>
<p>Let $F^<em>$ be the set of chosen edge , so $cost(F^</em>)\le OPT$ .</p>
<p>If $F^*$ is already a tree , we get the optimal solution .</p>
</li>
<li><p>Otherwise , for all $v\in V\backslash\{r\}$ , let $w’(e=(u,v))=w(u,v)-w(e_v)$ , and then contract zero-cost cycles to get a new graph $G’$ .</p>
</li>
<li>Process $G’$ as above .</li>
</ul>
</li>
<li><p>Extension : matrix-tree theorem</p>
<p>count # of spanning tree in a graph (either directed or undirected)</p>
<p>$Det(L_{00})$</p>
<p>variant : count # of spanning tree in a graph with given total weight</p>
</li>
</ol>
<h2 id="Chapter-3-Dynamic-Programming"><a href="#Chapter-3-Dynamic-Programming" class="headerlink" title="Chapter 3 Dynamic Programming"></a>Chapter 3 Dynamic Programming</h2><h3 id="3-1-weighted-interval-scheduling-problem"><a href="#3-1-weighted-interval-scheduling-problem" class="headerlink" title="3.1 weighted interval scheduling problem"></a>3.1 weighted interval scheduling problem</h3><ol>
<li><p>Description</p>
<p>Input : $n$ intervals $[l_i,r_i]$ with weight $w_i$</p>
<p>Goal : maximize the total weight s.t. the chosen intervals are distinct </p>
</li>
<li><p>key : Dynamic Programming recursion</p>
<ol>
<li>sort the jobs according to $r_i$ (like greedy algorithm)</li>
<li>define $opt(j)$ : the optimal value for the first $j$ intervals </li>
</ol>
<script type="math/tex; mode=display">
opt(i)=\max\{opt(i-1),opt(p(i))+w_i\}</script><p>where $p(i)=\max\{j|r_j&lt;l_i\}$ .</p>
<ol>
<li><p>If we don’t use interval $i$ , it is $opt(i-1)$ .</p>
<p>If we use interval $i$ , since $\{r_i\}$ non-decreasing , $opt(p(i))$ contains all valid optimal solutions before .</p>
</li>
</ol>
</li>
<li><p>Implementation</p>
<p>naive :</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">compute_opt</span><span class="params">(<span class="type">int</span> i)</span></span>&#123;</span><br><span class="line">   <span class="keyword">if</span>(i==<span class="number">0</span>)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">else</span> <span class="keyword">return</span> <span class="built_in">max</span>(<span class="built_in">compute_opt</span>(i<span class="number">-1</span>),w[i]+<span class="built_in">compute_opt</span>(<span class="built_in">p</span>(i)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Problem : can be exponential time !</p>
<p>Key : many redundant computation $\to$ store them</p>
</blockquote>
<p>memorization / filling out the DP table : </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> opt[]=&#123;<span class="number">-1</span>&#125;;</span><br><span class="line"><span class="comment">// recursive implementation </span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">compute_opt</span><span class="params">(<span class="type">int</span> i)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(i==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(opt[i]!=<span class="number">-1</span>)<span class="keyword">return</span> opt[i];</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> opt[i]=<span class="built_in">max</span>(<span class="built_in">compute_opt</span>(i<span class="number">-1</span>),w[i]+<span class="built_in">compute_opt</span>(<span class="built_in">p</span>(i)));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// non-recursive implementation</span></span><br><span class="line">&#123;</span><br><span class="line">    opt[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;++i)&#123;</span><br><span class="line">        opt[i]=<span class="built_in">max</span>(opt[i<span class="number">-1</span>],w[i]+opt[<span class="built_in">p</span>(i)]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="3-2-Subset-Sum-Knapsack"><a href="#3-2-Subset-Sum-Knapsack" class="headerlink" title="3.2 Subset Sum / Knapsack"></a>3.2 Subset Sum / Knapsack</h3><ol>
<li><p>Description</p>
<p>Input : $n$ items , item $i$ has weight $w_i$ and value $v_i$ ; Weight limit $W$ </p>
<p>Goal : find a subset of items $S$ s.t.  $\sum_{i\in S}w_i\le W$ and maximize $\sum_{i\in S}v_i$ .</p>
</li>
<li><p>DP Algorithm</p>
<ol>
<li><p>pseudo-poly time / poly time</p>
<p>pseudo-poly time : $poly(n,W)$</p>
<p>poly time : $poly(n,\log W)$ ( $\log W$ is the least number of bits to input $W$ )</p>
<blockquote>
<p>Knapsack : NP-Hard : hard to find poly time algorithm</p>
<p>But finding a pseudo-poly time algorithm is easy !</p>
</blockquote>
</li>
<li><p>$opt(i,w)$ : the optimal solution for first $i$ items and weight $= w$ </p>
<script type="math/tex; mode=display">
opt(i,w)=\max\begin{cases}opt(i-1,w)&\text{not using i-th item}\\
opt(i-1,w-w_i)+v_i&\text{using i-th item}
\end{cases}</script></li>
<li><p>Time Complexity : $\mathcal O(nW)$ .</p>
</li>
</ol>
</li>
<li><p>Implementation</p>
<p>$opt(w)$ : at $i$-th iteration : the optimal solution for first $i$ items and weight $= w$ </p>
<script type="math/tex; mode=display">
opt(w)=\max\{opt(w),opt(w-w_i)+v_i\}</script><p>iterate $w$ decreasingly </p>
<p><img src="\images\posts\AD4_fig1.jpg" alt=""></p>
<p>Memory Complexity : $\mathcal O(n+W)$ </p>
</li>
</ol>
<h3 id="3-3-RNA-secondary-structure"><a href="#3-3-RNA-secondary-structure" class="headerlink" title="3.3 RNA secondary structure"></a>3.3 RNA secondary structure</h3><ol>
<li><p>Description</p>
<p>Input : $\{A,C,G,U\}^n$</p>
<p>Constraints :</p>
<ul>
<li>no sharp turns : If pairing $(i,j)\in S$ , then $i&lt;j-4$</li>
<li>pairing : $A-G$ , $C-U$</li>
<li>one base can only appear in $\le 1$ pairs</li>
<li>no crossing pairing : $\not\exist (i,j),(l,r)\in S,i&lt;l&lt;j&lt;r$ </li>
</ul>
<p>Goal : maximize the size of pairing set $S$ </p>
</li>
<li><p>Algorithm</p>
<p>$opt(l,r)$ : the maximum number of pairs in subsequence $[l,r]$ </p>
<script type="math/tex; mode=display">
opt(l,r)=\max\begin{cases}
opt(l+1,r-1)&a[l]\text{ and }a[r] \text{ can pairing}\\
\max_{l\le i\le r-1}\{opt(l,i)+opt(i+1,r)\} 
\end{cases}</script><p>Order : $r-l$ increasing</p>
<p>Time Complexity : $\mathcal O(n^3)$</p>
<p>Memory Complexity : $\mathcal O(n^2)$</p>
</li>
</ol>
<h3 id="3-4-DP-on-tree-and-tree-like-graph"><a href="#3-4-DP-on-tree-and-tree-like-graph" class="headerlink" title="3.4 DP on tree and tree-like graph"></a>3.4 DP on tree and tree-like graph</h3><h4 id="3-4-1-maximum-independent-set-on-tree"><a href="#3-4-1-maximum-independent-set-on-tree" class="headerlink" title="3.4.1 maximum independent set on tree"></a>3.4.1 maximum independent set on tree</h4><ol>
<li><p>Description</p>
<p>Input : A tree , vertex-weighted </p>
<p>Goal : Find an Independent Set of maximum weight</p>
</li>
<li><p>Algorithm</p>
<p>$opt_{0/1}(x)$ : the maximum-weighted independent set of the subtree $x$ , $opt_1$ means we choose $x$ , $opt_0$ means we don’t choose $x$ .</p>
<script type="math/tex; mode=display">
opt_1(x)=w_x+\sum_{y\in child(x)}opt_0(y)\\
opt_0(x)=\sum_{y\in child(x)}\max\{opt_0(y),opt_1(y)\}</script></li>
<li><p>Tree DP View : </p>
<ul>
<li>subtree is the natural subinstance</li>
<li>delete $x$ , the tree will be separated into several parts , no connection between $subtree(y)$ ($y\in child(x)$) , $G\backslash subtree(x)$ </li>
</ul>
</li>
</ol>
<h3 id="3-4-2-treewidth"><a href="#3-4-2-treewidth" class="headerlink" title="3.4.2 treewidth"></a>3.4.2 treewidth</h3><ol>
<li><p>Tree Decomposition ( clique tree , junction trees )</p>
<ol>
<li><p>Def [separator] : For connected graph $G$ , a separator is a vertex set $S\subset V$ , s.t. $G\backslash S$ is not connected . </p>
</li>
<li><p>Def [tree decomposition] : $T(G)$ is a tree decomposition if :</p>
<p>Each vertex in $T(G)$ is called a bag $(V_T(x),E_T(x))$ : containing some vertices in $G$ and their edges</p>
<ul>
<li>$\cup_x V_T(x)=V$</li>
<li>$\forall e\in E,\exists x\in T(G) , e\in E_T(x)$</li>
<li>$\forall v\in V,\{x|v\in V_T(x)\}$ form a connected subtree</li>
</ul>
</li>
<li><p>Def [treewidth for tree decomposition] : $tw(T(G)):=\max_x \{|V_T(x)|\}-1$</p>
</li>
<li><p>Lemma : If $G$ is a tree , $tw(T(G))=1$</p>
</li>
<li><p>Def [treewidth for graph] : </p>
<script type="math/tex; mode=display">
tw(G)=\min\limits_{T(G)\text{ is a tree decomposition}}tw(T(G))</script></li>
</ol>
</li>
<li><p>Lemma [ bags and separators ] : $T(G)$ is a tree decomposition of $G$ , $A,B$ are adjacent bags in $T(G)$ , then </p>
<p>$V_T(A)$ is a separator of $G$ , $V_T(A\cap B)$ is a separator of $G$</p>
</li>
<li><p>Independent Set on a graph with a tree decomposition</p>
<p>Input : given $G$ and a tree decomposition $T(G)$</p>
<p>Goal : Find maximum Independent Set in Time Complexity $\mathcal O(n2^{tw(T(G))})$</p>
</li>
<li><p>Algorithm</p>
<ul>
<li><p>$opt(x,S)$ : consider the subtree $x$ in the tree decomposition , choosing exactly $S$ in $V_T(x)$ , the maximum independent set </p>
</li>
<li><script type="math/tex; mode=display">
opt(x,S)=w(S)+\sum_{y\in child(x)}\max_{S' , S\text{ consists with } S'\text{ on }S\cap S'}\{opt(y,S')-w(S\cap S')\}</script></li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-贪心</tag>
        <tag>算法-贪心-最优缓存</tag>
        <tag>算法-图论-最小树形图</tag>
        <tag>算法-动态规划</tag>
        <tag>算法-动态规划-线性DP</tag>
        <tag>算法-动态规划-背包</tag>
        <tag>算法-动态规划-区间DP</tag>
        <tag>算法-动态规划-树形DP</tag>
        <tag>算法-图论-树分解</tag>
      </tags>
  </entry>
  <entry>
    <title>Probability and Statistics 2</title>
    <url>/2023/10/08/Probability-and-Statistics-2/</url>
    <content><![CDATA[<h2 id="Chapter-1-Background-in-Probability"><a href="#Chapter-1-Background-in-Probability" class="headerlink" title="Chapter 1 Background in Probability"></a>Chapter 1 Background in Probability</h2><h3 id="1-2-Random-Variables"><a href="#1-2-Random-Variables" class="headerlink" title="1.2 Random Variables"></a>1.2 Random Variables</h3><ol>
<li><p>measurable map</p>
<ol>
<li><p>Remark of Thm [a sufficient condition for measurable map]</p>
<blockquote>
<p>If $\mathcal S$ is a $\sigma$-field , then $\{X^{-1}(B)|B\in \mathcal S\}$ is a $\sigma$-field</p>
<p>Def : [generation of measurable map] $\sigma(X)$ is the $\sigma$-field generated by $X$ </p>
<script type="math/tex; mode=display">
\sigma(X)=\{X^{-1}(B)|B\in \mathcal S\}</script><p>$\sigma(X)$ is the smallest $\sigma$-field on $\Omega$ that makes $X$ a measurable map to $(S,\mathcal S)$ .</p>
</blockquote>
</li>
<li><p>Thm [measurable map composition] : If $X:(\Omega,\mathcal F)\to(S,\mathcal S)$ , $f:(S,\mathcal S)\to (T,\mathcal T)$ are both measurable maps , then $f\circ X$ is a measurable map $(\Omega,\mathcal S)\to(T,\mathcal T)$ .</p>
<blockquote>
<p>Proof : $(f\circ X)^{-1}(B)=X^{-1}(f^{-1}(B))$ , $C:=f^{-1}(B)\in \mathcal S$ , $X^{-1}(C)\in \mathcal F$ </p>
</blockquote>
</li>
<li><p>Cor : If $X_1,\cdots,X_n$ are random variables , $f:(\mathbb R^n,\mathcal R^n)\to(\mathbb R,\mathcal R)$ is measurable , then $f(X_1,\cdots,X_n)$ is a random variable .</p>
</li>
<li><p>(*) Cor : $X_1,\cdots,X_n$ are random variables , then $X_1+\cdots+X_n$ is a random variable .</p>
</li>
<li><p>(*) Thm : $X_1,\cdots$ are random variables , then $\inf_n X_n$ , $\sup_n X_n$ , $\liminf_n X_n$ , $\limsup_n X_n$ are random variables</p>
<blockquote>
<p>Proof : $\{\inf_n X_n&lt;a\}=\cup_n\{X_n&lt;a\}\in\mathcal F$ .</p>
<script type="math/tex; mode=display">
\liminf_{n\to \infty}X_n=\sup_n\left(\inf_{m\ge n}X_m\right)</script><p>$Y_n=\inf_{m\ge n}X_m$ is a random variable .</p>
</blockquote>
</li>
</ol>
</li>
<li><p>(*) Generalization of random variable </p>
<ol>
<li><p>Def : [Converges almost surely] </p>
<p>From Thm above , the following set is a measurable set</p>
<script type="math/tex; mode=display">
\Omega_o:=\{w:\lim_{n\to\infty}X_n\text{ exists}\}=\{w:\limsup_{n\to\infty}X_n-\liminf_{n\to\infty}X_n=0\}</script><p>If $P(\Omega_o)=1$ , then $X_n$ converges almost surely ( a.s. ) .</p>
<blockquote>
<p>即一列 $X_n$ 是几乎处处收敛的 当且仅当 $X_n$ 不收敛的点集测度为 $0$ </p>
</blockquote>
<p>Def : $X_\infty:=\limsup_{n\to \infty} X_n$ </p>
<p>Problem : $X_{\infty}$ can have value $\pm\infty$ .</p>
</li>
<li><p>Def : [Generalized random variable] </p>
<p>A function whose domain is $D\in \mathcal F$ and range is $\mathbb R^<em>:=[-\infty,+\infty]$ is a random variable if $\forall B\in \mathcal R^</em> , X^{-1}(B)\in \mathcal F$ .</p>
<p>Def : [Extended Borel set ] : $\mathcal R^*$ is generated by intervals of form $[-\infty,a),(a,b),(b,+\infty]$ , $a,b\in \mathbb R$ .</p>
<blockquote>
<p>extended real line $(\mathbb R^<em>,\mathcal R^</em>)$ is a measurable space </p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h3 id="1-3-Distribution"><a href="#1-3-Distribution" class="headerlink" title="1.3 Distribution"></a>1.3 Distribution</h3><ol>
<li><p>Definition of Distribution</p>
<ol>
<li><p>Def : [distribution] $X$ is a random variable , then let $\mu=P\circ X^{-1}$ ( $\mu(A)=P(X\in A)$ ) , $\mu$ is a probability measure called distribution .</p>
</li>
<li><p>Check $\mu$ is a probability measure $(\mathbb R,\mathcal R)\to (\mathbb R,\mathcal R)$ .</p>
<ul>
<li><p>$\mu(A)=P(\{w\in \Omega:X(w)\in A\})\ge 0$ </p>
</li>
<li><p>If $A_i\in \mathcal R$ are disjoint countable sequence , then $X^{-1}(A_i)$ are also disjoint </p>
<script type="math/tex; mode=display">
\begin{aligned}
\mu(\cup_i A_i)&=P(X^{-1}(\cup_i A_i))\\
&=P(\cup_i X^{-1}(A_i))\\
&=\sum_{i} P(X^{-1}(A_i))\\
&=\sum_{i} \mu(A_i)
\end{aligned}</script></li>
</ul>
</li>
</ol>
</li>
<li><p>distribution function</p>
<ol>
<li><p>Def : [distribution function]  Let $F(x)=P(X\le x)$ , $F(x)$ is the distribution function .</p>
<blockquote>
<p>$F(x)=P(X\le x)$ : Let $A_x=(-\infty,x]$ , $F(x)=P(X^{-1}(A_x))=\mu(A_x)$</p>
<p>$F(x)$ can be viewed as the CDF of $\mu$ / the Stieltjes measure function </p>
</blockquote>
</li>
<li><p>Thm [Props of distribution function] : Let $F$ be a distribution function</p>
<ul>
<li><p>$F$ : non-decreasing</p>
</li>
<li><p>$\lim\limits_{x\to -\infty }F(x)=0$ , $\lim\limits_{x\to+\infty}F(x)=1$</p>
</li>
</ul>
<blockquote>
<p>$x\to -\infty , \{X\le x\}\downarrow \varnothing$ , $x\to+\infty , \{X\le x\}\uparrow \Omega$</p>
</blockquote>
<ul>
<li>$F$ : right continuous , $F(x^+)=\lim_{y\downarrow x}F(y)=F(x)$</li>
</ul>
<blockquote>
<p>$y=x+\epsilon$ , $\{X\le y\}=\{X\le x+\epsilon\}\downarrow \{X\le x\}$</p>
</blockquote>
<ul>
<li>$F(x^-)=P(X&lt;x)$</li>
</ul>
<blockquote>
<p>$y=x-\epsilon$ , $\{X\le y\}=\{X\le x-\epsilon\}\uparrow \{X&lt;x\}$</p>
</blockquote>
<ul>
<li>$P(X=x)=F(x)-F(x^-)$</li>
</ul>
</li>
<li><p>Thm [Judgement of distribution function] : $F$ satisfies (i) , (ii) , (iii) is a distribution function</p>
<blockquote>
<p>Proof : [construction]</p>
<p>Let $\Omega=(0,1)$ , $\mathcal F$ is the corresponding Borel set , $P$ is Lebesgue measure , so $P((a,b])=b-a$ .</p>
<p>Let $X(w)=\sup\{y:F(y)&lt;w\}$ , we want to prove that $P(X\le x)=F(x)$ .</p>
<p>Since $F(x)=P(\{w:w\le F(x)\})$ , we want to prove that :</p>
<script type="math/tex; mode=display">
\{w:X(w)\le x\}=\{w:w\le F(x)\} \quad\quad \quad (*)</script><p>$R\subset L$ : $\forall w,w\le F(x)$ , and $X(w)=\sup\{y:F(y)&lt;w\}$ , so $F(X(w))\le w\le F(x)$ , so $X(w)\le x$ .</p>
<p>$R^c\subset L^c$ : $\forall w,w&gt;F(x)$ , and $X(w)=\sup\{y:F(y)&lt;w\}$ . </p>
<p>$F$ : right continuous , so $\exists \epsilon&gt;0$ , $F(x+\epsilon)<w$ , so $X(w)\ge x+\epsilon>x$ .</p>
</blockquote>
<p>Equation (*) means :</p>
<p><img src="/images/posts/PS2_fig1.png" alt=""></p>
</li>
<li><p>Remark </p>
<p>Each distribution function $F$ corresponds to a unique distribution measure $\mu$ </p>
<p>One distribution function $F$ can correspond to many different random variables</p>
<p>Def [equal in distribution] : If $X,Y$ have same distribution measure/function , then $X$ and $Y$ are equal in distribution , denote as $X\overset{d}{=}Y$ or $X=_d Y$ .</p>
</li>
</ol>
</li>
<li><p>Density function</p>
<ol>
<li><p>Def [density function] : when a distribution function $F(x)=P(X\le x)$ has the form </p>
<script type="math/tex; mode=display">
F(x)=\int_{-\infty}^x f(y)\ dy</script><p>then $X$ has the density function $f$ , denote as $f_X(x)$ .</p>
<blockquote>
<script type="math/tex; mode=display">
P(X=x)=\lim_{\epsilon\to0}\int_{x-\epsilon}^{x+\epsilon} f(y)dy=0</script></blockquote>
</li>
<li><p>Prop : (necessary and sufficient)</p>
<ul>
<li>$f(x)\ge 0$ </li>
<li>$\int_{-\infty}^{+\infty}f(x)dx=1$</li>
</ul>
</li>
</ol>
</li>
<li><p>Discrete / Continuous</p>
<p>A probability measure $P$ is discrete if there exists a countable set $S$ that $P(S^c)=0$ ( only non-zero on countable set) .</p>
<p>Discrete : usually $[a_i,b_i)$ segments like .</p>
</li>
</ol>
<h3 id="1-4-Integration"><a href="#1-4-Integration" class="headerlink" title="1.4 Integration"></a>1.4 Integration</h3><p>Intuition : Expectation needs Integration .</p>
<ol>
<li><p>Notations </p>
<ol>
<li><p>Def : $(\Omega,\mathcal F)$ , with measure $\mu$ , $f:(\Omega,\mathcal F)\to(\mathbb R,\mathcal R)$ . Denote the integration as $\int fd\mu$ .</p>
</li>
<li><p>Restriction for $\mu$ : should be $\sigma$-finite measure</p>
<p>e.g. Lebesgue measure is $\sigma$-finite : $A_i=[-i,i]$ , so $\mu(A_i)&lt;\infty$ and $\cup_i{A_i}=\mathbb R$ .</p>
</li>
<li><p>Restriction for $\int fd\mu$ :</p>
<p>(i) $\varphi\ge 0$ $\mu$-a.e. , then $\int\varphi d\mu\ge 0$</p>
<p>Def [almost everywhere] : $\mu$-a.e. : $\mu(\{w:\varphi(w)&lt;0\})=0$ .</p>
<p>(ii) $\int a\varphi d\mu=a\int \varphi d\mu$</p>
<p>(iii) $\int (\varphi+\psi)d\mu=\int \varphi d\mu+\int \psi d\mu$</p>
<p>(iv) $\varphi\le \psi$ $\mu$-a.e. , then $\int \varphi d\mu\le \int \psi d\mu$</p>
<p>(v) $\varphi=\psi$ $\mu$-a.e. , then $\int \varphi d\mu=\int \psi d\mu$</p>
<p>(vi) $\left|\int \varphi d\mu\right|\le \int |\varphi|d\mu$ </p>
</li>
<li><p>Thm : (i),(ii),(iii) can derive (iv),(v),(vi)</p>
</li>
</ol>
</li>
<li><p>Simple Function</p>
<ol>
<li><p>Def [simple function] : If $\varphi(\omega)=\sum_{i=1}^n a_i \mathbb 1_{A_i}$ , and $A_i$ are disjoint sets , $\mu(A_i)&lt;\infty$ </p>
</li>
<li><p>Def [simple function integration] : If $\varphi(\omega)=\sum_{i=1}^n a_i \mathbb 1_{A_i}$ , define </p>
<script type="math/tex; mode=display">
\int \varphi d\mu=\sum_{i=1}^n a_i \mu(A_i)</script></li>
<li><p>Check Props</p>
<blockquote>
<p>(i) : $\varphi\ge 0$ $\mu$-a.e. , so for all $A_i$ with $\mu(A_i)&gt;0$ , $a_i\ge 0$ .</p>
<p>(ii) : trivial</p>
<p>(iii) : Suppose $\varphi=\sum_{i=1}^m a_i \mathbb 1_{A_i}$ , $\psi=\sum_{j=1}^n b_j \mathbb 1_{B_j}$ </p>
<p>Define $A_0=\cup_{j} B_j-\cup_i A_i$ , $B_0=\cup_i A_i-\cup_j B_j$ . Let $a_0=b_0=0$ .</p>
<p>Therefore $\cup_{j=1}^n B_j\subset \cup_{i=0}^n A_i$ and $\cup_{i=1}^m A_i\subset \cup_{j=1}^n B_j$ .</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int(\varphi+\psi)d\mu&=\sum_{i=0}^m \sum_{j=0}^n (a_i+b_j)\mu(A_i\cap B_j)\\
&=\sum_{i=0}^m a_i\sum_{j=0}^n \mu(A_i\cap B_j)+\sum_{j=0}^nb_j\sum_{i=0}^m \mu(A_i\cup B_j)\\
&=\sum_{i=0}^m a_i\mu(A_i)+\sum_{j=0}^n b_j\mu(B_j)\\
&=\int \varphi d\mu +\int \psi d\mu
\end{aligned}</script></blockquote>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>概率与统计</category>
      </categories>
      <tags>
        <tag>实分析-测度-可测映射</tag>
        <tag>概率论-随机变量</tag>
        <tag>概率论-分布函数</tag>
        <tag>概率论-概率密度函数</tag>
        <tag>实分析-Lebesgue积分</tag>
      </tags>
  </entry>
  <entry>
    <title>Probability and Statistics 1</title>
    <url>/2023/10/08/Probability-and-Statistics-1/</url>
    <content><![CDATA[<h2 id="Chapter-0"><a href="#Chapter-0" class="headerlink" title="Chapter 0"></a>Chapter 0</h2><ol>
<li><p>Categories</p>
<ol>
<li><p>Background in Probability</p>
<ul>
<li><p>what is probability $\to$ measure theory</p>
</li>
<li><p>what is integration $\to $ Riemann / Lebesgue Integration</p>
</li>
<li><p>Expectation &amp; its properties</p>
</li>
</ul>
</li>
<li><p>Probability foundations of Asymptotic Statistics</p>
<ul>
<li>weak law of large numbers</li>
<li>strong law of large numbers (proof by Kolmoguor)</li>
<li>central limit theorem </li>
<li>characteristic function</li>
</ul>
</li>
<li><p>Estimation inference &amp; testing</p>
<ul>
<li>hypothesis testing</li>
<li>regression analysis</li>
<li>frontiers of statistical research (e.g. distribution of free test)</li>
</ul>
</li>
</ol>
</li>
<li><p>Textbook</p>
<p>[Durrett] Probability Theory &amp; Examples (or PTE)</p>
</li>
</ol>
<h2 id="Chapter-1-Background-in-Probability"><a href="#Chapter-1-Background-in-Probability" class="headerlink" title="Chapter 1 Background in Probability"></a>Chapter 1 Background in Probability</h2><h3 id="1-1-Probability-Space"><a href="#1-1-Probability-Space" class="headerlink" title="1.1 Probability Space"></a>1.1 Probability Space</h3><ol>
<li><p>Probability Space</p>
<ol>
<li><p>Def : $(\Omega,\mathcal F,P)$</p>
<p>$\Omega$ : set of “outcomes”</p>
<p>$\mathcal F$ : set of “events” , like subset of $\Omega$</p>
<p>$P$ : function : $\mathcal F\to [0,1]$</p>
</li>
<li><p>$\mathcal F$ should be a $\sigma$-field</p>
<ol>
<li><p>Def : [$\sigma$-field] A nonempty collection of subsets of $\Omega$ that</p>
<ul>
<li>补封闭：If $A\in \mathcal F$ , then $A^c\in \mathcal F$</li>
<li>可数无穷并封闭：If $A_i\in \mathcal F$ , $A_i$ is a countable sequence , then $\cup_i A_i\in \mathcal F$</li>
</ul>
</li>
<li><p>Prop : </p>
<ul>
<li>$\varnothing \in \mathcal F,\Omega\in \mathcal F$</li>
</ul>
<blockquote>
<p>Proof : $A\in\mathcal F$ $\to $ $A^c\in \mathcal F$ $\to$ $A\cup A^c\in \mathcal F$ $\to$ $\Omega\in \mathcal F$ $\to$ $\varnothing\in \mathcal F$ </p>
</blockquote>
<ul>
<li>可数无穷交封闭：If $A_i\in \mathcal F$ , , $A_i$ is a countable sequence , then $\cap_i A_i\in \mathcal F$</li>
</ul>
<blockquote>
<p>Proof : $A\cap B=(A^c\cup B^c)^c$</p>
</blockquote>
</li>
</ol>
</li>
</ol>
</li>
<li><p>Measurable Space</p>
<ol>
<li><p>Def : [measure] : non-negative , countably additive , set function</p>
<p>A function $\mu:\mathcal F\to \mathbb R$ with :</p>
<ul>
<li><p>$\forall A\in \mathcal F$ , $\mu(A)\ge \mu(\varnothing)=0$</p>
</li>
<li><p>If $A_i\in \mathcal F$ , $A_i$ countable <strong>disjoint</strong> sequence , then</p>
<script type="math/tex; mode=display">
\mu(\cup_i A_i)=\sum_{i}\mu(A_i)</script></li>
</ul>
</li>
<li><p>Def : [probability measure] : a measure $\mu$ with $\mu(\Omega)=1$</p>
</li>
<li><p>Thm : a measure $\mu$ on $(\Omega,\mathcal F)$ satisfies</p>
<ul>
<li><p>monotonicity : $A\subset B\Rightarrow \mu(A)\le \mu(B)$</p>
</li>
<li><p>subadditivity : $A\subset \cup_{i=1}^{\infty} A_i\Rightarrow \mu(A)\le \sum_{i=1}^{\infty}A_i$</p>
</li>
<li><p>continuity : </p>
<blockquote>
<p>Def : [$A_i\uparrow A$ ]</p>
<p>For set $A$ : $A_1\subset A_2\subset\cdots , \cup_i A_i=A$ </p>
<p>For real number $A$ : $A_1\le A_2\le \cdots,\lim_{n\to\infty}A_n=A$</p>
</blockquote>
<p>If $A_i\uparrow A$ , then $\mu(A_i)\uparrow \mu(A)$</p>
<p>If $A_i\downarrow A$ , then $\mu(A_i)\downarrow \mu(A)$</p>
</li>
</ul>
<blockquote>
<p>Proof :</p>
<p>(i) : Let $B-A=B\cap A^c$ , so if $A\subset B$ , then $B=A+(B-A)$ , and $A,B-A$ are disjoint</p>
<script type="math/tex; mode=display">
\mu(B)=\mu(A+(B-A))=\mu(A)+\mu(B-A)\ge \mu(A)</script><p>(ii) : Let $A_n’:=A_n\cap A$ , so $A=\cup_{i=1}^{\infty} A’_i$ . Let $B_n=\begin{cases}A_1’&amp;n=1\\A_n’-\cup_{i=1}^{n-1} A_i’&amp;n\ge 2\end{cases}$</p>
<p>Therefore , $B_n$ are disjoint , and $\cup_{i=1}^{\infty} B_i=\cup_{i=1}^{\infty} A_i’=A$</p>
<script type="math/tex; mode=display">
\mu(A)=\mu(\cup_{i} B_i)=\sum_{i}\mu(B_i)\le \sum_{i} \mu(A_i)</script><p>(iii) : Let $B_n=A_n-A_{n-1}$ , so $B_n$ are disjoint , $\cup_{i=1}^{n} B_i=A_n$ , $\cup_{i=1}^{\infty}B_i=A$</p>
<script type="math/tex; mode=display">
\mu(A)=\sum_{i=1}^{\infty} \mu(B_i)=\lim_{n\to\infty}\sum_{i=1}^n \mu(B_i)=\lim_{n\to \infty}\mu(A_n)</script></blockquote>
</li>
<li><p>E.g.1 Discrete Probability Space</p>
<p>$\Omega$ : countable set , $\mathcal F$ : the set of all subsets of $\Omega$ , $p: \Omega\to[0,1]$ , where $\sum_{\omega\in \Omega}p(\omega)=1$  .</p>
<script type="math/tex; mode=display">
P(A):=\sum_{\omega\in A}p(\omega)</script></li>
</ol>
</li>
<li><p>Measure on real line</p>
<ol>
<li><p>Def : [generate] $\mathcal A$ is a set of some subsets of $\Omega$. A $\sigma$-field is generated by $\mathcal A$ if it is the smallest $\sigma$-field containing $\mathcal A$ :</p>
<script type="math/tex; mode=display">
\sigma(\mathcal A):=\bigcap_{\mathcal A\subset\mathcal F,\mathcal F\text{ is }\sigma\text{-field}}\mathcal F</script></li>
<li><p>Def : [Borel Set]</p>
<p>Let $\mathcal A$ be the open subsets of $\mathbb R^d$ , Borel set is $\sigma(\mathcal A)$ , denoted as $\mathcal R^d$ .</p>
</li>
<li><p>measure for $d=1$</p>
<ol>
<li><p>Def : [Stieltjes measure function] $F:\mathbb R\to\mathbb R$ satisfies :</p>
<ul>
<li>non-decreasing : $\forall x\ge y , F(x)\ge F(y)$</li>
<li>right-continuous : $\lim_{y\downarrow x}F(y)=\lim_{y\to x^+}F(y)=F(x)$</li>
</ul>
</li>
<li><p>Thm : For all Stieltjes measure function $F$ , there is a unique measure $\mu$ on $(\mathbb R,\mathcal R)$ , with</p>
<script type="math/tex; mode=display">
\mu((a,b])=F(b)-F(a)</script></li>
<li><blockquote>
<p>When $F(x)=x$ , $\mu$ is Lebesgue measure</p>
<p>right-continuous : If $b_n\downarrow b$ , then $\cup_{n}(a,b_n]=(a,b_n]$ （可以保持右闭）</p>
</blockquote>
</li>
<li><p>Def [CDF] : For probability measure : $\lim\limits_{x\to -\infty}F(x)=0,\lim\limits_{x\to+\infty}F(x)=1$</p>
<p>$F$ : Cumulative Distribution Function [CDF] .</p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>(*)  semi-algebra , algebra , $\sigma$-algebra</p>
<ol>
<li>Def : [semi-algebra , algebra , $\sigma$-algebra]</li>
</ol>
</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">structure</th>
<th style="text-align:center">complement</th>
<th style="text-align:center">intersection/union</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">semi-algebra</td>
<td style="text-align:center">$S^c$ is a finite , disjoint union of sets in $\mathcal S$</td>
<td style="text-align:center">$S,T\in \mathcal S$ , then $S\cap T\in\mathcal S$</td>
</tr>
<tr>
<td style="text-align:center">algebra</td>
<td style="text-align:center">$A\in \mathcal A$ , then $A^c\in \mathcal A$</td>
<td style="text-align:center">$S,T\in \mathcal A$ , then $S\cap T,S\cup T\in \mathcal A$</td>
</tr>
<tr>
<td style="text-align:center">$\sigma$-algebra</td>
<td style="text-align:center">$A\in \mathcal F$ , then $A^c\in \mathcal F$</td>
<td style="text-align:center">$A_i\in \mathcal F$ , countable sequence , then $\cup_i A_i\in \mathcal F$</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><p>E.g. [algebra but not $\sigma$-algebra]</p>
<p>$\Omega=\mathbb Z$ , $\mathcal A=\{A\subset \Omega | A\text{ or }A^c\text{ is finite}\}$ </p>
<p>$\mathcal A$ is obviously algebra but not $\sigma$-algebra</p>
</li>
<li><p>Lemma</p>
<p>If $\mathcal S$ is a semi-algebra , then $\bar{\mathcal S}=\{\text{finite disjoint union of sets in }\mathcal S\}$ is algebra .</p>
<p>$\bar{\mathcal S}$ is called the algebra generated by $\mathcal S$ .</p>
<blockquote>
<p>Proof : easy to check two properties of algebra</p>
<p>Question : Is this generation the smallest generation ? </p>
</blockquote>
</li>
<li><p>Def : [measure for algebra] a measure $\mu$ on an algebra $\mathcal A$ satisfies :</p>
<ul>
<li><p>$\forall A\in \mathcal A , \mu(A)\ge \mu(\varnothing)=0$</p>
</li>
<li><p>If $A_i\in \mathcal A$ is a disjoint sequence , and $\cup_i A_i\in \mathcal A$ , then</p>
<script type="math/tex; mode=display">
\mu(\cup_i A_i)=\sum_{i}\mu(A_i)</script></li>
</ul>
<p>Def : [$\sigma$-finite] If there exists a sequence of sets $A_n\in \mathcal A $ , $\mu(A_n)&lt;\infty $ , $\cup_n A_n=\Omega$ .</p>
<blockquote>
<p>We can let $A_n’=\cup_{i=1}^n A_i$ , then $A_n’\uparrow \Omega$ .</p>
<p>We can let $A_n’=A_n\cap(\cap_{i=1}^{n-1}A_i^c)$ , then $A_n’$ are disjoint .</p>
<p>即，在构造这样的 $A_n$ 的时候，我们可以直接考虑 $A_n\uparrow \Omega$ 或 $A_n$ 不交</p>
</blockquote>
</li>
<li><p>Thm : $\mathcal S$ is a semi-algebra , $\mu$ defined on $\mathcal S$ with $\mu(\varnothing)=0$</p>
<p>(i) . If $\mu$ satisfies :</p>
<ul>
<li>If $S\in \mathcal S$ is a finite disjoint union of sets $S_i\in \mathcal S$ , then $\mu(S)=\sum_{i}\mu(S_i)$ </li>
<li>If $S_i,S\in \mathcal S$ , $S=+_{i\ge 1} S_i$ , then $\mu(S)\le \sum_{i\ge 1} \mu(S_i)$</li>
</ul>
<p>Then $\mu$ has a unique extension $\bar \mu$ that is a measure on $\bar{\mathcal S}$ .</p>
<p>(ii) . If $\bar\mu$ is $\sigma$-finite , then there is a unique extension $\hat \mu$ that is a measure on $\sigma(\mathcal S)$ .</p>
</li>
<li><p>Lemma : If $\mathcal S$ is a semi-algebra ,  $\mu$ defined on $\mathcal S$ with $\mu(\varnothing)=0$ . If  $S\in \mathcal S$ is a finite disjoint union of sets $S_i\in \mathcal S$ , then $\mu(S)=\sum_{i}\mu(S_i)$ . Then ,</p>
<ul>
<li>If $A,B_i\in \bar{\mathcal S}$ , $A=+_{i=1}^n B_i$ , then $\bar \mu(A)=\sum_{i=1}^n \bar\mu(B_i)$ </li>
<li>If $A,B_i\in \bar{\mathcal S}$ , $A\subset \cup_{i=1}^n B_i$ , then $\bar \mu(A)\le \sum_{i=1}^n \bar\mu(B_i)$ </li>
</ul>
<blockquote>
<p>相当于，上面 (i) 中如果第一个条件成立，对于有限情况下的 第二个条件 一定成立，并可以直接扩展到 $\bar{\mathcal S}$ 和 $\bar \mu$ 上 。</p>
</blockquote>
</li>
<li><p>可以借助 Thm , 证明 Stieltjes measure function 对应的 measure 存在，且证明过程需要左开。</p>
</li>
</ol>
<ol>
<li><p>(*) measure on $\mathbb R^d$</p>
<ol>
<li><p>直接采用类似 Stieltjes measure function 的条件构造 measure 是不够的</p>
<p>Restrictions : </p>
<ul>
<li>non-decreasing : If $\vec x\le \vec y$ ( $\forall i\in [d] , x_i\le y_i$) , then $F(\vec x)\le F(\vec y)$</li>
<li>right-continuous : Define $\vec y\downarrow \vec x$ as $\forall i\in [d] , y_i\downarrow x_i$ , then $\lim_{\vec y\downarrow \vec x}F(\vec y)=F(\vec x)$</li>
<li>(probability measure) $\lim\limits_{\vec x\downarrow -\infty}F(\vec x)=0$ , $\lim_{\vec x\uparrow +\infty}F(\vec x)=1$ </li>
</ul>
<p>Problem : </p>
<script type="math/tex; mode=display">
F(x_1,x_2)=\begin{cases}
1&x_1\ge 1,x_2\ge 1\\
\frac{2}{3}&x_1\ge 1,x_2\in [0,1)\\
\frac{2}{3}&x_1\in [0,1),x_2\ge 1\\
0&\text{otherwise}
\end{cases}</script><script type="math/tex; mode=display">
\mu((a_1,b_1]\times(a_2,b_2])=F(b_1,b_2)-F(a_1,b_2)-F(b_1,a_2)+F(a_1,a_2)</script><p>Let $a_1,a_2=1-\epsilon $ , $b_1,b_2=1$ , $\epsilon \to 0$ , then</p>
<script type="math/tex; mode=display">
\mu(\{1\}\times\{1\})=-\frac{1}{3}<0</script></li>
<li><p>Def : [ $\mathbb R^d$ measure ] </p>
<p>Consider finite rectangles $A=(a_1,b_1]\times\cdots\times(a_d,b_d]$ , $V=\{a_1,b_1\}\times\cdots\times\{a_d,b_d\}$ </p>
<p>If $v\in V$ , define </p>
<script type="math/tex; mode=display">
sgn(v)=(-1)^{|\{i\in [d]|v_i=a_i\}|}\\
\Delta_A F:=\sum_{v\in V}sgn(v)F(v)</script><p>let $\mu(A)=\Delta_A F$ .</p>
<blockquote>
<p>此处相当于 $d$ 维前缀和与差分，$V$ 相当于 $d$ 维矩形 $A$ 的所有顶点，$sgn(v)$ 相当于顶点 $v$ 有多少维是左顶点，然后容斥求差分。</p>
</blockquote>
</li>
<li><p>Thm : [$\mathbb R^d$ measure ]  If $F:\mathbb R^d\to [0,1]$ , satisfies the $3$ restrictions above , and for all rectangles $A$ , $\Delta_A F\ge 0$ . Then there is a unique probability measure $\mu$ on $(\mathbb R^d,\mathcal R^d)$ that $\mu(A)=\Delta_A F$ for all finite rectangles .</p>
</li>
<li><blockquote>
<p>If $F(\vec x)=\prod_{i=1}^d F_i(x_i)$ , $F_i$ are all Stieltjes measure function , then </p>
<script type="math/tex; mode=display">
\Delta_A F=\prod_{i=1}^d (F_i(b_i)-F_i(a_i))</script><p>When $F_i(x)=x$ for all $i\in [d]$ , $F$ is Lebesgue measure on $\mathbb R^d$ .</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h3 id="1-2-Random-Variables"><a href="#1-2-Random-Variables" class="headerlink" title="1.2 Random Variables"></a>1.2 Random Variables</h3><ol>
<li><p>measurable map</p>
<ol>
<li><p>Def : [measurable map] $X:\Omega\to S$ is a measurable map from $(\Omega,\mathcal F)$ to $(S,\mathcal S)$ if </p>
<script type="math/tex; mode=display">
\forall B\in \mathcal S , X^{-1}(B):=\{w\in \Omega|X(w)\in B\}\in \mathcal F</script><p>Def : [random vector] When $(S,\mathcal S)=(\mathbb R^d,\mathcal R^d)$ , $X$ is random vector .</p>
<p>Def : [random variable] When $(S,\mathcal S)=(\mathbb R,\mathcal R)$ , $X$ is a random variable .</p>
</li>
<li><blockquote>
<p>虽然 measurable map 写作 from $(\Omega,\mathcal F)$ to $(S,\mathcal S)$ ，但 $X$ 本身并不实现 $\mathcal F\to\mathcal S$ 的映射，只有 $\Omega\to S$ 的映射。 $\mathcal F,\mathcal S$ 是表明 measurable 的”范围” </p>
<p>Random variable is not a variable but a (measurable) map</p>
<p>这也很好解释了 $E(X^2)$ 这种类型的记号的实际含义</p>
</blockquote>
</li>
<li><p>Thm [a sufficient condition for measurable map]</p>
<p>$X:\Omega\to S$ , $\mathcal A $ : a collection of some subsets of $S$ , If</p>
<ul>
<li>$\forall A\in \mathcal A , X^{-1}(A)\in \mathcal F$</li>
<li>$\mathcal A$ generates $\mathcal S$</li>
</ul>
<p>Then $X$ is a measurable map from $(\Omega,\mathcal F)$ to $(S,\mathcal S)$ .</p>
<blockquote>
<p>Proof : Prove $\mathcal B=\{B\subset S|X^{-1}(B)\in \mathcal F\}$ is a $\sigma$-field , and obviously $\mathcal A\subset \mathcal B$ . Consider generation is the smallest , $\mathcal S\subset \mathcal B$ .</p>
</blockquote>
</li>
<li><p>E.g. $f:\mathbb R^d\to \mathbb R$ : $f(x_1,\cdots,x_d)=\sum_{i=1}^d x_i$ is a measurable map from $(\mathbb R^d,\mathcal R^d)$ to $(\mathbb R,\mathcal R)$ .</p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>概率与统计</category>
      </categories>
      <tags>
        <tag>概率论-概率空间</tag>
        <tag>实分析-测度</tag>
        <tag>实分析-代数空间</tag>
        <tag>实分析-测度-可测映射</tag>
        <tag>概率论-随机变量</tag>
      </tags>
  </entry>
  <entry>
    <title>答疑坊 微积分 趣题若干</title>
    <url>/2023/10/07/%E7%AD%94%E7%96%91%E5%9D%8A-%E5%BE%AE%E7%A7%AF%E5%88%86-%E8%B6%A3%E9%A2%98%E8%8B%A5%E5%B9%B2/</url>
    <content><![CDATA[<h3 id="P1"><a href="#P1" class="headerlink" title="P1"></a>P1</h3><p>类型：极限存在性</p>
<h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>证明下面的极限存在：</p>
<script type="math/tex; mode=display">
\lim_{n\to +\infty}(1+\frac{1}{2^2})(1+\frac{1}{3^2})\cdots(1+\frac{1}{n^2})</script><h4 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h4><p>显然上述数列单调增，因此只需要证明有上界</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad \prod_{k=2}^n (1+\frac{1}{k^2})\\
&=\exp\left(\sum_{k=2}^n \ln (1+\frac{1}{k^2})\right)\\
&\le \exp\left(\sum_{k=2}^n \frac{1}{k^2}\right)\\
&\le \exp\left(\sum_{k=2}^n \frac{1}{k(k-1)}\right)\\
&=\exp\left(\sum_{k=1}^n \frac{1}{k-1}-\frac{1}{k}\right)\\
&=\exp(1-\frac{1}{n})\\
&\le e
\end{aligned}</script><p>其中第 $3$ 行使用常见不等式 $\ln(1+x)\le x$ ，第 $4\sim 6$ 行为裂项。</p>
<hr>
<h3 id="P2"><a href="#P2" class="headerlink" title="P2"></a>P2</h3><p>类型：$e$ 相关极限不等式</p>
<h4 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h4><p>证明，对任意正整数 $n\ge 2$ ，</p>
<script type="math/tex; mode=display">
\sum_{k=0}^n \frac{1}{k!}-\frac{3}{2n}<(1+\frac{1}{n})^n<\sum_{k=0}^n \frac{1}{k!}</script><h4 id="解答-1"><a href="#解答-1" class="headerlink" title="解答"></a>解答</h4><p>使用二项式定理</p>
<script type="math/tex; mode=display">
\begin{aligned}
(1+\frac{1}{n})^n&=\sum_{k=0}^n \frac{n!}{k!(n-k)!}\frac{1}{n^k}\\
&=\sum_{k=0}^n \frac{1}{k!}\prod_{j=0}^{k-1}(1-\frac{j}{n})
\end{aligned}</script><p>显然对 $n\ge 2$ ,  $\prod_{j=0}^{k-1}(1-\frac{j}{n})&lt; 1$ ，故右侧不等式成立。</p>
<p>考虑 Bernoulli 不等式：</p>
<script type="math/tex; mode=display">
\forall a_1,\cdots,a_n>-1,且a_i 同号，则\prod_{k=1}^n (1+a_k)>1+\sum_{k=1}^n a_k</script><p>故</p>
<script type="math/tex; mode=display">
\begin{aligned}
(1+\frac{1}{n})^n&>\sum_{k=0}^n\left( \frac{1}{k!}-\frac{1}{k!}\sum_{j=1}^{k-1}\frac{j}{n}\right)\\
&=\sum_{k=0}^n \frac{1}{k!}-\sum_{k=0}^n \frac{1}{k!}\frac{k(k-1)}{2n}\\
&=\sum_{k=0}^n \frac{1}{k!}-\sum_{k=2}^n \frac{1}{(k-2)!}\frac{1}{2n}\\
&\ge \sum_{k=0}^n \frac{1}{k!}-\frac{1}{2n}\left(2+\sum_{k=2}^{n-2} \frac{1}{k(k-1)}\right)\\
&=\sum_{k=0}^n \frac{1}{k!}-\frac{1}{2n}(3-\frac{1}{n-2})\\
&>\sum_{k=0}^n \frac{1}{k!}-\frac{3}{2n}
\end{aligned}</script><hr>
]]></content>
      <categories>
        <category>答疑坊</category>
        <category>微积分</category>
      </categories>
      <tags>
        <tag>答疑坊</tag>
        <tag>微积分</tag>
        <tag>多元微积分</tag>
        <tag>级数</tag>
        <tag>微分方程</tag>
      </tags>
  </entry>
  <entry>
    <title>答疑坊 程设/离散/DSA 趣题若干</title>
    <url>/2023/10/07/%E7%AD%94%E7%96%91%E5%9D%8A-%E7%A8%8B%E8%AE%BE-DSA-%E8%B6%A3%E9%A2%98%E8%8B%A5%E5%B9%B2/</url>
    <content><![CDATA[<h3 id="P1"><a href="#P1" class="headerlink" title="P1"></a>P1</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>输入 $3n-1$ 个数，其中 $n-1$ 个数出现恰好 $3$ 次，$1$ 个数恰好出现 $2$ 次。求这个出现 $2$ 次的数。</p>
<p>要求，时间复杂度 $\tilde {\mathcal O}(n)$ , 空间复杂度 $\mathcal O(1)$ .</p>
<h4 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h4><p>考虑使用三进制不进位加法，显然满足交换律、结合律，且 $a\oplus a\oplus a=0$ ，出现 $3$ 次的数都抵消掉，只剩下恰好出现 $2$ 次的那个数 $a$ 的 “2倍”。</p>
<p>对最后结果 $sum$ 求 $sum\oplus sum$ 。因 $sum=a\oplus a$ , 故 $sum\oplus sum=a\oplus a\oplus a\oplus a=a$ 。</p>
]]></content>
      <categories>
        <category>答疑坊</category>
        <category>程设/离散/DSA</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>答疑坊</tag>
        <tag>程序设计</tag>
        <tag>离散数学</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 3</title>
    <url>/2023/10/07/Algorithm-Design-3/</url>
    <content><![CDATA[<h2 id="Chapter-2-Greedy-Algorithm"><a href="#Chapter-2-Greedy-Algorithm" class="headerlink" title="Chapter 2 Greedy Algorithm"></a>Chapter 2 Greedy Algorithm</h2><h3 id="2-3-Minimum-Spanning-Tree-undirected"><a href="#2-3-Minimum-Spanning-Tree-undirected" class="headerlink" title="2.3 Minimum Spanning Tree (undirected)"></a>2.3 Minimum Spanning Tree (undirected)</h3><ol>
<li><p>Definition</p>
<ol>
<li><p>Input : undirected , edge-weighted graph $G$</p>
</li>
<li><p>Output : Minimum Spanning Tree of $G$</p>
<p>Spanning Tree : a subgraph of $G$ that is a tree containing all vertices</p>
<p>Minimum : the sum of weights on tree’s all edges is minimized</p>
</li>
</ol>
</li>
<li><p>Properties</p>
<ol>
<li><p>Spanning Tree</p>
<ol>
<li>Connectivity $\leftrightarrow$ check connectivity $\to$ <strong>cut</strong></li>
<li>Acyclic</li>
</ol>
</li>
<li><p>Cut</p>
<ol>
<li><p>Def : Graph $G=(V,E),V=A\cup B,A\cap B=\varnothing$</p>
<p>$(A,B)$-cut : $E(A,B)=\{(u,v)\in E|u\in A,v\in B\}$</p>
</li>
<li><p>Observation : If $T$ is a Spanning Tree , $E(A,B)$ is any cut , then $T\cap E(A,B)\neq \varnothing$</p>
<blockquote>
<p>Otherwise , not connected</p>
</blockquote>
</li>
</ol>
</li>
<li><p>Lemma : Assumption : weights are distinct </p>
<p>Suppose $e=(u,v),u\in A,v\in B$ is the edge with minimum weight in $(A,B)$-cut , then every MST must contain $e$ .</p>
<p>Proof : [Exchange Argument , Prove by contradiction]</p>
<p>Suppose $T$ is a MST but $e\notin T$ . By the observation , there exists $e’\in T,e’\in E(A,B)$ .</p>
<p>Let $T’=T-e’+e$ , then $T$ is still connected , with smaller weight .</p>
</li>
</ol>
</li>
</ol>
<div class="note warning"><p>Problem : $T$ can have cycle !</p>
</div>
<div class="note success"><p>Correction :<br>Choose one specific $e’$ : $e\notin T$ , then $e$ and some edges in $T$ can form a cycle ( the path from $u$ to $v$ on $T$ ). This cycle must contain an edge $e’\in E(A,B)$ .</p>
</div>
<ol>
<li><p>Kruskal’s Algorithm</p>
<ol>
<li><p>Algorithm</p>
<p>Successively inserting edges from $E$ in increasing order of weight .</p>
<p>If edge $e$ would create a cycle . discard it .</p>
<p>(Using Union-Find Set)</p>
</li>
<li><p>Proof of Correctness</p>
<p>Consider every added edge $e$ , $e$ is the min-weight edge in some cut </p>
<p>$e=(u,v)$ , consider the connected component $C$ containing $u$ , then $e$ is the minimum weight edge in cut $E(C,V\backslash C)$ </p>
</li>
</ol>
</li>
<li><p>Prim’s Algorithm</p>
<ol>
<li><p>Algorithm</p>
<p>At each step , add the node that can be attached as cheaply as possible to the partial tree we already have .</p>
</li>
<li><p>Naive Implementation</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// d[x] : current minimum edge in cut E(&#123;x&#125;,S)</span></span><br><span class="line"><span class="comment">// Init </span></span><br><span class="line"><span class="keyword">for</span> (v in V)&#123;</span><br><span class="line">    d[v]=+inf;</span><br><span class="line">&#125;</span><br><span class="line">d[<span class="number">1</span>]=<span class="number">0</span>;S.<span class="built_in">insert</span>(<span class="number">1</span>);</span><br><span class="line"><span class="keyword">while</span>(S!=V)&#123;</span><br><span class="line">    <span class="type">int</span> x=<span class="built_in">argmin</span>(d[v] | v in V\S);</span><br><span class="line">    S.<span class="built_in">insert</span>(x); <span class="comment">// using the edge d[x]</span></span><br><span class="line">    <span class="keyword">for</span>(y in <span class="built_in">Neighbour</span>(x))&#123;</span><br><span class="line">        d[y]=<span class="built_in">min</span>(d[y],<span class="built_in">w</span>(x,y));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Improvement : Priority Queue</p>
</li>
</ol>
</li>
<li><p>Reverse Deletion</p>
<p>Successively removing edges from $E$ in decreasing order of weight .</p>
<p>If removing edge $e$ would cause disconnectedness , discard it  .</p>
<blockquote>
<p>The reverse version of Kruskal’s Algorithm</p>
</blockquote>
</li>
<li><p>Faster algorithms</p>
<p>[Chazelle] Deterministic Algorithm $\mathcal O(|E|\alpha(|E|))$</p>
<blockquote>
<p>This means MST is weaker than sorting </p>
</blockquote>
<p>[Karger , Klem , Tarjan] Randomized Algorithm $\mathcal O(|E|+|V|)$ </p>
<p><strong>Open Question</strong> : Deterministic Linear Algorithm </p>
</li>
</ol>
<h3 id="2-4-Union-Find-Set"><a href="#2-4-Union-Find-Set" class="headerlink" title="2.4 Union-Find Set"></a>2.4 Union-Find Set</h3><ol>
<li><p>Definition</p>
<p>Maintain sets of elements , support :</p>
<ol>
<li><code>find(element e)</code> , return the set that contains $e$</li>
<li><code>union(set Si,set Sj)</code> , combine set $S_i$ and $S_j$ , return the union set $S_i\cup S_j$</li>
</ol>
</li>
<li><p>Implementation</p>
<p>The sets can be viewed as trees .</p>
<p>The set can be represented by the root of the corresponding tree .</p>
<p><code>find</code> : find the root of the tree (keep finding father)</p>
<p><code>union</code> : $fa(root(S_1))\leftarrow root(S_2)$ </p>
<p>Problem : tree can be very deep</p>
</li>
<li><p>Improve</p>
<ol>
<li><p>启发式合并 / Heuristic merging</p>
<p>To make the tree shallow , suppose $|S_1|\le |S_2|$ </p>
<p>小集合合并到大集合上</p>
</li>
<li><p>路径压缩 / Path Compression</p>
<p>Suppose one find : $e,v_1,\cdots,v_k,root$ , after the find , let $fa(e)=fa(v_1)=\cdots=fa(v_k)=root$</p>
</li>
</ol>
</li>
<li><p>Time Complexity</p>
<p>Amortized Analysis / 均摊分析   ( using Path Compression and Heuristic merging)</p>
<p>Suppose we have a sequence of <code>find</code> or <code>union</code> operations , with length $m$ . </p>
<p>Total running time of union-find is  $\mathcal O(m\alpha(m,n))$</p>
<p>$\alpha$ : inverse Ackermann’s function , grows very slow </p>
<blockquote>
<p>Another slow function : $\log^*(n)$ , number of logs to make $n$ to small constant (i.e. $\log(\log\cdots\log(n))\le 5$)</p>
</blockquote>
<p>See [CLRS] for detailed proof .</p>
</li>
<li><p>Kruskal’s Algorithm Implementation</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sort</span>(E); <span class="comment">// weight-increasing</span></span><br><span class="line"><span class="keyword">for</span>(edge e=(u,v):E)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">find</span>(u)!=<span class="built_in">find</span>(v))&#123;</span><br><span class="line">        T.<span class="built_in">insert</span>(e);</span><br><span class="line">        <span class="built_in">union</span>(<span class="built_in">find</span>(u),<span class="built_in">find</span>(v));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Total time complexity : $\mathcal O(|E|\log|E|)$ .</p>
</li>
</ol>
<h3 id="2-5-Priority-Queue"><a href="#2-5-Priority-Queue" class="headerlink" title="2.5 Priority Queue"></a>2.5 Priority Queue</h3><ol>
<li><p>Definition</p>
<p>Maintain a set of elements $S$ , support :</p>
<p><code>insert(S,x)</code> , insert $x$ into $S$ .</p>
<p><code>extract_max(S)</code> , return the maximum element in $S$ , and then remove this element .</p>
<p><code>max(S)</code> , return the maximum element in $S$</p>
<p><code>increase_key(S,x,k)</code> , increase value of $x$ by $k$</p>
</li>
<li><p>(Basic) (max) Heap</p>
<ol>
<li><p>Property : Complete binary tree , parent $\ge $ both children </p>
</li>
<li><p>Can be implemented in an array</p>
<p><code>fa[x]=x/2</code> , <code>left_child[x]=2x</code> , <code>right_child[x]=2x+1</code> </p>
</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">insert</span>(S,x)&#123; <span class="comment">// flow down , O(log n)</span></span><br><span class="line">    val[|S|]=x;|S|++;</span><br><span class="line">    <span class="type">int</span> p=|S|<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">while</span>(p!=root&amp;&amp;val[p]&gt;val[fa[p]])&#123;</span><br><span class="line">        <span class="built_in">swap</span>(val[p],val[fa[p]]);</span><br><span class="line">        p=fa[p];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Note : build a heap : O(n)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">extract_max</span>(S)&#123; <span class="comment">// flow up , O(log n)</span></span><br><span class="line">    <span class="type">int</span> ret_val=val[root];</span><br><span class="line">    <span class="built_in">swap</span>(val[root],val[|S|<span class="number">-1</span>]); <span class="comment">// swap the root and the last element</span></span><br><span class="line">    |S|--; <span class="comment">// delete original root</span></span><br><span class="line">    <span class="type">int</span> p=root;</span><br><span class="line">    <span class="keyword">while</span>(val[p]&lt;val[left_child[p]]||val[p]&lt;val[right_child[p]])&#123;</span><br><span class="line">        <span class="keyword">if</span>(val[left_child[p]]&gt;val[right_child[p]])&#123;</span><br><span class="line">            <span class="built_in">swap</span>(val[p],val[left_child[p]]);</span><br><span class="line">            p=left_child[p];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">swap</span>(val[p],val[right_child[p]]);</span><br><span class="line">            p=right_child[p];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret_val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">max</span>(S)&#123; <span class="comment">// O(1)</span></span><br><span class="line">    <span class="keyword">return</span> val[root];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">increase_key</span>(S,p,k)&#123; <span class="comment">// flow down , O(log n)</span></span><br><span class="line">    val[p]=val[p]+k;</span><br><span class="line">    <span class="keyword">while</span>(p!=root&amp;&amp;val[p]&gt;val[fa[p]])&#123;</span><br><span class="line">        <span class="built_in">swap</span>(val[p],val[fa[p]]);</span><br><span class="line">        p=fa[p];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Advanced : Fibonacci Heap</p>
<p>Time complexity (amortized-time)</p>
<p>| operations     | binary heap          | Fibonacci heap       |<br>| ——————— | —————————— | —————————— |<br>| <code>insert</code>       | $\mathcal O(\log n)$ | $\mathcal O(1)$      |<br>| <code>extract_max</code>  | $\mathcal O(\log n)$ | $\mathcal O(\log n)$ |<br>| <code>max</code>          | $\mathcal O(1)$      | $\mathcal O(1)$      |<br>| <code>increase_key</code> | $\mathcal O(\log n)$ | $\mathcal O(1)$      |</p>
<p>See [CLRS] for detailed Fibonacci heap .</p>
</li>
<li><p>Prim’s Algorithm with Priority Queue</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// using min priority queue</span></span><br><span class="line"><span class="keyword">for</span> (v in V\&#123;<span class="number">1</span>&#125;)&#123;</span><br><span class="line">    d[v]=+inf;</span><br><span class="line">    inS[v]=<span class="literal">false</span>;</span><br><span class="line">    PQ.<span class="built_in">insert</span>((v,+inf));</span><br><span class="line">&#125;</span><br><span class="line">d[<span class="number">1</span>]=<span class="number">0</span>;inS[<span class="number">1</span>]=<span class="literal">true</span>;</span><br><span class="line">PQ.<span class="built_in">insert</span>((<span class="number">1</span>,<span class="number">0</span>));</span><br><span class="line"><span class="comment">// index 1 , value 0</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;n;++i)&#123; <span class="comment">// n-1 rounds</span></span><br><span class="line">    <span class="type">int</span> x=PQ.<span class="built_in">extract_min</span>(); <span class="comment">// argmin value</span></span><br><span class="line">    inS[x]=<span class="literal">true</span>;</span><br><span class="line">    <span class="comment">// using edge d[x]</span></span><br><span class="line">    <span class="keyword">for</span>(y in <span class="built_in">Neighbour</span>(x))&#123;</span><br><span class="line">        <span class="keyword">if</span>(!inS[y] &amp;&amp; <span class="built_in">w</span>(x,y)&lt;d[y])&#123;</span><br><span class="line">            PQ.<span class="built_in">decrease_key</span>(y,d[y]-<span class="built_in">w</span>(x,y));</span><br><span class="line">            d[y]=<span class="built_in">w</span>(x,y);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Time Complexity : $\mathcal O(|E|\log |V|)$ .</p>
</li>
<li><p>Dijkstra’s Algorithm with Priority Queue</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// using min priority queue</span></span><br><span class="line"><span class="keyword">for</span>(v in V\&#123;s&#125;)&#123;</span><br><span class="line">    d[v]=+inf;</span><br><span class="line">    inS[v]=<span class="literal">false</span>;</span><br><span class="line">    PQ.<span class="built_in">insert</span>((v,+inf))</span><br><span class="line">&#125;</span><br><span class="line">d[s]=<span class="number">0</span>;inS[s]=<span class="literal">true</span>;</span><br><span class="line">PQ.<span class="built_in">insert</span>((v,+inf));</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;n;++i)&#123; <span class="comment">// n-1 rounds</span></span><br><span class="line">    <span class="type">int</span> x=PQ.<span class="built_in">extract_min</span>();</span><br><span class="line">    inS[x]=<span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span>(y in <span class="built_in">Neighbour</span>(x))&#123;</span><br><span class="line">        <span class="keyword">if</span>(!inS[y] &amp;&amp; d[y] &gt; d[x]+<span class="built_in">w</span>(x,y) )&#123;</span><br><span class="line">            PQ.<span class="built_in">decrease_key</span>(d[y]-d[x]-<span class="built_in">w</span>(x,y));</span><br><span class="line">            d[y]=d[x]+<span class="built_in">w</span>(x,y);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Time Complexity : $\mathcal O(|E|\log |V|)$ .</p>
</li>
</ol>
<h3 id="2-6-Huffman-Code"><a href="#2-6-Huffman-Code" class="headerlink" title="2.6 Huffman Code"></a>2.6 Huffman Code</h3><ol>
<li><p>Code Definition : a set $S$ of letters encode to $\{0,1\}^<em>$ , $r:S\to \{0,1\}^</em>$</p>
<p>Properties</p>
<ol>
<li>$r$ is one-to-one / injection</li>
<li>encoding / decoding efficiently</li>
<li>minimize average length of the code</li>
<li>[optional] robust to errors , Error Correction Code</li>
</ol>
</li>
<li><p>Prefix Code</p>
<ol>
<li><p>Def : $\forall x,y\in S$ , $r(x)$ is not a prefix of $r(y)$</p>
</li>
<li><p>Property : useful for decoding (unique decode) , can decode greedily</p>
</li>
<li><p>e.g. $S=\{a,b,c,d,e\}$</p>
<p>Prefix Code </p>
<script type="math/tex; mode=display">
r(a)=11,r(b)=01,r(c)=001,r(d)=10,r(e)=000</script><script type="math/tex; mode=display">
decode(0000011101)=decode(000\ 001\ 11\ 01)=ecab</script><p>non-Prefix Code</p>
<script type="math/tex; mode=display">
r(a)=110,r(b)=11,r(c)=01</script><script type="math/tex; mode=display">
decode(11011)=decode(110\ 11)=decode(11\ 01\ 1)</script></li>
<li><p>Prefix Code can be represented using a binary tree</p>
<p>Leafy tree : each leaf corresponds to a letter</p>
</li>
</ol>
</li>
<li><p>Minimize the length</p>
<ol>
<li><p>Average encoding length</p>
<p>Suppose each letter has a frequency $p(x)$ , $\sum_{x\in S}p(x)=1$ .</p>
<p>Average encoding length :</p>
<script type="math/tex; mode=display">
AEL(r)=\sum_{x\in S}p(x)|r(x)|</script></li>
<li><p>[Shannon] Source Coding Theorem</p>
<script type="math/tex; mode=display">
\forall r,AEL(r)\ge \sum_{x\in S}-p(x)\log p(x) =:H</script><p>See : [Thomas Cover] Information Theory</p>
</li>
</ol>
</li>
<li><p>Lemma</p>
<p>There is an optimal code ( or an optimal binary tree ) in which two lowest-frequency letter are assigned to leaves that are as deep as possible , and are siblings .</p>
<blockquote>
<p>Proof :  同层换：对 AEL 无影响；跨层： exchange argument </p>
</blockquote>
</li>
<li><p>Huffman’s Code</p>
<p>Initially , construct a set $S$ containing all letters.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(|S|&gt;<span class="number">1</span>)&#123;</span><br><span class="line">    elem x,y;</span><br><span class="line">    x=<span class="built_in">extract_max</span>(S) , y=<span class="built_in">extract_max</span>(S);</span><br><span class="line">    elem new_elem=(id,<span class="built_in">p</span>(x)+<span class="built_in">p</span>(y));</span><br><span class="line">    <span class="built_in">insert</span>(S,new_elem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-图论</tag>
        <tag>算法-图论-最小生成树</tag>
        <tag>算法-图论-最短路</tag>
        <tag>算法-数据结构-并查集</tag>
        <tag>算法-数据结构-堆/优先队列</tag>
        <tag>编码-Huffman编码</tag>
      </tags>
  </entry>
  <entry>
    <title>Zero-Knowledge Proof and Secure Multi-Party Computation 1</title>
    <url>/2023/10/06/Zero-Knowledge-Proof-and-Secure-Multi-Party-Computation-1/</url>
    <content><![CDATA[<h2 id="Lec01-Basic-Definitions-and-Examples-of-ZKP"><a href="#Lec01-Basic-Definitions-and-Examples-of-ZKP" class="headerlink" title="Lec01 Basic Definitions and Examples of ZKP"></a>Lec01 Basic Definitions and Examples of ZKP</h2><h3 id="1-Basic-Notations"><a href="#1-Basic-Notations" class="headerlink" title="1 Basic Notations"></a>1 Basic Notations</h3><ol>
<li><p>Complexity</p>
<ol>
<li><p>Efficient Algorithm : poly-time algorithm .</p>
</li>
<li><p>NP class : class of problems that are easy to verify a solution (but may be hard to solve it)</p>
<p>Formal def : Suppose $L\subset \{0,1\}^*$ is a language . $L\in NP$ if there exists a poly-time algorithm $\mathcal A$ :</p>
<ul>
<li>If $x\in L$ , $\exists w\in \{0,1\}^{poly(|x|)}$ , $\mathcal A(x,w)=1$</li>
<li>If $x\notin L$ , $\forall w\in \{0,1\}^{poly(|x|)}$ , $\mathcal A(x,w)=0$</li>
</ul>
<p>$w$ that makes $\mathcal A(x,w)=1$ is called the proof of $x\in L$ , and $\mathcal A$ is called the verification algorithm .</p>
</li>
</ol>
</li>
<li><p>Proof</p>
<ol>
<li><p>Def : a static sequence of rules </p>
</li>
<li><p>E.g.1 : Prove that $f(x,y)=x^2y^3-5xy+4=0$ has a solution .</p>
<p>Proof 1 [Explicit Proof] : $f(1,1)=0$</p>
<p>Proof 2 [Implicit Proof] : $f(2,1)<0,f(2,2)>0$ .</p>
<p>( $f(2,x)$ is continuous , so $\exists x_0\in (1,2) , f(2,x_0)=0$ )</p>
</li>
</ol>
</li>
</ol>
<h3 id="2-Interactive-Proof"><a href="#2-Interactive-Proof" class="headerlink" title="2 Interactive Proof"></a>2 Interactive Proof</h3><ol>
<li><p>Interactive Proof </p>
<ol>
<li><p>A Prover and a Verifier , Prover needs to convince Verifier some proposition .</p>
<p>Usually , Prover needs to convince Verifier that some proposition is <strong>true</strong> . $(x\in L)$</p>
</li>
<li><p>Prover : with unbounded computation sources </p>
<p>Verifier : Only efficient algorithm </p>
</li>
<li><p>Complexity Class : IP : class of problems that are easy to determine with an interactive proof</p>
<ul>
<li>$NP\subsetneq IP$</li>
</ul>
</li>
<li><p>Transcript</p>
<p>Prover sends $m_1$ to Verifier , Verifier receives $m_1$ and sends $m_2$ to Prover , …</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(x,r_p)\to m_1\quad \quad &\\
V(x,r_v,m_1,\cdots,m_{i-1})\to m_i\quad& \text{for odd }i\\
P(x,r_p,m_1,\cdots,m_{i-1})\to m_i\quad& \text{for even }i\\
V(x,r_v,m_1,\cdots,m_k)\to y\in \{0,1\}&
\end{aligned}</script><ol>
<li>Def : transcript : $\tau=\{m_1,\cdots,m_k\}$</li>
<li>Def : $\braket{P,V}(x):=y$</li>
</ol>
</li>
</ol>
</li>
<li><p>IP Criteria</p>
<ol>
<li><p>Completeness : If Prover is honest , Verifier accepts the proof .</p>
<p>$\forall x\in L , \Pr\{\left<P,V\right>(x)=1\}=1$</p>
</li>
<li><p>Soundness : If Prover is dishonest , Verifier rejects the proof (with high probability) .</p>
</li>
<li><p>Zero-Knowledge : If Prover is honest , Verifier cannot know more than “knowing the proposition is true”</p>
</li>
</ol>
</li>
<li><p>Definition (somewhat formal (?) )</p>
<p>A pair of randomized algorithms $(P,V)$ is an interactive proof for $L$ </p>
<ol>
<li><p>$V$ runs in poly-time</p>
</li>
<li><p>Completeness : $\forall x\in L , \Pr\{\braket{P,V}(x)=1\}=1$</p>
</li>
<li><p>Soundness : $\forall x\notin L,\forall P^<em>,\Pr\{\braket{P^</em>,V}(x)=1\}&lt; \epsilon$</p>
</li>
<li><p>Zero-Knowledge (Optimal) </p>
<p>$\forall x\in L$ , $V$ can generate everything itself with bounded computation time without interaction</p>
<p><strong>gain sth. cannot gain by PPT $\Rightarrow$ gain knowledge</strong></p>
</li>
</ol>
</li>
</ol>
<h3 id="3-IP-Examples"><a href="#3-IP-Examples" class="headerlink" title="3 IP Examples"></a>3 IP Examples</h3><ol>
<li><p>Graph non-Isomorphism</p>
<ol>
<li><p>Description</p>
<p>Prover and Verifier know graphs $G_0,G_1$ .</p>
<p>Prover wants to convince verifier that $G_0$ is not isomorphic with $G_1$ .</p>
<script type="math/tex; mode=display">
\begin{aligned}
&G_0=(V,E_0) , G_1=(V,E_1)\\
\not\exists \pi\in S_{|V|}&\ ,\ \{(\pi(u),\pi(v)):(u,v)\in E_0\}=E_1
\end{aligned}</script></li>
<li><p>Protocol</p>
<p>Global : Prover and Verifier already know $G_0,G_1$ .</p>
<p>|                     Prover                     |                  | Verifier                                                     |<br>| :——————————————————————: | :———————: | —————————————————————————————— |<br>|                                                |                  | $b\leftarrow \{0,1\}$ , $\pi\leftarrow S_{|V|}$              |<br>|                                                | &lt;- $\tilde G$ — | $\tilde G:=\pi(G_b)$                                         |<br>| Find $\tilde b $ , $G_{\tilde b}\sim \tilde G$ | — $\tilde b$ -&gt; |                                                              |<br>|                                                |                  | $y=\begin{cases}1&amp; \text{if }b=\tilde b\\0&amp;\text{otherwise}\end{cases}$ |</p>
</li>
<li><p>Analysis</p>
<ol>
<li><p>Completeness : When $G_0\not\sim G_1$ , $G_{\tilde b}\sim \tilde G\sim G_b$ , so $b=\tilde b$ . Always accept .</p>
</li>
<li><p>Soundness : When $G_0\sim G_1$ , any prover can only guess $\tilde b$ randomly since $G_0\sim G_1\sim \tilde G$ . $\Pr\{V\text{ accept}\}\le \frac{1}{2}$ .</p>
<p>Proof : $P^*$ knows $\tilde G$ and wants to guess $b$ , we need to prove that</p>
<script type="math/tex; mode=display">
\forall P^* , \Pr\{P^*(\tilde G)=b\}\le \frac{1}{2}</script><p>Since $\forall \tilde G\sim G_0\sim G_1$ , </p>
<script type="math/tex; mode=display">
\begin{aligned}
\Pr\{\pi(G_0)=\tilde G\}&=\Pr\{\pi(G_1)=\tilde G\}\\
\Rightarrow \Pr\{\pi(G_b)=\tilde G|b=0\}&=\Pr\{\pi(G_b)=\tilde G|b=1\}\\
\Rightarrow \Pr\{b=0|\pi(G_b)=\tilde G\}&=\Pr\{b=1|\pi(G_b)=\tilde G\}\\
\end{aligned}</script><p>Therefore ,</p>
<script type="math/tex; mode=display">
\forall P^* , \Pr\{P^*(\tilde G)=b|\pi(G_b)=\tilde G\}\le\frac{1}{2}</script><p>Therefore ,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Pr\{P^*(\tilde G)=b\}&=\sum_{\tilde G}\Pr\{P^*(\tilde G)=b|\pi(G_b)=\tilde G\}\Pr\{\pi(G_b)=\tilde G\}\\
&\le \frac{1}{2}\sum_{\tilde G}\Pr\{\pi(G_b)=\tilde G\}\\
&=\frac{1}{2}
\end{aligned}</script><p>$k$ rounds : $\Pr\{V\text{ accepts}\}\le \frac{1}{2^k}$ .</p>
</li>
<li><p>Zero-Knowledge </p>
<p>Verifier itself knows $b,\pi,\tilde G$ .  Prover tells Verifier $\tilde b$ .</p>
<p>Only consider $G_0\not\sim G_1$ , then $\tilde b$ must be $b$ . Verifier itself can generate the transcript .</p>
<blockquote>
<p>注意：因为 zero-knowledge 是对 Prover 的保护，我们只需要保护诚实 Prover 的隐私，因此只要考虑命题成立的情况（即 $G_0\not\sim G_1$）。</p>
</blockquote>
</li>
</ol>
</li>
<li><p>Notes</p>
<blockquote>
<p>Given $G_0\not\sim G_1$ , Verifier can generate the proof itself </p>
<p>Due to Verifier’s randomness , Prover cannot generate the proof itself</p>
<p>Graph non-isomorphism Problem is not $NP$ , but can be solved by $IP$ . </p>
</blockquote>
</li>
</ol>
</li>
<li><p>Collision Problem</p>
<ol>
<li><p>Definition</p>
<ol>
<li>Given $h:\{0,1\}^n\to\{0,1\}^n$ , either $h$ is a permutation or $|Im(h)|\le 2^{n-1}$ .</li>
<li>Prover wants to convince Verifier that $h$ is a permutation .</li>
</ol>
</li>
<li><p>Protocol 1</p>
<p>Global : Prover and Verifier already know $h$ . $\forall x\in \{0,1\}^n$ , Verifier can get $h(x)$ in poly-time .</p>
<p>|              Prover               |                  |                          Verifier                          |<br>| :———————————————-: | :———————: | :————————————————————————————: |<br>|                                   |                  |                  $x\leftarrow \{0,1\}^n$                   |<br>|                                   |    &lt;- $y$ —     |                         $y:=h(x)$                          |<br>| Find $\tilde x$ , $h(\tilde x)=y$ | — $\tilde x$ -&gt; |                                                            |<br>|                                   |                  | $\begin{cases}1&amp;x=\tilde x\\0&amp;\text{otherwise}\end{cases}$ |</p>
</li>
<li><p>Analysis 1</p>
<ol>
<li><p>Completeness : When $h$ is a permutation , $h$ is a injection , $x=\tilde x$ .</p>
</li>
<li><p>Soundness : When $h$ is not a permutation , then $|Im(h)|\le 2^{n-1}$ .</p>
<script type="math/tex; mode=display">
\forall x , P^*\ ,\ \Pr\{P^*(h(x))=x\}=\frac{1}{|h^{-1}(h(x))|}</script><p>Therefore ,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Pr\{V\text{ accepts}\}&=\sum_{y\in Im(h)}\Pr\{y=f(x)|x\in \{0,1\}^n\}\Pr\{P^*(h(x))=x\}\\
&=\sum_{y\in Im(h)}\frac{|h^{-1}(y)|}{2^n}\frac{1}{|h^{-1}(y)|}\\
&=\frac{|Im(h)|}{2^n}\\
&\le \frac{1}{2}
\end{aligned}</script><p>$k$ rounds : $\Pr\{V \text{ accepts}\}\le\frac{1}{2^k}$ .</p>
</li>
<li><p>Zero-knowledge : </p>
<p>Verifier itself knows $x,y$ . Prover tells Verifier $\tilde x$ .</p>
<p>When $h$ is a permutation , $\tilde x=x$ , so Verifier can generate $\tilde x$ itself .</p>
</li>
</ol>
</li>
<li><p>Protocol 2</p>
<p>Global : Prover and Verifier already know $h$ . $\forall x\in \{0,1\}^n$ , Verifier can get $h(x)$ in poly-time .</p>
<p>|       Prover        |           |                        Verifier                        |<br>| :————————-: | :———-: | :——————————————————————————: |<br>|                     | &lt;- $y$ — |                $y\leftarrow \{0,1\}^n$                 |<br>| Find $x$ , $h(x)=y$ | — $x$ -&gt; |                                                        |<br>|                     |           | $\begin{cases}1&amp;h(x)=y\\0&amp;\text{otherwise}\end{cases}$ |</p>
</li>
<li><p>Analysis 2</p>
<ol>
<li><p>Completeness : When $h$ is a permutation , $h$ is a bijection , $x$ always exists and unique . Always accept .</p>
</li>
<li><p>Soundness : When $h$ is not a permutation , $|Im(h)|\le 2^{n-1}$ .</p>
<script type="math/tex; mode=display">
\Pr\{h^{-1}(y)=\varnothing|y\leftarrow \{0,1\}^n\}=1-\frac{|Im(h)|}{2^n}\ge \frac{1}{2}</script><p>When $h^{-1}(y)=\varnothing$ , any Prover cannot convince verifier , so </p>
<script type="math/tex; mode=display">
\Pr\{V\text{ accepts}\}\le 1-\Pr\{h^{-1}(y)=\varnothing|y\leftarrow \{0,1\}^n\}\le \frac{1}{2}</script><p>$k$ rounds : $\Pr\{V \text{ accepts}\}\le\frac{1}{2^k}$ .</p>
</li>
<li><p>Zero-knowledge : NOT zero-knowledge</p>
<p>$V$ knows $h^{-1}(y)$ , which cannot be computed in poly-time by itself .</p>
</li>
</ol>
</li>
<li><p>Notes</p>
<blockquote>
<p>In Protocol 1 , when $h$ is a permutation , Verifier can always generate a valid proof</p>
<p>In both Protocols , Prover must have the ability to solve $h^{-1}$ . This may lead Verifier knowing something more .</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h3 id="4-Formularize-zero-knowledge-IP"><a href="#4-Formularize-zero-knowledge-IP" class="headerlink" title="4 Formularize zero-knowledge IP"></a>4 Formularize zero-knowledge IP</h3><ol>
<li><p>Security Parameter : $\kappa$</p>
<ol>
<li>For efficiency : usually $|x|=poly(\kappa)$  , PPT should run in $poly(\kappa)$ .</li>
<li>For security : negligible function</li>
</ol>
</li>
<li><p>Negligible function : $\epsilon$</p>
<ol>
<li><p>Def : negligible function $\epsilon:\mathbb Z^<em>\to \mathbb R^</em>$</p>
<p>For all polynomial $p$ , $\exists c\in \mathbb Z^* , \forall k&gt;c,\epsilon(k)&lt;\frac{1}{p(k)}$ .</p>
</li>
<li><p>Propositions :</p>
<ol>
<li>$\epsilon , \delta$ negligible $\to$ $\epsilon+\delta , \epsilon\delta$ negligible </li>
<li>$\epsilon$ negligible $\to$ For all polynomial $p$ , $p\epsilon$ negligible</li>
</ol>
</li>
<li><p>Conventions :</p>
<ol>
<li><p>$\epsilon$ noticeable : $\exists$ polynomial $p$ , $\exists c\in \mathbb Z^*,\forall k&gt;c,\epsilon(k)\ge \frac{1}{p(k)}$ </p>
</li>
<li><p>$A$ happens with overwhelming probability : </p>
<script type="math/tex; mode=display">
\Pr\{A\}\ge 1-\epsilon \quad\quad \epsilon \text{ is negligible}</script></li>
</ol>
</li>
</ol>
<blockquote>
<p>There exists function that is neither negligible nor noticeable </p>
</blockquote>
</li>
<li><p>View of Verifier</p>
<script type="math/tex; mode=display">
View_V^P(x):=(x,r,\tau)</script></li>
<li><p>Honest-Verifier Zero-knowledge</p>
<ol>
<li><p>Perfect honest-verifier zero-knowledge</p>
<script type="math/tex; mode=display">
\exists M\in PPT\ ,\ \forall x\in L\ ,\ View_V^P(x)\equiv M(x)</script></li>
<li><p>Statistical honest-verifier zero-knowledge</p>
<p>The statistical distance between $View_V^P(x)$ and $M(x)$ is negligible</p>
<p>$\exists M\in PPT , \forall x\in L$ ,</p>
<script type="math/tex; mode=display">
SD(View_V^P(x),M(x))=\frac{1}{2}\sum_{s}\left|\Pr\{View_V^P(x)=s\}-\Pr\{M(x)=s\}\right|<\epsilon</script></li>
<li><p>Computational honest-verifier zero-knowledge</p>
<p>$\exists M\in PPT , \forall x\in L ,  \forall$ distinguisher $D\in PPT$ ,</p>
<script type="math/tex; mode=display">
\left|\Pr\{D(View_V^P(x))=1\}-\Pr\{D(M(x))=1\}\right|<\epsilon</script></li>
</ol>
</li>
<li><p>Dishonest Verifier</p>
<p>$\braket{P,V}$ achieves perfect/statistical/computational dishonest-verifier zero-knowledge if :</p>
<p>$\forall V^<em>\in PPT ,  \exists$ expected poly-time randomized algorithm $M^</em>$ , $\forall x\in L$ , $View_{V^<em>}^P(x)$ and $M^</em>(x)$ are perfectly/statistically/computationally indistinguishable .</p>
</li>
<li><p>Graph Isomorphism IP with dishonest-verifier zero-knowledge</p>
<ol>
<li><p>Definition</p>
<p>Prover and Verifier know graph $G_0,G_1$ .</p>
<p>Prover wants to convince Verifier that $G_0\sim G_1$ , i.e. $\exists \pi,\pi(G_0)=G_1$ .</p>
</li>
<li><p>Protocol</p>
<p>|                  Prover                   |                |                           Verifier                           |<br>| :———————————————————-: | :——————: | :—————————————————————————————: |<br>|           $\pi_r\leftarrow S_n$           |                |                                                              |<br>|          $\tilde G:=\pi_r(G_0)$           | —$\tilde G$-&gt; |                                                              |<br>|                                           |    &lt;-$b$—     |                    $b\leftarrow \{0,1\}$                     |<br>| find $\pi_b$ , s.t. $\tilde G=\pi_b(G_b)$ |  —$\pi_b$-&gt;   |                                                              |<br>|                                           |                | $\begin{cases}1&amp;\tilde G=\pi_b(G_b)\\0&amp;\text{otherwise}\end{cases}$ |</p>
<blockquote>
<p>Note : If Prover knows $\pi$ that $\pi(G_0)=G_1$ , then $\pi_b$ can be constructed :</p>
<script type="math/tex; mode=display">
\pi_b=\begin{cases}\pi_r&b=0\\\pi_r\circ\pi^{-1}&b=1\end{cases}</script></blockquote>
</li>
<li><p>Analysis</p>
<ol>
<li><p>Completeness : If $G_0\sim G_1$ , $\pi_b$ can be constructed as above . Always Accept .</p>
</li>
<li><p>Soundness : If $G_0\not\sim G_1$ , $\exists b^<em>\in \{0,1\},G_{b^</em>}\not\sim\tilde G$ </p>
<p>$\Pr\{V \text{ rejects}\}\ge \Pr\{V\text{ chooses }b^*\}=\frac{1}{2}$ </p>
<blockquote>
<p>Note : we cannot let Verifier just choose $b=1$ , since a malicious Prover can violate the protocol , $\pi_r$ may not be a permutation , and $\tilde G$ may not isomorphic to $G_0$ .</p>
</blockquote>
</li>
<li><p>honest-verifier zero-knowledge : </p>
<p>Verifier itself knows : $G_0,G_1,b$ , $r$ : randomness generating $b$ . Prover tells verifier $\tilde G,\pi_b$ .</p>
<p>$View_V^P=(G_0,G_1,r,b,\tilde G,\pi_b)$ .</p>
<p>$M_V$ : </p>
<pre><code> 1. sample $b\leftarrow \&#123;0,1\&#125;$
 2. choose $\pi_b\leftarrow S_n$
 3. $\tilde G:=\pi_b(G_b)$
</code></pre><p>Since Prover and Verifier are honest , $b$ is independent of $\tilde G $ , and $\pi_b$ is generated by a uniformly random $\pi_r$ hence is also uniformly random .</p>
</li>
<li><p>dishonest-verifier zero-knowledge :</p>
<p>Malicious Verifier can choose $b$ based on $\tilde G$ to gain more knowledge .</p>
<p>$M^*$ : should perform as Prover :</p>
<pre><code> 1.  guess $b^*\in \&#123;0,1\&#125;$ 
 2.  compute $\pi_r\leftarrow S_n$ , $\tilde G:=\pi_r(G_&#123;b^*&#125;)$
 3.  use Verifier to receive $b$
 4.  If $b\neq b^*$ , go back to 1.
 5.  If $b=b^*$ , then let $\pi_b=\pi_r$ , get the view 
</code></pre></li>
</ol>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>零知识证明和多方安全计算</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>密码学-零知识证明</tag>
        <tag>密码学-交互式证明</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 2</title>
    <url>/2023/10/06/Algorithm-Design-2/</url>
    <content><![CDATA[<h3 id="1-3-dfs-application"><a href="#1-3-dfs-application" class="headerlink" title="1.3 dfs application"></a>1.3 dfs application</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">dfs</span><span class="params">(u)</span></span>&#123;</span><br><span class="line">    ++time;</span><br><span class="line">    discover[u]=time;</span><br><span class="line">    col[u]=grey;</span><br><span class="line">    <span class="keyword">for</span>(v in <span class="built_in">Neighbour</span>(u) )&#123;</span><br><span class="line">        <span class="keyword">if</span>(col[v]==white)&#123;</span><br><span class="line">            <span class="built_in">dfs</span>(v)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    col[u]=black;</span><br><span class="line">    ++time;</span><br><span class="line">    finish[u]=time;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>THM parenthesis</p>
<p>on dfs tree , if $u$ is one ancestor of $v$ , then $[v.d,v.f]\subset[u.d,u.f]$</p>
<p>on dfs tree , otherwise , then $[u.d,u.f]\cap [v.d,v.f]=\varnothing$</p>
</li>
<li><p>THM white-path</p>
<p>on dfs tree ,  at the time when $u$ is discovered , </p>
<p> $u$ is one ancestor of $v$  $\Leftrightarrow$ $\exists$ white path from $u$ to $v$ </p>
<p>即： $v$ 是 $u$ 的后代$\Leftrightarrow$在 $u$ 刚访问到的时候一定存在一条完全没有被访问过的路径 $u\to v$ .</p>
<ul>
<li>Proof : use parenthesis THM</li>
</ul>
<p>$\Rightarrow$ trivial</p>
<p>$\Leftarrow$  proof by contradiction </p>
<p>Consider a white path $x_1=u,x_2,\cdots,x_m=v$ , and $x_k$ is the last vertex that is a descendent of $u$ (including $u$ itself) .</p>
<p>We need to prove that $x_{k+1}$ is also a descendent of $u$ , leading to contradiction . </p>
<p>Therefore , $u.d&lt;x_k.d&lt;x_k.f&lt;u.f$ , </p>
<p>Case 1 : $x_k.d<x_{k+1}.d<x_{k+1}.f<x_k.f$ -> $x_{k+1}$ is also a descendent of $u$ .</p>
<p>Case 2 : $x_k.d&lt;x_k.f&lt;x_{k+1}.d&lt;x_{k+1}.f$ : Impossible .</p>
</li>
<li><p>Strongly Connected Components (SCC)</p>
<ol>
<li><p>View : any directed graph can be viewed as a DAG of SCC</p>
</li>
<li><p>Find SCC ?  Kosaraju’s Algorithm</p>
<ol>
<li><p>Algorithm</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">dfs1</span>(G);<span class="comment">// compute the finishing time u.f</span></span><br><span class="line">G_=<span class="built_in">reverse_edge</span>(G);</span><br><span class="line"><span class="built_in">dfs2</span>(G_); <span class="comment">// In main loop , consider vertices in decreasing order of u.f</span></span><br><span class="line">Claim : Each dfs-<span class="function">tree in <span class="title">dfs2</span><span class="params">(G_)</span> is a SCC</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Proof</p>
<p>Intuition : find and “delete” sink components in dfs2 , like a-topological order in $G_$ ( i.e. topological order in $G$ ) </p>
<ol>
<li><p>Lemma : If we start DFS at a node in a sink component , we will visit precisely all vertices in this component .</p>
<p>Trivial .</p>
</li>
<li><p>Lemma (key) : node with largest $u.f$ belongs to a start component in $G$ (i.e. a sink component in $G_$)</p>
<p>Only need to prove the following lemma .</p>
</li>
<li><p>Lemma ( for proving key Lemma ) : $C,D$ are two SCC , $D$ is reachable from $C$ , </p>
<p>Then for $v\in C$ , which is the firstly visited vertex in $C$ , then $\forall u\in D,v.f&gt;u.f$</p>
<p>Proof : </p>
<p>Case 1 : $v$ is also the firstly visited vertex in $C\cup D$ , then by white-path THM , all nodes in $C\cup D$ are descendent of $v$ , so $\forall u\in C\cup D\backslash \{v\} , v.f&gt;u.f$ .</p>
<p>Case 2 : $\exists y\in D$ , $y$ is the firstly visited vertex in $C\cup D$ , so $v$ is not a descendent of $y$ since $C$ is not reachable from $D$ .</p>
<p>Therefore , by parenthesis THM , $y.d\le u.d&lt;u.f\le y.f&lt;v.d&lt;v.f$ for all $u\in D$ .</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Chapter-2-Greedy-Algorithm"><a href="#Chapter-2-Greedy-Algorithm" class="headerlink" title="Chapter 2 : Greedy Algorithm"></a>Chapter 2 : Greedy Algorithm</h2><h3 id="2-1-Interval-Scheduling"><a href="#2-1-Interval-Scheduling" class="headerlink" title="2.1 Interval Scheduling"></a>2.1 Interval Scheduling</h3><ol>
<li><p>Description </p>
<p>Input : $n$ jobs $s_i,f_i$</p>
<p>Goal : maximize the #jobs , s.t. at most one job at a time .</p>
</li>
<li><p>Another view</p>
<p>Connect all jobs pairs $[s_i,f_i],[s_j,f_j]$ if $[s_i,f_i]\cap [s_j,f_j]\neq \varnothing$ .</p>
<p>Goal $\Leftrightarrow$ maximum independent set .</p>
<p>In general graph : NP-hard for general graph .</p>
</li>
<li><p>Algorithm</p>
<p>Repeat : Select the available jobs that finishes first .</p>
</li>
<li><p>Proof of optimality :</p>
<p>Method : [Exchange Argument] : Compare our solution and the optimal solution .</p>
<p>SOL : $i_1,i_2,\cdots ,i_m$ , OPT : $j_1,j_2,\cdots,j_k$ .</p>
<ol>
<li>Claim : If $f_{i_1}\le f_{j_1}$ , then $f_{i_r}\le f_{j_r}$ for all $r\ge 1$ .</li>
</ol>
<p>Proof : [Induction]</p>
<p>If the claim is true for $r-1$ , suppose the claim is not true for $r$ , i.e. $f_{i_r}&gt;f_{j_r}$ . Since $f_{i_{r-1}}\le f_{j_{r-1}}$ , and $s_{j_r}&gt;f_{j_{r-1}}$ , then $f_{i_{r-1}}&lt;s_{j_r}$ , so $j_r$ is also available , but has earlier finish time , contradict .</p>
<ol>
<li>If $m&lt;k$ , then $f_{i_m}&lt;f_{j_m}$ . We can use $j_{m+1},\cdots,j_k$ after  $i_m$ .</li>
</ol>
</li>
</ol>
<h3 id="2-2-Scheduling-to-minimize-lateness"><a href="#2-2-Scheduling-to-minimize-lateness" class="headerlink" title="2.2. Scheduling to minimize lateness"></a>2.2. Scheduling to minimize lateness</h3><ol>
<li><p>Description</p>
<p>Input : $n$ jobs , each job $i$ has ddl $d_i$ and length $t_i$ . Def lateness : $l_i:=\max\{0,f_i-d_i\}$</p>
<p>Goal : find a schedule of all $n$ jobs , and minimize the maximal lateness .</p>
</li>
<li><p>Equal formularization</p>
<p>Goal : find a permutation $\{p_i\}\in S_n$ , then $f_i=\sum_{j=1}^i t_{p_j}$ , $l_i:=\max\{0,f_i-d_{p_i}\}$ . Minimize $\max\{l_i\}$ .</p>
<p>$\Leftrightarrow$ $l_i:=f_i-d_{p_i}$ , Minimize $\max\{0,\max\{l_i\}\}$ .</p>
</li>
<li><p>Algorithm</p>
<p>Schedule the job in increasing order of $d_i$ .</p>
</li>
<li><p>Proof of optimality</p>
<ol>
<li><p>Def (Inversion) : Consider a schedule $A’$ , $(i,j)$ is an inversion if $i$ is scheduled before $j$ but $d_i&gt;d_j$ .</p>
</li>
<li><p>If OPT$\neq $ SOL , there must be an inversion , then there must be an adjacent inversion . Suppose $(i,i+1)$ is an inversion , then $d_{p_i}&gt;d_{p_{i+1}}$ .</p>
<p>Let $f=\sum_{j=1}^{i-1}t_{p_j}$ , so $f_i=f+t_{p_i}$ , $f_{i+1}=f+t_{p_i}+t_{p_{i+1}}$ , so $l_i=f+t_{p_i}-d_{p_i}$ , $l_{i+1}=f+t_{p_i}+t_{p_{i+1}}-d_{p_{i+1}}$ .</p>
<p>If swap $(i,i+1)$ , then $f_{i+1}’=f+t_{p_{i+1}}$ , $f_{i}’=f+t_{p_{i+1}}+t_{p_i}$ , so $l_{i+1}’=f+t_{p_{i+1}}-d_{p_{i+1}}$ , $l_i’=f+t_{p_{i+1}}+t_{p_i}-d_{p_i}$ .</p>
<p>Therefore , obviously , $l_{i+1}’<l_{i+1}$ . Since $d_{p_i}>d_{p_{i+1}}$ , then $l_i’&lt;l_i$ . Therefore , swap can lead to better solution , so OPT is not optimal . </p>
</li>
</ol>
</li>
</ol>
<h3 id="2-3-Shortest-Path-without-w-lt-0"><a href="#2-3-Shortest-Path-without-w-lt-0" class="headerlink" title="2.3. Shortest Path (without $w&lt;0$)"></a>2.3. Shortest Path (without $w&lt;0$)</h3><ol>
<li><p>Description</p>
<p>Input : weighted graph $G=(V,E)$ , start vertex $s$ .</p>
<p>Output : $d(u)$ for all $u\in V$ , where $d(u)=\min_{p\text{ is a path }s\to u}\{\sum_{e\in p}l(e)\}$  .</p>
</li>
<li><p>Algorithm [Dijkstra 1959]</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Init</span></span><br><span class="line"><span class="keyword">for</span>(u in V)&#123;</span><br><span class="line">    d[u]=+inf;</span><br><span class="line">&#125;</span><br><span class="line">d[s]=<span class="number">0</span>; S.<span class="built_in">insert</span>(s);</span><br><span class="line"><span class="comment">// Main Algorithm</span></span><br><span class="line"><span class="keyword">while</span>(S != V)&#123;</span><br><span class="line">    v=<span class="built_in">argmin</span>(d[u]+<span class="built_in">l</span>(e) |v: v in V\S , e=(u,v) , u in S );</span><br><span class="line">    d[v]=d[u]+<span class="built_in">l</span>(e);</span><br><span class="line">    S.<span class="built_in">insert</span>(v);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Proof</p>
<p>Prove by induction on $S$ . $\forall v\in S , d(v)=\min dist(s,v)$ ,</p>
<p>Suppose we grow $S$ by adding $v$ , suppose the proposition does not hold . Then $d(u)+l_e$ is not the shortest distance to $v$ . Let $p$ be the shortest path from $s$ to $v$ .</p>
<p>Let $w$ be the last vertex that is in $S$ , then by induction , all vertices on $p$ from $s$ to $w$ are all in $S$ . Let $p’=path(s,u)+e$ .</p>
<p>$p’: s\to w\to u\to v$ , $p:s\to w\to x\to v$ . ($x\notin S$) . Since $d(u)+l_e$ is minimal , $d(u)+l_e\le d(w)+l_{w,x}$ , but $dist(p)=d(w)+l_{w,x}+dist(x,v)$ , where $dist(x,v)\ge 0$ , and $dist(p)&lt;dist(p’)=d(u)+l_e$ , so $d(w)+l_{w,x}\le dist(p)&lt;d(u)+l_e$ , contradict .</p>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-搜索</tag>
        <tag>算法-强连通分量(SCC)</tag>
        <tag>算法-贪心</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 1</title>
    <url>/2023/10/05/Algorithm-Design-1/</url>
    <content><![CDATA[<h2 id="Chapter-0-Logistics"><a href="#Chapter-0-Logistics" class="headerlink" title="Chapter 0 Logistics"></a>Chapter 0 Logistics</h2><p><strong>Content</strong> : discrete(combinatorial) algorithm  ,  Theoretical </p>
<ul>
<li>[minority] Complexity , NP-Completeness</li>
<li>Basic Graph Algorithm , DFS / BFS</li>
<li>Greedy</li>
<li>Dynamic Programming</li>
<li>Divide and Conquer</li>
<li>NP-completeness Theory</li>
<li>Approximation Algorithm</li>
<li>Randomized Algorithm (Probability Analysis)</li>
<li><ul>
<li>Computational Geometry</li>
</ul>
</li>
<li><ul>
<li>Streaming Algorithm (online)</li>
</ul>
</li>
</ul>
<p><strong>Textbook</strong> : [Kleinberg&amp;Tardos] Algorithm Design </p>
<p><strong>Reference Book</strong> : [CLRS] Intro to Algorithm</p>
<h2 id="Chapter-1"><a href="#Chapter-1" class="headerlink" title="Chapter 1"></a>Chapter 1</h2><h3 id="1-1-Stable-matching"><a href="#1-1-Stable-matching" class="headerlink" title="1.1 Stable matching"></a>1.1 Stable matching</h3><ol>
<li><p>Def</p>
<ul>
<li><p>Input : $boys=\{B_1,\cdots,B_n\} , girls=\{G_1,\cdots,G_n\}$</p>
<p>Preference List : $BP_i$ : a permutation of $girls$ , $GP_i$ : a permutation of $boys$</p>
</li>
<li><p>output : a stable matching </p>
</li>
<li><p>stable matching : no unstable pairs</p>
</li>
<li><p>unstable pair : $(B_i,G_j)$ s.t.  $M(B_i)$ after $G_j$ in $BP_i$ and $M(G_j)$ after $B_i$ in $GP_j$</p>
</li>
</ul>
</li>
<li><p>Efficient Algorithm : Gale &amp; Shapley Algorithm ( propose-reject )</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>( exists sb. single )&#123;</span><br><span class="line">    A &lt;- an arbitrary single boy</span><br><span class="line">    X &lt;- <span class="function">first girl A has <span class="keyword">not</span> proposed yet</span></span><br><span class="line"><span class="function">    <span class="title">if</span> <span class="params">( X is single )</span></span></span><br><span class="line"><span class="function">        A-X engage</span></span><br><span class="line"><span class="function">    <span class="keyword">else</span></span></span><br><span class="line"><span class="function">        <span class="title">if</span> <span class="params">( A is better than M(X) )</span></span></span><br><span class="line"><span class="function">            A-X engage</span></span><br><span class="line"><span class="function">        <span class="keyword">else</span></span></span><br><span class="line"><span class="function">            X reject A</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Analysis</p>
<ol>
<li><p>Proof of Termination </p>
<ol>
<li>For girl : once engaged , engaged forever </li>
<li>For one boy : If used all preference list : then all girls must be engaged</li>
</ol>
</li>
<li><p>Proof of Correctness</p>
<p>Prove by Contradiction</p>
<p>$(B_i,G_j)$ : an unstable pair</p>
<p>$B_i$ preference list : $\cdots G_j \cdots M(B_i)$</p>
<p>$G_j$ preference list : $\cdots B_i \cdots Z=M(G_j)$</p>
<p>$\therefore$ $G_j$ rejected $B_i$ , but $G_j$ should not reject $B_i$ compared with $Z$</p>
</li>
<li><p>Running Time : $\mathcal O(n^2)$ ( All boys used up their preference list)</p>
</li>
</ol>
</li>
<li><p>Random ver.</p>
<ol>
<li><p>def : $BP_i$ , $GP_i$ are all random permutations (uniformly distributed) </p>
</li>
<li><p>How to get a uniformly distributed random permutation ?</p>
<p>draw-likely process</p>
</li>
<li><p>THM : $\mathbb E[T]\le n\cdot H_n$  ($\mathbb E[T]=\mathcal O(n\log n)$)</p>
</li>
<li><p>Proof </p>
<ol>
<li><p>key observation : </p>
<p>G-S’ : each time a boy propose to a random girl not proposed yet</p>
<p><strong>This is equivalent as generate a uniformly distributed random permutation</strong></p>
<p>G-S’’ : each time a boy propose to a random girl (can be proposed yet)</p>
<p>$\therefore $ $T(G-S)=T(G-S’)\le T(G-S’’)$</p>
</li>
<li><p>Coupon Collector Problem ( Bins-Balls ) </p>
<p>$n$ bins , each time throw a ball to a random bin . </p>
<p>Q : $\mathbb E[\text{balls}]$ s.t. every bin is nonempty .</p>
<p>A : $\mathbb E[\text{balls}]=n\cdot H_n$</p>
<p>Construct Sequence $a_i\in \{0,1\}$ , $a_i=1\Leftrightarrow$ a ball falls in an empty bin</p>
<p>Exactly $n$ number of $1$s . -&gt; $n$ segments like $0\cdots 01$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb E[T]&=\mathbb E\left[\sum_{i=1}^n \text{len of i-th segment}\right]\\
&=\sum_{i=1}^n\mathbb E\left[ \text{len of i-th segment}\right]\\
&=\sum_{i=1}^n \frac{1}{\Pr\{\text{in i-th seg , choosed empty bin}\}}\\
&=\sum_{i=1}^n \frac{n}{n-i+1}\\
&=n\cdot H_n
\end{aligned}</script></li>
<li><p>Consider boy -&gt; ball , girl -&gt; bin</p>
<p>G-S’’ -&gt; Bins-Balls Problem</p>
</li>
<li><ul>
<li><p>Concentration inequality for Coupon Collection Running Time</p>
<p>Same as Chernoff Bound</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="1-2-BFS-amp-DFS"><a href="#1-2-BFS-amp-DFS" class="headerlink" title="1.2 BFS &amp; DFS"></a>1.2 BFS &amp; DFS</h3><ol>
<li><p>BFS</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// bfs</span></span><br><span class="line">q.<span class="built_in">clear</span>();</span><br><span class="line"><span class="keyword">for</span>(u in Vertices)&#123;</span><br><span class="line">    dep[u]=inf;</span><br><span class="line">    prev[u]=<span class="literal">NULL</span>;</span><br><span class="line">    col[u]=white;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dep[s]=<span class="number">0</span>;</span><br><span class="line">prev[s]=<span class="literal">NULL</span>;</span><br><span class="line">col[s]=grey;</span><br><span class="line">q.<span class="built_in">push</span>(s);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(!q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">    u=q.<span class="built_in">front</span>();q.<span class="built_in">pop</span>();</span><br><span class="line">    <span class="keyword">for</span>(v in <span class="built_in">Neighbour</span>(u) )&#123;</span><br><span class="line">        <span class="keyword">if</span>(col[v]==white)&#123;</span><br><span class="line">            dep[v]=dep[u]+<span class="number">1</span>;</span><br><span class="line">            prev[v]=u;</span><br><span class="line">            col[v]=grey;</span><br><span class="line">            q.<span class="built_in">push</span>(v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    col[v]=black;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>BFS Tree Property : no edges with depth difference $\ge 2$ .</p>
</li>
<li><p>DFS</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">dfs</span><span class="params">(u)</span></span>&#123;</span><br><span class="line">    ++time;</span><br><span class="line">    discover[u]=time;</span><br><span class="line">    col[u]=grey;</span><br><span class="line">    <span class="keyword">for</span>(v in <span class="built_in">Neighbour</span>(u) )&#123;</span><br><span class="line">        <span class="keyword">if</span>(col[v]==white)&#123;</span><br><span class="line">            <span class="built_in">dfs</span>(v)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    col[u]=black;</span><br><span class="line">    ++time;</span><br><span class="line">    finish[u]=time;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>DFS Tree Properties</p>
<p>Time stamp intervals</p>
<ol>
<li>non-crossing : only non-intersect / totally include</li>
<li>Tree Structure $\Leftrightarrow$ Time stamp intervals Structure </li>
</ol>
</li>
<li><p>Connectivity</p>
<ol>
<li><p>undirected graph : connected component</p>
</li>
<li><p>directed graph : strongly connected component (SCC)</p>
<p>every vertex can reach other vertex</p>
</li>
<li><p>directed acyclic graph (DAG)</p>
<p>i.e. no directed cycle</p>
<p>i.e. no SCC has $\ge 2$ vertices</p>
<ul>
<li>DAG has a topological order</li>
</ul>
</li>
<li><p>A useful view of directed graph : a DAG of SCC</p>
<p>a.k.a. 缩点</p>
</li>
<li><p>DAG has a topological order</p>
<p>get topological order : use bfs/dfs starting from $InDeg=0$ </p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-匹配</tag>
        <tag>算法-搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/10/05/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>测试文档</category>
      </categories>
      <tags>
        <tag>测试文档</tag>
      </tags>
  </entry>
</search>
