<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Probability and Statistics 4</title>
    <url>/2023/11/03/Probability-and-Statistics-4/</url>
    <content><![CDATA[<h2 id="Chapter-1-Background-in-Probability"><a href="#Chapter-1-Background-in-Probability" class="headerlink" title="Chapter 1 Background in Probability"></a>Chapter 1 Background in Probability</h2><h3 id="1-5-Properties-of-Integration"><a href="#1-5-Properties-of-Integration" class="headerlink" title="1.5 Properties of Integration"></a>1.5 Properties of Integration</h3><ol>
<li><p>Holder’s Inequality</p>
<ol>
<li><p>Def [ <strong><em>$L_p$ Norm</em></strong> ] : For $p\ge 1$ , we can define $L_p$ Norm :  $||f||_p=\left(\int |f|^p d\mu\right)^{1/p}$</p>
</li>
<li><p>THM [ <strong><em>Holder’s Inequality</em></strong> ] : $\forall p,q&gt;1$ , s.t. $\frac{1}{p}+\frac{1}{q}=1$ , then</p>
<script type="math/tex; mode=display">
\int |fg|d\mu \le ||f||_p||g||_q</script></li>
<li><p>Proof </p>
<blockquote>
<p>Lemma : If $p,q&gt;1,\frac{1}{p}+\frac{1}{q}=1$ , then $\forall x,y\ge 0,xy\le \frac{1}{p}x^p+\frac{1}{q}y^q$ .</p>
<p>Proof : </p>
<p>Fix $y$ , let $f(x)=\frac{1}{p}x^p-yx+\frac{1}{q}y^q$ , so $f’(x)=x^{p-1}-y$ </p>
<p>Therefore , $f(x)_{\min}=f(x=y^{1/(p-1)})=0$ , so $f(x)\ge 0$ . $\Box$</p>
</blockquote>
<p>For either $||f||_p=0$ or $||g||_q=0$ , that means $f=0$ a.e. or $g=0$ a.e. , so $\int |fg|d\mu=0$ </p>
<p>When $||f||_p\neq 0,||g||_q\neq 0$  , we can suppose that $||f||_p=||g||_q=1$ without loss of generality .</p>
<p>Therefore , using the lemma , </p>
<script type="math/tex; mode=display">
\begin{aligned}
\int |fg|d\mu&\le \int \left(\frac{1}{p}|f|^p+\frac{1}{q}|g|^q\right)d\mu\\
&=\frac{1}{p}||f||_p^p+\frac{1}{q}||g||_q^q\\
&=\frac{1}{p}+\frac{1}{q}\\
&=1\\
&=||f||_p||g||_q
\end{aligned}</script><blockquote>
<p>Remark : Here it is necessary to suppose $||f||_p=||g||_q=1$ , or otherwise we will get </p>
<script type="math/tex; mode=display">
\frac{1}{p}||f||_p^p+\frac{1}{q}||g||_q^q\ge ||f||_p||g||_q</script><p>Which is not what we want . </p>
<p>But exactly when $||f||_p=||g||_q=1$ , the above inequality is equality .</p>
</blockquote>
</li>
<li><p>Remark : When $p=q=2$ , Holder’s Inequality becomes <strong><em>Cauchy-Schwarz Inequality</em></strong>.</p>
<script type="math/tex; mode=display">
\left(\int |fg|d\mu\right)^2\le \left(\int f^2d\mu\right)\left(\int g^2d\mu\right)</script></li>
</ol>
</li>
<li><p>Convergence of functions</p>
<ol>
<li><p>Def [ <strong><em>converge a.e.</em></strong> ] : $f_1,f_2,\cdots$ is a sequence of functions . If $f_n$ <strong><em>converge to $f$ almost everywhere</em></strong> , it means that </p>
<script type="math/tex; mode=display">
\mu\left(\left\{w:\lim_{n\to\infty} f_n(w)\neq f(w)\right\}\right)=0</script></li>
<li><p>Def [ <strong><em>converge in measure</em></strong> ] : $f_1,f_2,\cdots$ is a sequence of functions . If $f_n$ <strong><em> converge to $f$ in measure</em></strong> , it means that </p>
<script type="math/tex; mode=display">
\forall \epsilon>0 , \lim_{n\to\infty}\mu\left(\left\{w:|f_n(w)-f(w)|\ge \epsilon\right\}\right)=0</script></li>
<li><p>Def [ <strong><em>almost uniform convergence</em></strong> ] : $f_1,f_2,\cdots$ is a sequence of functions . Suppose $f:E\to \mathbb R$ is a function . If $f_n$ <strong><em>converge to $f$ almost uniformly</em></strong> , it means that</p>
<p>$\forall \epsilon_1&gt;0$ , there exists a measurable set $D\subseteq E$ such that $\mu(D)&lt;\epsilon_1$ , such that </p>
<script type="math/tex; mode=display">
\forall \epsilon>0,\exists N>0,\forall n>N,x\in E\backslash D,|f_n(x)-f(x)|<\epsilon</script></li>
<li><p>THM [ <strong><em>Egovov’s THM</em></strong> ] : If the support set $E$ of $f$ with $\mu(E)&lt;\infty$ , then </p>
<p>$f_n\to f$ a.e. $\Rightarrow$ $f_n\to f$ almost uniformly</p>
</li>
<li><p>THM : $f_n\to f$ almost uniformly $\Rightarrow$ $f_n\to f$ in measure</p>
</li>
<li><p>Remarks</p>
<ol>
<li><p>the difference between convergences</p>
<ul>
<li>a.e. : 相当于逐点收敛，但每个点的收敛速度可能不一样</li>
<li>uniform : 相当于一致收敛，趋于无穷时存在的 $N$ 只和 $\epsilon$ 有关（而不依赖于 $w$ ），相当于衡量一致的收敛速度</li>
<li>in measure : 类似于一致收敛，但可以允许不收敛的地方依赖 $n$ </li>
</ul>
<p>converge almost uniform 强于 converge in measure</p>
<p>在 $\mu(E)&lt;\infty$ 时，converge a.e. 强于 converge almost uniformly</p>
</li>
<li><p>$\mu(E)=\infty$ , Egovov’s THM 可能不成立</p>
<p>$(\mathbb R,\mathcal R,\lambda)$ , $f_n(x)=\mathbb 1_{[n,n+1]}(x)$ </p>
<p>$f_n\to 0$ a.e. , but let $\epsilon=\frac{1}{2}$ , $\mu(\{w:|f_n(w)|&gt;\frac{1}{2}\})=1$ </p>
</li>
<li><p>Convergence of random variables</p>
<p>$\mu$ : probability measure , $f$ , $f_1,\cdots$ random variables</p>
<p>Def [ <strong><em>converge a.s.</em></strong> ] : $f_n\to f$ a.e. , then we say $f_n$ <strong><em>converges to $f$ almost surely</em></strong> .</p>
<p>Def [ <strong><em>converge in probability</em></strong> ] : $f_n\to f$ in measure , then we say $f_n$ <strong><em>converges to $f$ in probability</em></strong> , denote as $f_n\xrightarrow{P} f$.</p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>Bounded Convergence Theorem</p>
<ol>
<li><p>THM [ <strong><em>Bounded Convergence Theorem (BCT)</em></strong> ] </p>
<p>Condition : </p>
<p>(i)  $E\in \mathcal F$ , s.t. $\mu(E)&lt;\infty$ and $\forall n\ge 1,f_n(E^c)=0$ .</p>
<p>(ii) $\exists M&gt;0$ , $\forall n\ge 1$ , $|f_n(x)|\le M$</p>
<p>(iii) $f_n\to f$ in measure</p>
<p>Result : </p>
<script type="math/tex; mode=display">
\lim_{n\to\infty}\int f_n d\mu=\int fd\mu</script></li>
<li><p>Proof</p>
<p>$\forall \epsilon&gt;0$ , let $G_n=\{x:|f_n(x)-f(x)|&lt;\epsilon\}$ , let $B_n=\Omega-G_n$ </p>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad\left|\int f_nd\mu-\int fd\mu\right|\\
&\le \int |f_n-f|d\mu\\
&=\int_{G_n}|f_n-f|d\mu+\int_{B_n\cap \{|f|\le M+1\}}|f_n-f|d\mu+\int_{B_n\cap \{|f|>M+1\}}|f_n-f|d\mu\\
&\le \epsilon \mu(G_n)+3M\mu(B_n)+\int_{\{|f|\ge M+\frac{1}{2}\}} (|f|+M)d\mu\\
&\le \epsilon \mu(G_n)+3M\mu(B_n)+M\mu\left\{|f|\ge M+\frac{1}{2}\right\}+\int_{\{|f|\ge M+\frac{1}{2}\}}|f|d\mu
\end{aligned}</script><p>Since $f_n\to f$ in measure , $\mu(B_n)\to 0$ as $n\to \infty$ </p>
<p>If $\mu\{|f|\ge M+\frac{1}{2}\}\neq 0$ ,then as $n\to\infty$ , $\mu\{x:|f_n(x)-f(x)|\ge \frac{1}{2}\} \ge \mu\{|f|\ge M+\frac{1}{2}\}\not\to 0$ , contradict with $f_n\to f$ in measure </p>
<p>$\int_{\{|f|\ge M+\frac{1}{2}\}}|f|d\mu=0$ ( though I don’t know how to prove it , maybe here is a bug)</p>
</li>
</ol>
</li>
<li><p>Fatou’s Lemma</p>
<ol>
<li><p>Lemma [ <strong><em>Fatou’s Lemma</em></strong> ] : If $f_n\ge 0$ , then </p>
<script type="math/tex; mode=display">
\liminf_{n\to\infty} \int f_nd\mu\ge \int \left(\liminf_{n\to\infty}f_n\right)d\mu</script></li>
<li><p>Proof</p>
<p>Let $g_n(x)=\inf_{m\ge n} f_m(x)$ , so $f_n(x)\ge g_n(x)$ . And as $n\uparrow\infty$ , $g_n(x)\uparrow g(x)=\liminf\limits_{n\to\infty} f_n(x)$ .</p>
<p>Therefore , we only need to prove that </p>
<script type="math/tex; mode=display">
\lim_{n\to\infty} \int g_nd\mu \ge \int d\mu</script><p>Consider $E_m\uparrow \Omega$ , $E_1\subseteq E_2\subseteq\cdots$ , and $\bigcup_{m=1}^{\infty} E_i=\Omega$ , then</p>
<p>$\forall m&gt;0$ , $m$ fixed , $(g_n\land m)\mathbb 1_{E_m}\to (g\land m)\mathbb 1_{E_m}$ a.e. </p>
<p>Therefore , for any fixed $m&gt;0$ , </p>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad \lim_{n\to\infty}\int g_nd\mu\\
&\ge \lim_{n\to\infty} \int_{E_m}(g_n\land m)d\mu\\
&=\int_{E_m}(g\land m)d\mu\\
\end{aligned}</script><p>Therefore , </p>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad \lim_{n\to\infty}\int g_nd\mu\\
&\ge \sup_{m>0}\int_{E_m}(g\land m)d\mu\\
&=\lim_{m\to\infty} \int_{E_m}(g\land m)d\mu\\
&=\int gd\mu
\end{aligned}</script></li>
</ol>
</li>
<li><p>Monotone Convergence Theorem</p>
<ol>
<li><p>THM [ <strong><em>Monotone Convergence Theorem (MCT)</em></strong> ] </p>
<p>Condition :</p>
<p>(i) $f_n\ge 0$</p>
<p>(ii) $f_n\uparrow f$ a.e.</p>
<p>Result : </p>
<script type="math/tex; mode=display">
\int f_nd\mu \uparrow \int fd\mu</script></li>
<li><p>Proof</p>
<p>Since $f_n\uparrow f$ , $\limsup\limits_{n\to\infty} \int f_nd\mu\le \int fd\mu$.</p>
<p>By <strong><em>Fatou’s Lemma</em></strong> ,  </p>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad \liminf_{n\to\infty}\int f_nd\mu\\
&\ge \int \left(\liminf_{n\to\infty} f_n\right)d\mu\\
&=\int \left(\lim_{n\to\infty} f_n\right)d\mu\\
&=\int fd\mu
\end{aligned}</script><p>Therefore , $\lim\limits_{n\to\infty} \int f_nd\mu=\int fd\mu$ .</p>
</li>
</ol>
</li>
<li><p>Dominated Convergence Theorem</p>
<ol>
<li><p>THM [ <strong><em>Dominated Convergence Theorem (DCT)</em></strong> ]</p>
<p>Condition :</p>
<p>(i) $f_n\to f$ a.e.</p>
<p>(ii) There exists a function $g$ that $\forall n\ge 1,|f_n|\le g$ a.e.</p>
<p>(iii) $g$ is integrable , i.e. $\int |g|d\mu &lt;\infty$</p>
<p>Result :</p>
<script type="math/tex; mode=display">
\int f_n d\mu\to \int fd\mu</script></li>
<li><p>Proof</p>
<p>Since $|f_n|\le g$ , $f_n+g\ge 0$ a.e. and $-f_n+g\ge 0$ a.e.</p>
<p>By <strong><em>Fatou’s Lemma</em></strong> , </p>
<script type="math/tex; mode=display">
\begin{aligned}
\liminf_{n\to\infty} \int (f_n+g)d\mu\ge \int (f+g)d\mu \quad&\Rightarrow\quad \liminf_{n\to\infty} \int f_nd\mu \ge \int fd\mu\\
\liminf_{n\to\infty} \int (-f_n+g)d\mu\ge \int (-f+g)d\mu \quad&\Rightarrow\quad \limsup_{n\to\infty} \int f_nd\mu \le \int fd\mu\\
\end{aligned}</script></li>
</ol>
</li>
</ol>
<h3 id="1-6-Expected-Value"><a href="#1-6-Expected-Value" class="headerlink" title="1.6 Expected Value"></a>1.6 Expected Value</h3><ol>
<li><p>Basic concept of Expectation</p>
<ol>
<li><p>Def [ <strong><em>Expected Value</em></strong> ] : For $X\ge 0$ be a random variable on $(\Omega,\mathcal F,P)$ , its <strong><em>expected value</em></strong> is </p>
<script type="math/tex; mode=display">
E[X]=\int X dP</script><p>For general case , let $X^+=\max\{X,0\}$ , $X^-=\max\{-X,0\}$ .</p>
<p>Define $E[X]$ when $E[X^+]&lt;\infty$ or $E[X^-]&lt;\infty$ , as $E[X]=E[X^+]-E[X^-]$ .</p>
</li>
<li><p>Remarks</p>
<ol>
<li><p>The definition of expected value is a little bit bigger than <strong><em>integrable</em></strong> , since we also define $E[X]$ when $E[X^+]=\infty$ or $E[X^-]=\infty$ . But usually this does not matter.</p>
</li>
<li><p>We can construct an example that $E[X]=\infty$</p>
<p>$P(X=2^{j})=2^{-j}$ for integer $j\ge 1$ , then $E[X]=\sum_{j=1}^{\infty} 2^jP(X=2^j)=\sum_{j=1}^{\infty}1=\infty$</p>
</li>
<li><p>$E[X]$ is often called <strong><em>mean</em></strong> of $X$ , and denoted as $\mu$ (different from measure!).  </p>
</li>
</ol>
</li>
<li><p>Basic properties</p>
<ol>
<li>$E[X+Y]=E[X]+E[Y]$</li>
<li>$E[aX+b]=aE[X]+b$</li>
<li>If $X\ge Y$ , then $E[X]\ge E[Y]$</li>
</ol>
</li>
</ol>
</li>
<li><p>Inequalities</p>
<ol>
<li><p>THM [ <strong><em>Jensen’s Inequality</em></strong> ] : Suppose $\varphi$ is a convex , and $E[\varphi(X)],E[X]$ exist , then</p>
<script type="math/tex; mode=display">
E[\varphi(X)]\ge \varphi(E[X])</script><blockquote>
<p>Corollary :  $|E[X]|\le E[|X|]$ , $(E[X])^2\le E[X^2]$</p>
</blockquote>
</li>
<li><p>THM [ <strong><em>Holder’s Inequality</em></strong> ] : If $p,q\in [1,\infty]$ , $\frac{1}{p}+\frac{1}{q}=1$ , then</p>
<script type="math/tex; mode=display">
E|XY|\le ||X||_p||Y||_q</script><p>Here define $||X||_r=(E[X^r])^{1/r}$ for $r\in [1,\infty)$ ,</p>
<p>define $||X||_{\infty}=\inf\{M:P(|X|&gt;M)=0\}$  ( like the maximum )</p>
</li>
<li><p>THM [ <strong><em>Chebyshev’s Inequality</em></strong> ] : $\varphi :  \mathbb R\to \mathbb R$ , $\varphi\ge 0$ </p>
<p>Let $A\in \mathcal R$ , and let $i_A=\inf\{\varphi(y):y\in A\}$ . Therefore ,</p>
<script type="math/tex; mode=display">
i_A P(X\in A)\le E[\varphi(X)\mathbb 1(X\in A)]\le E[\varphi(X)]</script><p>Proof :</p>
<script type="math/tex; mode=display">
i_A\mathbb 1(X\in A)\le \varphi(X)\mathbb 1(X\in A)\le \varphi(X)</script></li>
<li><p>THM [ <strong><em>Chebyshev’s Inequality 2</em></strong> ] : Let $\varphi(x)=(x-\mu)^2$ , then</p>
<script type="math/tex; mode=display">
\Pr\{|X-\mu|\ge t\}=\frac{Var(X)}{t^2}</script></li>
<li><p>THM [ <strong><em>Markov’s Inequality</em></strong> ] : If $X\ge 0$ a.s. then </p>
<script type="math/tex; mode=display">
\Pr\{X\ge t\}\le \frac{E[X]}{t}</script></li>
</ol>
</li>
<li><p>Convergence</p>
<ol>
<li><p>THM [ <strong><em>Egovov’s Theorem</em></strong> ] : $X_1,X_2 \cdots$ is a sequence of random variables , then</p>
<p>$X_n\to X$ a.s. $\Rightarrow$ $X_n\to X$ in probability</p>
</li>
<li><p>THM [ <strong><em>Bounded Convergence Theorem</em></strong> ] :</p>
<p>Condition :</p>
<p>(i) $X_n\to X$ a.s.</p>
<p>(ii) $\exists M&gt;0$ , $\forall n\ge 1,|X_n|\le M$ </p>
<p>Result : $E[X_n]\to E[X]$</p>
</li>
<li><p>THM [ <strong><em>Fatou’s Lemma</em></strong> ] : If $X_n\ge 0$ , then </p>
<script type="math/tex; mode=display">
\liminf_{n\to\infty} E[X_n]\ge E\left[\liminf_{n\to\infty} X_n\right]</script></li>
<li><p>THM [ <strong><em>Monotone Convergence Theorem</em></strong> ] : </p>
<p>Condition :</p>
<p>(i) $X_n\ge 0$</p>
<p>(ii) $X_n\uparrow X$ a.s.</p>
<p>Result : $E[X_n]\uparrow E[X]$ a.s.</p>
</li>
<li><p>THM [ <strong><em>Dominated Convergence Theorem</em></strong> ] : </p>
<p>Condition : </p>
<p>(i) $X_n\to X$ a.s.</p>
<p>(ii) $\exists Y$ , $\forall n\ge 1$ , $|X_n|\le Y$</p>
<p>(iii) $E[|Y|]&lt;\infty$</p>
<p>Result : $E[X_n]\to E[X]$</p>
</li>
<li><p>THM </p>
<p>Condition : </p>
<p>(i) $X_n\to X$ a.s.</p>
<p>(ii) $g,h$ continuous functions</p>
<p>(iii) $g\ge 0$ , $g(x)\to\infty$ as $|x|\to\infty$ </p>
<p>(iv) $|h(x)|/g(x)\to 0$ as $|x|\to\infty$</p>
<p>(v) $\exists K&gt;0$ , s.t.  $E[g(X)]\le K&lt;\infty$</p>
<p>Result : $E[h(X_n)]\to E[h(X)]$ . </p>
<blockquote>
<p>Remark : similar to DCT , use $g$ to dominate $h$ </p>
<p>Proof : see book</p>
<p>Intuition : truncation , consider $\bar X=X\cdot \mathbb 1(|X|\le M)$ , $M$ to make $g(x)&gt;0$ .</p>
<script type="math/tex; mode=display">
E[h(X_n)]\to E[h(\bar X_n)]\to E[h(\bar X)]\to E[h(X)]</script></blockquote>
</li>
</ol>
</li>
<li><p>Computing Expected Value</p>
<ol>
<li><p>THM [ <strong><em>Change of variable formula</em></strong> ] : </p>
<p>Condition :</p>
<p>(i). $X$ is a random variable on $(S,\mathcal S)$ , with distribution $\mu$ ( i.e. $\mu(A)=P(X\in A)$ )</p>
<p>(ii). $f$ is a measurable function from $(S,\mathcal S)$ to $(\mathbb R,\mathcal R)$ </p>
<p>(iii). $f\ge 0$ or $E[|f(X)|]&lt;\infty$</p>
<p>Result :</p>
<script type="math/tex; mode=display">
E[f(X)]=\int_S f(y)\mu(dy)</script><blockquote>
<p>Remark : let $\mu=P\circ X^{-1}$ , so </p>
<script type="math/tex; mode=display">
\int f(X)dP=\int_S f(y)d(P\circ X^{-1})</script></blockquote>
</li>
<li><p>Further computation</p>
<p>When $X$ is a continuous random variable , we can use <strong>Radon-Nikodym derivative</strong> to represent <strong>PDF</strong> , so </p>
<script type="math/tex; mode=display">
\begin{aligned}
E[f(X)]&=\int_S f(y)d\mu\\
&=\int_\mathbb R f \frac{d\mu }{d\lambda} d\lambda\\
&=\int_{-\infty}^{\infty} f(x)p(x)dx
\end{aligned}</script></li>
<li><p>Momentum </p>
<ol>
<li><p>Def [ <strong><em>$k$-th momentum</em></strong> ] : If $k\in \mathbb N^<em>$ , then $E[X^k]$ is called the **</em>$k$-th moment of $X$<em>*</em> .</p>
</li>
<li><p>Def [ <strong><em>Variance</em></strong> ] : $Var(X)=E[(X-E[X])^2]$</p>
<p>Property : $Var(X)=E[X^2]-(E[X])^2$ </p>
</li>
</ol>
</li>
<li><p>Examples</p>
<ol>
<li><p>Def [ <strong><em>Bernoulli random variable</em></strong> ] : $X\in \{0,1\}$ , $\begin{cases}P(X=0)&amp;=1-p\\P(X=1)&amp;=p\end{cases}$ </p>
<p>$E[X]=p$ , $Var(X)=p(1-p)$</p>
</li>
<li><p>Def [ <strong><em>Poisson random variable</em></strong> ] : with parameter $\lambda$ , </p>
<script type="math/tex; mode=display">
P(X=k)=e^{-\lambda} \frac{\lambda^k}{k!} \quad k=0,1,2,\cdots</script><p>Property : </p>
<script type="math/tex; mode=display">
E[\prod_{i=0}^{k-1}(X-i)]=\lambda^k</script><p>$Var(X)=\lambda$</p>
</li>
<li><p>Def [ <strong><em>Geometric distribution</em></strong> ] : with success probability $p$ , </p>
<script type="math/tex; mode=display">
P(X=k)=p(1-p)^{k-1}</script><p>$E[X]=\frac{1}{p}$ , $Var(X)=\frac{1-p}{p^2}$ </p>
</li>
<li><p>Def [ <strong><em>Gaussian random variable / Normal distribution</em></strong> ] : with mean $\mu$ and variance $\sigma^2$ , </p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)</script><p>$E[X]=\mu$ , $Var[X]=\sigma^2$</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="1-7-Product-Measure-and-Fubini’s-Theorem"><a href="#1-7-Product-Measure-and-Fubini’s-Theorem" class="headerlink" title="1.7 Product Measure and Fubini’s Theorem"></a>1.7 Product Measure and Fubini’s Theorem</h3><ol>
<li><p>product of measure</p>
<ol>
<li><p>Definition</p>
<p>For measurable space $(X,\mathcal A,\mu_1)$ and $(Y,\mathcal B,\mu_2)$ , </p>
<p>Let $\Omega=X\times Y=\{(x,y):x\in X,y\in Y\}$</p>
<p>Let $\mathcal S=\{A\times B:A\in \mathcal A,B\in \mathcal B\}$ , so $\mathcal S$ is a semi-algebra since $(A\times B)^c=(A^c\times B)\cup (A\times B^c)\cup (A^c\times B^c)$ .</p>
<p>Let $\mathcal F=\sigma(\mathcal S)$ , that is the $\sigma$-field generated by $\mathcal S$ , denote as $\mathcal F=\mathcal A\times\mathcal B$ .</p>
</li>
<li><p>THM [ <strong><em>measure for measurable space product</em></strong> ] :</p>
<p>There is a unique measure $\mu$ on $\mathcal F$ , s.t. $\mu(A\times B)=\mu_1(A)\mu_2(B)$ .</p>
<p>(*) Proof :</p>
<p>By the property of $\sigma$-field generation , we only need to prove that :</p>
<p>If $A\times B=+_i (A_i\times B_i)$ is a finite or countable disjoint union , then</p>
<script type="math/tex; mode=display">
\mu(A\times B)=\sum_{i} \mu(A_i\times B_i)</script><p>For each $x\in A$ , let $I(x)=\{i:x\in A_i\}$ , firstly consider $B=+_{i\in I(x)}B_i$ , so</p>
<script type="math/tex; mode=display">
\mathbb 1_A(x)\mu_2(B)=\sum_{i}\mathbb 1_{A_i}(x)\mu_2(B_i)</script><p>Integrate it with $\mu_1$ , so</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int \mathbb 1_A(x)\mu_2(B)d\mu_1&=\int \sum_{i}\mathbb 1_{A_i}(x)\mu_2(B_i)d\mu_1\\
\iff\quad\quad\quad\mu_1(A)\mu_2(B)&=\sum_{i}\mu_2(B_i)\int\mathbb 1_{A_i}(x)d\mu_1\\
\iff\quad\quad\quad\mu_1(A)\mu_2(B)&=\sum_{i}\mu_1(A_i)\mu_2(B_i)\\
\end{aligned}</script></li>
<li><p>Remark </p>
<ol>
<li><p>$\mu$ is often denoted as $\mu=\mu_1\times \mu_2$</p>
</li>
<li><p>We can generate this result to $n$ measurable space product , so </p>
<p>Consider measurable space $(\Omega_i,\mathcal F_i,\mu_i)$ , Let $\Omega=\times_{i=1}^n \Omega_i$ , $\mathcal F=\times_{i=1}^n \mathcal F_i$ , so there is a unique measure $\mu$ that for $A=\times_{i=1}^n A_i$ , where $A_i\in \mathcal F_i$ , </p>
<script type="math/tex; mode=display">
\mu(\times_{i=1}^n A_i)=\prod_{i=1}^n \mu_i(A_i)</script></li>
<li><p>When $(\Omega_i,\mathcal F_i,\mu_i)=(\mathbb R,\mathcal R,\lambda)$ , then $\mu$ is the <strong><em>Lebesgue measure</em></strong> on the Borel subsets of $\mathbb R^n$ </p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>Fubini’s Theorem</p>
<ol>
<li><p>THM [ <strong><em>Fubini’s Theorem</em></strong> ] : For measurable space $(X,\mathcal A,\mu_1)$ and $(Y,\mathcal B,\mu_2)$. If $f\ge 0$ or $\int |f|d\mu&lt;\infty$ , then</p>
<script type="math/tex; mode=display">
\int_X\int_Y f(x,y)\mu_2(dy)\mu_1(dx)=\int_{X\times Y} fd\mu=\int_Y\int_X f(x,y)\mu_1(dx)\mu_2(dy)</script></li>
<li><p>(*) Proof </p>
<p>Firstly , we need to make sure that </p>
<ul>
<li>Fixing $x$ , $y\to f(x,y)$ is a measurable map on $\mathcal B$</li>
<li>$x\to\int_Y f(x,y)\mu_2(dy)$ is a measurable map on $\mathcal A$</li>
</ul>
<p>We have the following lemma , dealing with $f=\mathbb 1_{E}$ </p>
<ul>
<li><p>Lemma 1 : Let $E_x=\{y:(x,y)\in E\}$ . If $E\in \mathcal A\times \mathcal B$ , then $E_x\in \mathcal B$</p>
</li>
<li><p>Lemma 2 : If $E\in \mathcal A\times \mathcal B$ , then $g(x):=\mu_2(E_x)$ is a measurable map on $\mathcal A$ , and </p>
<script type="math/tex; mode=display">
\int_X gd\mu_1=\mu(E)</script></li>
</ul>
<p>By these lemma , we can prove Fubini’s Theorem on $f=\mathbb 1_E$ for any $E\in \mathcal A\times \mathcal B$</p>
<p>Using the linearity of integration , we can prove Fubini’s Theorem holds for all simple functions.</p>
<p>For non-negative function , we can let $f_n(x,y)=([2^nf(x,y)]/2^n)\land n$ , so $f_n$ is simple function and $f_n\uparrow f$ . By MCT , Fubini’s Theorem holds for all non-negative functions.</p>
<p>For general function , we can compute $f=f^+-f^-$ each . Then Fubini’s Theorem holds for all integrable functions.</p>
</li>
<li><p>Remarks</p>
<ol>
<li><p>Toneli’s Theorem : proved that Fubini’s Theorem holds for $f\ge 0$ .</p>
</li>
<li><p>When $f$ is not non-negative and not integrable , Fubini’s Theorem may fail :</p>
<p>Let $X=Y=\mathbb N^<em>$ , $\mathcal A=\mathcal B=\{S:S\subseteq \mathbb N^</em>\}$ , $\mu_1=\mu_2=\text{counting measure}$ .</p>
<p>Let $f(m,n)=\begin{cases}1&amp;m=n\-1&amp;m=n+1\\0&amp;otherwise\end{cases}$ for all $m,n\ge 1$ , so </p>
<script type="math/tex; mode=display">
\sum_m\sum_n f(m,n)=1,\sum_{n}\sum_{m}f(m,n)=0</script></li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Chapter-2-Laws-of-Large-Numbers"><a href="#Chapter-2-Laws-of-Large-Numbers" class="headerlink" title="Chapter 2 Laws of Large Numbers"></a>Chapter 2 Laws of Large Numbers</h2><h3 id="2-1-Independence"><a href="#2-1-Independence" class="headerlink" title="2.1 Independence"></a>2.1 Independence</h3><ol>
<li>Definition ( for probability space $(\Omega,\mathcal F,P)$ )<ol>
<li>Def [ <strong><em>Independence of Events</em></strong> ] : Let $A,B\in \mathcal F$ , $A$ and $B$ are <strong><em>independent</em></strong> if $P(A\cap B)=P(A)P(B)$, denote as $A\perp!!!\perp B$.</li>
<li>Def [ <strong><em>Independence of Random Variables</em></strong> ] : Let $X,Y$ be two random variables , $X$ and $Y$ are <strong><em>independent</em></strong> if $\forall C,D\in \mathcal R$ , $P(\{X\in C\}\cap \{Y\in D\})=P(\{X\in C\})P(\{Y\in D\})$, denote as $X\perp!!!\perp Y$.</li>
<li>Def [ <strong><em>Independence of $\sigma$-field</em></strong> ] : Two $\sigma$-field $\mathcal F,\mathcal G$ are <strong><em>independent</em></strong> if $\forall A\in \mathcal F$ , $\forall B\in \mathcal G$ , $A,B$ are independent, denote as $\mathcal F\perp!!!\perp \mathcal G$.</li>
</ol>
</li>
<li>THM [ <strong><em>Independence of r.v. is a special case of $\sigma$-field</em></strong> ] <ol>
<li>If random variables $X,Y$ are independent , then $\sigma(X),\sigma(Y)$ are independent</li>
<li>If $\mathcal F$ and $\mathcal G$ are independent , $X\in \mathcal F,Y\in \mathcal G$ , then $X,Y$ are independent</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>概率与统计</category>
      </categories>
      <tags>
        <tag>实分析-积分性质</tag>
        <tag>实分析-积分收敛定理</tag>
        <tag>概率论-期望</tag>
        <tag>概率论-期望-期望性质</tag>
        <tag>概率论-期望-换元公式</tag>
        <tag>实分析-测度积</tag>
        <tag>实分析-Fubini定理</tag>
        <tag>概率论-独立性</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 7</title>
    <url>/2023/10/31/Algorithm-Design-7/</url>
    <content><![CDATA[<ol>
<li><p>NP-Hardness</p>
<ul>
<li><p>NP Completeness : Decision Problem </p>
</li>
<li><p>NP Hardness : Optimization Problem</p>
</li>
<li><p>Problem in <strong>NP-Hard</strong> : not in <strong>NPC</strong> , but the decision version is <strong>NPC</strong></p>
</li>
</ul>
</li>
<li><p>How to almost solve <strong>NP-Hard</strong> problem ?</p>
<ul>
<li>Special Restrictions</li>
<li>Approximation</li>
<li>Randomization</li>
</ul>
</li>
</ol>
<h2 id="Chapter-5-Extending-the-Limits-of-Tractability"><a href="#Chapter-5-Extending-the-Limits-of-Tractability" class="headerlink" title="Chapter 5 Extending the Limits of Tractability"></a>Chapter 5 Extending the Limits of Tractability</h2><p>Solving <strong>NP-Hard</strong> problem under some special restrictions</p>
<ol>
<li><p>Max Independent Set</p>
<ul>
<li>general graphs : NP-Hard</li>
<li>interval graph : P</li>
<li>tree / graph with bounded tree width : P</li>
</ul>
</li>
<li><p>FPT (fixed parameter tractable)</p>
<p>Intuition : Consider the parametrization of an instance</p>
<p>Def [ <strong><em>FTP</em></strong> ]: Input size $n$ , parameter $k$ . A problem is <strong><em>FPT</em></strong> if there exists an algorithm that solve the problem in $\mathcal O(f(k)poly(n))$.</p>
<blockquote>
<p>$k$ can be viewed similar to constant , $\mathcal O(2^kpoly(n))$ is still FPT , but $\mathcal O(n^k)$ is not FPT ( $poly(n)$ is independent from $k$ ).</p>
</blockquote>
</li>
<li><p>Vertex Cover , $k=|VC|$ </p>
<blockquote>
<p>We want to construct an algorithm in $\mathcal O(f(k)poly(n))$ , which is better than trivial algorithm in $\mathcal O(n^{k})$ .</p>
</blockquote>
<ol>
<li><p>Key Observation : $e=(u,v)$ , $VC(G)\le k\iff VC(G-u)\le k-1\lor CV(G-v)\le k-1$ .</p>
</li>
<li><p>Algorithm : check $(G,k)$</p>
<ol>
<li>If $G$ has no edge , then the empty set is a VC , return <code>true</code> .</li>
<li>If $|E|&gt; k|V|$ , then $\not\exists VC,|VC|\le k$ , return <code>false</code> .</li>
<li>Let $e=(u,v)\in E$ , return <code>check(G-u,k-1)||check(G-v,k-1)</code> .</li>
</ol>
</li>
<li><p>Running time : By recursive tree , at most $k$ levels , level $i$ has $2^{i-1}$ possibilities , with $\mathcal O(n(k-i+1))$ computation in each node.</p>
<p>Total time : $\mathcal O(2^kkn)$ .</p>
</li>
<li><p>Best FPT : $\mathcal O(1.2738^kpoly(n))$ .</p>
<p>Method : reduce more in Key Observation (e.g. $k\to k-1$ improve to $k\to k-2$) by considering more edges.</p>
<p>Kernel method : $\mathcal O(|E|+(5^{1/4})^k k^2)$ .</p>
</li>
</ol>
</li>
</ol>
<h2 id="Chapter-6-Approximation-Algorithm"><a href="#Chapter-6-Approximation-Algorithm" class="headerlink" title="Chapter 6  Approximation Algorithm"></a>Chapter 6  Approximation Algorithm</h2><h3 id="6-1-Approximation-Criteria"><a href="#6-1-Approximation-Criteria" class="headerlink" title="6.1 Approximation Criteria"></a>6.1 Approximation Criteria</h3><ol>
<li><p>Approximation ration $\alpha\ge 1$</p>
<p>minimization problem : $\alpha:=\sup_I \frac{SOL(I)}{OPT(I)}$.</p>
<p>maximization problem : $\alpha:=\sup_I \frac{OPT(I)}{SOL(I)}$.</p>
</li>
<li><p>Some examples</p>
<ul>
<li>Exact solution : $\alpha=1$</li>
<li>Constant approximation : $\alpha=\mathcal O(1)$</li>
<li>Logarithm approximation : $\alpha=\mathcal O(\log n)$</li>
<li>$\alpha=\mathcal O(n)$ : usually not interesting.</li>
<li>Poly-Time Approximation Scheme ( <strong><em>PTAS</em></strong> ) : $\alpha=1+\epsilon$ , Time : poly when $\epsilon$ is constant. </li>
<li>Fully Poly-Time Approximation Scheme ( <strong><em>FPTAS</em></strong> ) : Time : $\mathcal O(poly(\frac{1}{\epsilon},n))$.</li>
</ul>
</li>
</ol>
<h3 id="6-2-Load-Balancing-Problem"><a href="#6-2-Load-Balancing-Problem" class="headerlink" title="6.2 Load Balancing Problem"></a>6.2 Load Balancing Problem</h3><ol>
<li><p>Description : </p>
<p>Input : $n$ jobs , $t_i$ : time for job $i$ , $m$ machines </p>
<p>Output : assign each job to one machine , say machine $j$ is assigned jobs $S_j$ , minimize $\max_{j=1}^m \left\{\sum_{i\in S_j}t_i\right\}$ .</p>
</li>
<li><p><strong>Load Balancing Problem</strong> is <strong>NP-Hard</strong></p>
<p>Prove 1 : <strong>special subset sum</strong> $\le_p$ <strong>Load Balancing Problem</strong> </p>
<p><strong>special subset sum</strong> : $W=\frac{\sum_{i=1}^n w_i}{2}$ . This is still <strong>NPC</strong>.</p>
<p>Prove 2 : <strong>3-partition problem</strong> $\le_p$ <strong>Load Balancing Problem</strong></p>
<p><strong>3-partition problem </strong> : </p>
<pre><code>Given $3n$ numbers $a_1,\cdots,a_&#123;3n&#125;$ , partition them into $n$ groups , with each group $3$ numbers.

Decide whether we can find a partition , s.t. the sum of each group is the same.
</code></pre><p>Theorem : <strong>3-partition problem</strong> is <strong>NPC</strong> ( even if $a_i$ is poly-sized ) </p>
</li>
<li><p>Greedy Algorithm 1</p>
<p>Process jobs in arbitrary order one-by-one . Assign the job to the machine with current minimal load.</p>
<ol>
<li><p>Analysis : $\alpha=2$.</p>
</li>
<li><p>Proof : </p>
<p>load of OPT : $T^*$.</p>
<ul>
<li>Observation 1 : $T^*\ge \max_{i=1}^n t_i$.</li>
<li>Observation 2 : $T^*\ge \frac{1}{m}\sum_{i=1}^n t_i$.</li>
</ul>
<p>Let’s prove that $SOL\le \max_{i=1}^n t_i+\frac{1}{m}\sum_{i=1}^n t_i$ , so $SOL\le 2T^*$.</p>
<p>Let $SOL=\sum_{i\in S_j} t_i$ , and $t_k$ is the last job assigned to machine $j$ . Therefore , we can prove that before assigning $t_k$ , the load of machine $j$ is $\le \frac{1}{m}\sum_{i=1}^m t_i$ ( since machine $j$ with current minimal load ) . $t_k\le \max_{i=1}^n t_i$ . Therefore , $SOL\le \frac{1}{m}\sum_{i=1}^m t_i +\max_{i=1}^n t_i$.</p>
</li>
<li><p>Worst case :</p>
<p> $n=m(m-1)+1$ , with $m(m-1)$ jobs $t_j=1$ , and $1$ job $t_j=m$. </p>
<p>Then $SOL=2m-1$ , but $OPT=m$. $\alpha=\frac{2m-1}{m}\to 2$ .</p>
</li>
</ol>
</li>
<li><p>Greedy Algorithm 2 ( improvement )</p>
<p>Process jobs in decreasing order.</p>
<ol>
<li><p>Analysis : $\alpha=1.5$ ( not tight )</p>
</li>
<li><p>Proof : </p>
<ol>
<li><p>Case 1 : $n\le m$ , $OPT=SOL$</p>
</li>
<li><p>Case 2 : $n&gt;m$ , so $t_k\le t_{m+1}\le \frac{OPT}{2}$.</p>
<p>$t_{m+1},t_i$ must be both in one machine for some $1\le i\le m$.</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="6-3-k-Center-Problem"><a href="#6-3-k-Center-Problem" class="headerlink" title="6.3 $k$-Center Problem"></a>6.3 $k$-Center Problem</h3><ol>
<li><p>Description</p>
<p>Input : a metric graph $G=(V,E,d)$ , number of centers $k$.</p>
<p>Def [ <strong><em>metric graph</em></strong> ] : $d$ satisfies $\begin{cases}d(u,u)=0&amp;u\in V\\d(u,v)=d(v,u)&amp;u,v\in V\\d(s,t)\le d(s,w)+d(w,t)&amp;s,t,w\in V\end{cases}$.</p>
<p>Output : Choose $k$ centers $C=\{c_1,\cdots,c_k\}\subseteq V$. Minimize max cluster radius.</p>
<script type="math/tex; mode=display">
d(v,C):=\min_{i=1}^k \{d(v,c_i)\}</script><p>Minimize $\max_{v\in V}d(v,C)$.</p>
</li>
<li><p>Remark : Applications in ML</p>
<p>unsupervised learning.</p>
<p>$k$-means : minimize $\sum_{v\in V}d(v,C)^2$.</p>
<p>$k$-median : minimize $\sum_{v\in V}d(v,C)$.</p>
</li>
<li><p>Greedy Algorithm with $\alpha=2$</p>
<ol>
<li><p>Algorithm</p>
<ol>
<li><p>Guess optimal radius $r$ ( at most $|E|$ possibilities )</p>
</li>
<li><p>Choose an arbitrary vertex $v\in S$ , $C\gets C\cup\{v\}$</p>
</li>
<li><p>Let $D=\{u\in S:d(u,v)\le 2r\}$ , $S\gets S\backslash D$</p>
</li>
<li><p>If $S\neq \varnothing$ , go to (2)</p>
</li>
<li><p>Finally , if $|C|&gt;k$ , our guess is too small ( fail )</p>
<p>If $|C|\le k$ , our guess succeed.</p>
</li>
</ol>
<p>Hopefully , we want if we success at $r$ , then $OPT\ge r$ and $SOL\le 2r$ .</p>
</li>
<li><p>Proof </p>
<p>Key Observation : If $r\ge OPT$ , then we will definitely success.</p>
<p>Suppose $C_{OPT}=\{c_1,\cdots,c_{opt}\}$ , and $b_v=\arg\min_{1\le i\le OPT} d(v,c_i)$ .</p>
<p>In each step , suppose that we choose $v$ , then $\forall u\in V,b_u=b_v$ , $d(u,v)\le d(u,c_{b_v})+d(c_{b_v},v)\le 2r$ . Therefore , $\{u:b_u=b_v\}\subseteq D_{v}$ .</p>
<p>That is , each step we can remove at least one optimal cluster .</p>
</li>
</ol>
</li>
<li><p>$\alpha=2$ is the best result</p>
<p>Inapproximability : $\forall \alpha&lt;2$ , this problem is NP-Hard .</p>
<p>Promise Problem : For a given instance , such that the optimal value is $\le h$ or $&gt; \alpha h$ . Decide which case it is.</p>
<p>Claim : Any $\alpha$-approximation algorithm can be used to solve Promise Problem.</p>
<blockquote>
<p>Approximation answer : $\le \alpha h$  , then $OPT\le \alpha h$ , then $OPT\le h$ , output <code>true</code></p>
<p>Approximation answer : $&gt;\alpha h $ , then $OPT&gt;h$ , then $OPT&gt;\alpha h$ , output <code>false</code></p>
</blockquote>
</li>
<li><p>THM : Promise $k$-center for any $\alpha&lt;2$ is NPC</p>
<p>Proof : Dominating Set $\le_p$ Promise $k$-center for $\alpha&lt;2$ </p>
<p>$G$ for Dominating Set , construct $G’$ for Promise $k$-center for $\alpha&lt;2$</p>
<p>$G’$ : $E’$ complete graph , $w(e)=\begin{cases}1&amp;e\in E\\2&amp;e\notin E\end{cases}$</p>
<p>$(G,k)\in DS\iff$ the answer of $G’$ is <code>true</code> for $(G’,k,h=1)$ .</p>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>计算理论</tag>
        <tag>计算理论-NP-Hard</tag>
        <tag>算法-近似算法</tag>
        <tag>算法-近似算法-负载平衡问题</tag>
        <tag>算法-近似算法-聚类问题</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 6</title>
    <url>/2023/10/31/Algorithm-Design-6/</url>
    <content><![CDATA[<h2 id="Chapter-4-NP-Completeness"><a href="#Chapter-4-NP-Completeness" class="headerlink" title="Chapter 4 NP Completeness"></a>Chapter 4 NP Completeness</h2><h3 id="4-4-Other-Problems-in-NPC"><a href="#4-4-Other-Problems-in-NPC" class="headerlink" title="4.4 Other Problems in NPC"></a>4.4 Other Problems in NPC</h3><p>Intuition : Find problems $L\in NP$ that $SAT\le_p L$ . </p>
<h4 id="4-4-1-3-SAT"><a href="#4-4-1-3-SAT" class="headerlink" title="4.4.1  3-SAT"></a>4.4.1  3-SAT</h4><ol>
<li><p>Description : </p>
<ol>
<li><p>Def [ <strong><em>literal</em></strong> ] : For $\vec x=(x_1,\cdots,x_n)\in \{0,1\}^n$ , <strong><em>literal</em></strong> is an element of $X=\{x_i|i\in [n]\}\cup\{\bar x_i|i\in [n]\}$ .</p>
<p>Def [ <strong><em>CNF-clause</em></strong> ] : A <strong><em>CNF-clause</em></strong> $c$ is a formula with form :</p>
<script type="math/tex; mode=display">
c=\lor_{j=1}^m a_j</script><p>where $a_1,\cdots,a_m$ are distinct literals .</p>
<p>Def [ <strong><em>CNF</em></strong> ] : A <strong><em>conjunctive normal formula (CNF)</em></strong> is a formula $\{0,1\}^n\to \{0,1\}$ with following form :</p>
<script type="math/tex; mode=display">
f(\vec x)=\land_{i=1}^k c_i(\vec x)</script><p>where $c_i(\vec x)$ is a CNF-clause .</p>
<p>Def [ <strong><em>3-CNF</em></strong> ] : A <strong><em>3-CNF</em></strong> is a CNF with all clauses having exactly $3$ literals . $c_i=a_{i,1}\lor a_{i,2}\lor a_{i,3}$ , where $a_{i,1},a_{i,2},a_{i,3}$ are distinct literals .</p>
</li>
<li><p>Input : a 3-CNF </p>
<p>Output : Decide whether there exists an assignment for $\vec x$ such that $f(\vec x)=1$ .</p>
</li>
</ol>
</li>
<li><p><strong>3-SAT</strong> is <strong>NPC</strong></p>
<p>Obviously , <strong>3-SAT</strong> is <strong>NP</strong> .</p>
<p>Goal : <strong>SAT</strong> $\le_p $ <strong>3-SAT</strong> </p>
<p>Given a SAT instance $I$ , we construct in poly-time a 3-SAT instance $I’$ , s.t. $I\in SAT\iff I’\in 3-SAT$ .</p>
<blockquote>
<p><strong><em>Cook reduction</em></strong> : can use oracle poly times<br><strong><em>Karp reduction</em></strong> : only use oracle once </p>
<p>Originally , Cook define NPC by Cook reduction , but almost always Karp reduction is enough (though stronger than Cook’s) ,</p>
</blockquote>
<p>For each node $v\in I$ , construct a variable $x_v$ in $I’$ </p>
<ul>
<li>If $v=\lnot$ with child $u$ , we need $x_v=\overline{x_u}$ , so add to $I’$ a clause $(x_v\lor x_u)\land (\overline{x_v}\lor\overline{x_u})$ .</li>
<li>If $v=\lor$ with children $u,w$ , we need $x_v=x_u\lor x_w$ , so add to $I’$ a clause $(x_v\lor \bar x_u)\land (x_v\lor \bar x_w)\land (\bar x_v\lor x_u\lor x_w)$ .</li>
<li>If $v=\land$ with children $u,v$ , we need  $x_v=x_u\land x_w$ , so add to $I’$ a clause $(\bar x_v\lor x_u)\land (\bar x_v\lor x_w)\land (x_v\lor \bar x_u\lor \bar x_w)$ .</li>
<li>If $v=0/1$ , add to $I’$ a clause $\bar x_v$ or $x_v$ .</li>
</ul>
<p>Extend each clause into $3$ literals : $(a\lor b)=(a\lor b\lor c)\land (a\lor b\lor \bar c)$ </p>
<p>Claim : $I\in SAT\iff I’\in 3-SAT$ </p>
</li>
</ol>
<h4 id="4-4-2-Independent-Set-IS"><a href="#4-4-2-Independent-Set-IS" class="headerlink" title="4.4.2 Independent Set ( IS )"></a>4.4.2 Independent Set ( <strong>IS</strong> )</h4><p>Obviously , <strong>IS</strong> is <strong>NP</strong></p>
<p>Goal : <strong>3-SAT</strong> $\le_p$ <strong>IS</strong></p>
<ol>
<li><p>Construction</p>
<p>Given <strong>3-SAT</strong> instance $I$ , construct an <strong>IS</strong> instance $I’=(G,k)$ , s.t.  $(G,k)\in$ <strong>IS</strong> $\iff I\in $ <strong>3-SAT</strong> .</p>
<ul>
<li>Each clause $x_{i,1}\lor x_{i,2}\lor \bar x_{i,3}$ , construct a 3-cycle $(x_{i,1} , x_{i,2} , \bar x_{i,3})$ (finally $3n$ nodes , $3n$ edges). </li>
<li>Then connecting $x_i$ and $\bar x_i$ for all $i$ .</li>
</ul>
<blockquote>
<p>In each triangle , we can choose at most $1$ node .</p>
</blockquote>
</li>
<li><p>Claim : $(G,k)\in$ <strong>IS</strong> $\iff I\in $ <strong>3-SAT</strong>  ( $k$ is the number of clauses )</p>
<p>Proof : </p>
<ol>
<li>If $I\in $ <strong>3-SAT</strong> , then there is a valid assignment $\vec x$ . Each clause has at least one <code>true</code> , so we choose that literal in the clause , this is an independent set in $G$ with size $k$ .</li>
<li>If $(G,k)\in $ <strong>IS</strong>  , for each literal $x_i$ ($\bar x_i$) chosen , let $x_i=1$ ( $x_i=0$ ) . The rest of variables are assigned arbitrarily . This assignment is a valid assignment for $I$ , so $I\in$ <strong>3-SAT</strong> . </li>
</ol>
</li>
<li><p>Cor : Vertex Coloring , Set Coloring $\in$ NPC</p>
</li>
</ol>
<h4 id="4-4-3-Hamiltonian-Cycle-HC"><a href="#4-4-3-Hamiltonian-Cycle-HC" class="headerlink" title="4.4.3 Hamiltonian Cycle ( HC )"></a>4.4.3 Hamiltonian Cycle ( <strong>HC</strong> )</h4><ol>
<li><p>Description</p>
<ol>
<li>Input : A directed/undirected graph</li>
<li>Output : Decide whether there exists a cycle visiting each vertex exactly once</li>
</ol>
<blockquote>
<p><strong>TSP</strong> : weighted , determine the shortest Hamiltonian Cycle </p>
<p><strong>TSP</strong> is <strong>NP-Hard</strong></p>
</blockquote>
</li>
<li><p><strong>HC</strong> is <strong>NPC</strong></p>
<p>Obviously , <strong>HC</strong> is <strong>NP</strong></p>
<p>Goal : <strong>3-SAT</strong> $\le_p$ <strong>HC</strong></p>
<ol>
<li><p>Construction</p>
<p>Consider an instance $I$ for <strong>3-SAT</strong> , construct $G$ , s.t. $G\in$ <strong>HC</strong> $\iff I\in $ <strong>3-SAT</strong> . </p>
<p>Suppose $I$ has $n$ variables , $k$ clauses . </p>
<ul>
<li><p>$S,T\in V$ , $(T,S)\in E$</p>
</li>
<li><p>$n$ lines ($P_i$) : on line $i$ : $x_{i,0/1}\in V$ , $v_{i,j,0/1/2}\in V$ , $1\le j\le k$ , $(v_{i,j,0},v_{i,j,1}),(v_{i,j,1},v_{i,j,2}),(v_{i,j,2},v_{i,j+1,0})\in E$ </p>
<p>$(x_{i,0},v_{i,1,0}) , (v_{i,k,2},x_{i,1})\in E$</p>
</li>
<li><p>Between two lines :  $(x_{i,0},x_{i+1,0}) , (x_{i,0},x_{i+1},1) , (x_{i,1},x_{i+1,0}),(x_{i,1},x_{i+1,1})\in E$ </p>
<p>$(S,x_{1,0}),(S,x_{1,1}),(x_{n,0},T),(x_{n,1},T)\in E$</p>
</li>
<li><p>Clause : For example $c_a=(x_i\lor \bar x_j\lor x_k)$</p>
<p>$c_a\in V$ , $(v_{i,a,0},c_a),(c_a,v_{i,a,1})\in E$ , $(v_{j,a,1},c_a),(c_a,v_{j,a,0})\in E$ , $(v_{k,a,0},c_a),(c_a,v_{k,a,1})\in E$</p>
</li>
</ul>
<p><img src="/images/posts/AD6_fig1.png" alt=""></p>
<blockquote>
<p>$v_{i,j,2}$ for constraining the Hamiltonian cycle be the form $S\to x_{1,0/1}\to x_{1,1/0}\to \cdots\to x_{n,0/1}\to x_{n,1/0}\to T\to S$ .</p>
</blockquote>
</li>
<li><p>Cor : <strong>Hamiltonian Cycle</strong> / <strong>Hamiltonian Path</strong> for directed/undirected graph are all <strong>NPC</strong> .</p>
</li>
</ol>
</li>
</ol>
<h4 id="4-4-4-3-Dimensional-Matching-3DM"><a href="#4-4-4-3-Dimensional-Matching-3DM" class="headerlink" title="4.4.4 3-Dimensional Matching ( 3DM)"></a>4.4.4 3-Dimensional Matching ( 3DM)</h4><ol>
<li><p>Description</p>
<p>Def [ <strong><em>3D graph</em></strong> ] : $G=(X,Y,Z,E)$ , $E=\{e=(x_e,y_e,z_e)|x_e\in X,y_e\in Y,z_e\in Z\}$ .</p>
<p>Def [ <strong><em>3D Matching</em></strong> ] : $M\subseteq E$ satisfies </p>
<ul>
<li>$\forall e_1\neq e_2\in M $, $x_{e_1}\neq x_{e_2}$ , $y_{e_1}\neq y_{e_2}$ , $z_{e_1}\neq z_{e_2}$ </li>
<li>$\bigcup\limits_{e\in M}x_e=X$ , $\bigcup\limits_{e\in M}y_e=Y$ , $\bigcup\limits_{e\in M}z_e=Z$.</li>
</ul>
<p>Input : A 3D graph</p>
<p>Output : Decide whether there is a 3D Matching for this graph. </p>
</li>
<li><p><strong>3DM</strong> is <strong>NPC</strong></p>
<p>Obviously , <strong>3DM</strong> is <strong>NP</strong></p>
<p>Goal : <strong>3-SAT</strong> $\le_p$ <strong>3DM</strong></p>
<p>Consider an instance $I$ for <strong>3-SAT</strong> , construct $G$ , s.t. $G\in$ <strong>3DM</strong> $\iff I\in $ <strong>3-SAT</strong> . </p>
<p>Suppose $I$ has $n$ variables , $k$ clauses . </p>
<p>Construction :</p>
<p><strong>Vertices :</strong></p>
<ul>
<li>For each variable $x_i$ , construct $A_i=\{a_{i,1},\cdots,a_{i,2k}\}$ , $B_i=\{b_{i,1},\cdots,b_{i,k}\}$ , $C_i=\{c_{i,1},\cdots,c_{i,k}\}$ .</li>
<li>For each clause $c_i$ , construct $d_i,e_i$ .</li>
<li>Cleanup garget : for $i$-th garget construct $y_i,z_i$ .</li>
</ul>
<p>$X=\bigcup A_i$ , $Y=\left(\bigcup B_i \right)\cup \{d_i\}\cup\{y_i\}$ , $Z=\left(\bigcup C_i \right)\cup \{e_i\}\cup\{z_i\}$ </p>
<p><strong>Hyperedges</strong> :</p>
<ul>
<li>For each variable $x_i$ : $e_{i,2j-1}=(a_{i,2j-1},b_{i,j},c_{i,j})$ , $e_{i,2j}=(a_{i,2j},b_{i,j+1},c_{i,j})$ . ($1\le j\le k$ )</li>
<li>For each clause $c_t$ : suppose $c_t=x_i\lor \bar x_j\lor x_k$ : $e_{t,1}=(a_{i,2t},d_t,e_t)$ , $e_{t,2}=(a_{j,2t-1},d_t,e_t)$ , $e_{t,3}=(a_{k,2t},d_t,e_t)$ . ($1\le t\le k$)</li>
<li>For each cleanup garget $g_t$ : $e_{t,i,j}=(a_{i,j},y_t,z_t)$ . ($1\le i\le n,1\le j\le 2k$)</li>
</ul>
<p><img src="/images/posts/AD6_fig2.png" alt=""></p>
<blockquote>
<p>Cleanup garget : we need $(n-1)k$ gargets to recycle all free $a_{i,j}$ ( tips ) .</p>
</blockquote>
</li>
</ol>
<h4 id="4-4-5-3-Coloring-Problem-3CoL"><a href="#4-4-5-3-Coloring-Problem-3CoL" class="headerlink" title="4.4.5 3-Coloring Problem (3CoL)"></a>4.4.5 3-Coloring Problem (3CoL)</h4><ol>
<li><p>Description</p>
<ol>
<li><p>Def [ <strong><em>valid coloring</em></strong> ] : Assign each vertex a color , such that no two vertices having same color share an edge</p>
<p>$\forall e=(u,v)\in E$ , $c(u)\neq c(v)$ .</p>
</li>
<li><p>Def [ <strong><em>chromatic number</em></strong> ] $\chi(G)$ :The minimum number of colors that can color the graph validly .</p>
</li>
<li><p>Input : A graph $G$</p>
<p>Output : Decide whether $\chi(G)\le 3$.</p>
</li>
</ol>
<blockquote>
<ul>
<li><p><strong>2CoL</strong> is <strong>P</strong> , using <code>dfs</code></p>
</li>
<li><p><strong>4CoL Theorem</strong> : Any planar graph can be $4$-colored.</p>
<p>Current Proof : need computer assistant ( $\sim 10^2$ cases to verify )</p>
<p>Open : a proof of <strong>4CoL Theorem</strong> without computer assistant ?</p>
</li>
</ul>
</blockquote>
</li>
<li><p><strong>3CoL</strong> is <strong>NPC</strong></p>
<p>Obviously <strong>3CoL</strong> is <strong>NP</strong> .</p>
<p>Goal : <strong>3CoL</strong> $\le_p$ <strong>3-SAT</strong> .</p>
<p>Consider an instance $I$ for <strong>3-SAT</strong> , construct $G$ , s.t. $G\in$ <strong>3CoL</strong> $\iff I\in $ <strong>3-SAT</strong> . </p>
<p>Suppose $I$ has $n$ variables , $k$ clauses . </p>
<p>Construction :</p>
<p><strong>Vertices</strong> :</p>
<ul>
<li>Special garget : $T,F,B$ .</li>
<li>Variable garget : $v_i$ , $\bar v_i$ .</li>
<li>Clause garget : $6$ vertices for each clause .</li>
</ul>
<p><strong>Edges</strong> :</p>
<ul>
<li><p>Special garget : $(T,F),(T,B),(F,B)$</p>
<blockquote>
<p>To ensure $T,F,B$ having distinct colors</p>
</blockquote>
</li>
<li><p>Variable garget : $(B,v_i)$ , $(v_i,\bar v_i)$ , $(B,\bar v_i)$ </p>
<blockquote>
<p>To ensure either $c(v_i)=c(T),c(\bar v_i)=c(F)$ , or $c(v_i)=c(F),c(\bar v_i)=c(T)$</p>
</blockquote>
</li>
<li><p>Clause garget : Example $c=(x_i\lor \bar x_j\lor x_k)$ </p>
<p>$(v_i,a),(T,a),(a,b),(T,b)$</p>
<p>$(\bar v_j,c),(T,c)$</p>
<p>$(v_k,d),(T,d),(d,e),(F,e)$</p>
<p>$(f,a),(f,c),(f,e)$</p>
<blockquote>
<p>To ensure that $f$ can be colored if and only if $v_i,\bar v_j,v_k$ are not all colored with $c(F)$.</p>
<p>Otherwise , $c(v_i)=c(F)$ , so $c(a)=c(B)$ , so $c(b)=c(F)$</p>
<p>$c(\bar v_j)=c(F)$ , so $c(c)=c(B)$</p>
<p>$c(v_3)=c(F)$ , so $c(d)=c(B)$ , so $c(e)=c(T)$</p>
<p>Then $f$ cannot be colored </p>
</blockquote>
</li>
</ul>
<p><img src="/images/posts/AD6_fig3.png" alt=""></p>
</li>
</ol>
<h4 id="4-4-6-Subset-Sum-Problem"><a href="#4-4-6-Subset-Sum-Problem" class="headerlink" title="4.4.6 Subset Sum Problem"></a>4.4.6 Subset Sum Problem</h4><ol>
<li><p>Description</p>
<p>Input : $w_1,\cdots,w_n$ , target $W$ .</p>
<p>Output : determine whether there exists $S\subseteq [n]$ , s.t. $\sum_{i\in S}w_i=W$ .</p>
<blockquote>
<p>Subset Sum can be solved in $\mathcal O(nW)$ with DP method</p>
</blockquote>
</li>
<li><p><strong>Subset Sum</strong> is <strong>NPC</strong></p>
<p>Obviously <strong>Subset Sum</strong> is <strong>NP</strong>.</p>
<blockquote>
<p>$W$ in the reduction must be exponentially large</p>
</blockquote>
<p>Goal : <strong>3DM</strong> $\le_p$ <strong>Subset Sum</strong></p>
<ol>
<li><p>Construction</p>
<p>For each hyperedge $e=(x,y,z)$ , construct $w_e=2^{x-1+|Y|+|Z|}+2^{y-1+|Z|}+2^{z-1}$ </p>
<p>Let $W=\sum\limits_{x=1}^{|X|}2^{x-1+|Y|+|Z|}+\sum\limits_{y=1}^{|Y|}2^{y-1+|Z|}+\sum\limits_{z=1}^{|Z|}2^{z-1}$ </p>
<p><strong>Problem : carry ( 进位 )</strong></p>
</li>
<li><p>Improved Construction</p>
<p>Let $b=|E|+1$</p>
<p>For each hyperedge $e=(x,y,z)$ , construct $w_e=b^{x-1+|Y|+|Z|}+b^{y-1+|Z|}+b^{z-1}$ </p>
<p>Let $W=\sum\limits_{x=1}^{|X|}b^{x-1+|Y|+|Z|}+\sum\limits_{y=1}^{|Y|}b^{y-1+|Z|}+\sum\limits_{z=1}^{|Z|}b^{z-1}$ </p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-图论</tag>
        <tag>算法-动态规划</tag>
        <tag>算法-动态规划-背包</tag>
        <tag>计算理论</tag>
        <tag>计算理论-NPC</tag>
        <tag>计算理论-3SAT</tag>
        <tag>算法-图论-独立集</tag>
        <tag>算法-图论-哈密顿路</tag>
        <tag>算法-图论-高维图匹配</tag>
        <tag>算法-图论-染色问题</tag>
      </tags>
  </entry>
  <entry>
    <title>Probability and Statistics 3</title>
    <url>/2023/10/16/Probability-and-Statistics-3/</url>
    <content><![CDATA[<h2 id="Chapter-1-Background-in-Probability"><a href="#Chapter-1-Background-in-Probability" class="headerlink" title="Chapter 1 Background in Probability"></a>Chapter 1 Background in Probability</h2><h3 id="1-4-Integration"><a href="#1-4-Integration" class="headerlink" title="1.4 Integration"></a>1.4 Integration</h3><ol>
<li><p>Judgement : an integration should satisfies :</p>
<p>(i) $\varphi\ge 0$ $\mu$-a.e. , then $\int\varphi d\mu\ge 0$</p>
<p>(ii) $\int a\varphi d\mu=a\int \varphi d\mu$</p>
<p>(iii) $\int (\varphi+\psi)d\mu=\int \varphi d\mu+\int \psi d\mu$</p>
</li>
<li><p>Basic Properties of integration</p>
<p>(iv) $\varphi\le \psi$ $\mu$-a.e. , then $\int \varphi d\mu\le \int \psi d\mu$</p>
<p>(v) $\varphi=\psi$ $\mu$-a.e. , then $\int \varphi d\mu=\int \psi d\mu$</p>
<p>(vi) $\left|\int \varphi d\mu\right|\le \int |\varphi|d\mu$ </p>
</li>
<li><p>Bounded Function</p>
<ol>
<li><p>Def [ <strong><em>Bounded Function</em></strong> ] : $\exists M&lt;\infty,|f(x)|\le M$ , and $\exists E\subseteq \Omega , \mu(E)&lt;\infty,f(E^c)=0$ .</p>
</li>
<li><p>Def [ <strong><em>Integration for Bounded Function</em></strong>  ] : Let $\Phi_s$ be the set of simple functions . If $f$ is a bounded function,</p>
<script type="math/tex; mode=display">
\int fd\mu:=\sup_{\varphi\in \Phi_s , \varphi\le f}\int \varphi d\mu =\inf_{\psi\in \Phi_s , \psi\ge f}\int \psi d\mu</script><blockquote>
<p>$\varphi \le f,\psi\ge f$ : $\mu$-a.e.</p>
<p>Intuition : Like Darboux’s upper sum and lower sum</p>
</blockquote>
</li>
<li><p>Proof of $\sup\limits_{\varphi\in \Phi_s , \varphi\le f}\int \varphi d\mu =\inf\limits_{\psi\in \Phi_s , \psi\ge f}\int \psi d\mu$ </p>
<ol>
<li><p>By Prop (iv) , $\varphi\le f\le \psi$ , so $\int \varphi d\mu\le \int \psi d\mu$ </p>
</li>
<li><p>Prove $\sup\limits_{\varphi\in \Phi_s , \varphi\le f}\int \varphi d\mu \ge \inf\limits_{\psi\in \Phi_s , \psi\ge f}\int \psi d\mu$</p>
<p>Since $|f|\le M$ , let $E_k:=\{x\in E|\frac{(k-1)M}{n}&lt;f(x)\le \frac{kM}{n}\}$ , $-n\le k\le n$ .</p>
<p>Let $\varphi_n=\sum\limits_{k=-n}^n \frac{(k-1)M}{n}\mathbb 1_{E_k}$ , $\psi_n=\sum\limits_{k=-n}^n \frac{kM}{n}\mathbb 1_{E_k}$ </p>
<p>Therefore , $\int (\psi_n-\varphi_n)d\mu=\frac{M}{n}\mu(E)$ ,</p>
<script type="math/tex; mode=display">
\sup\limits_{\varphi\in \Phi_s , \varphi\le f}\int \varphi d\mu \ge\int \varphi_nd\mu =-\frac{M}{n}\mu(E)+\int \psi_n d\mu \ge -\frac{M\mu(E)}{n}+\inf\limits_{\psi\in \Phi_s , \psi\ge f}\int \psi d\mu</script><p>When $n\to +\infty$ , $-\frac{M\mu(E)}{n}\to 0$ , so $\sup\limits_{\varphi\in \Phi_s , \varphi\le f}\int \varphi d\mu =\inf\limits_{\psi\in \Phi_s , \psi\ge f}\int \psi d\mu$ .</p>
<p><img src="/images/posts/PS3_fig1.png" alt="diagram of the proof"></p>
</li>
</ol>
</li>
<li><p>Proof of (i)(ii)(iii)</p>
<p>(i) : Let $\varphi=0$ , trivial</p>
<p>(ii) : For $a&gt;0$ , $a\varphi\le af\iff \varphi\le f$ </p>
<script type="math/tex; mode=display">
\int afd\mu =\sup_{a\varphi\le af}\int a\varphi d\mu =\sup_{\varphi\le f}\int a\varphi d\mu=a\sup_{\varphi\le f}\int \varphi d\mu=a\int fd\mu</script><p>(iii) : Firstly prove $\int (f+g)d\mu\le \int fd\mu +\int gd\mu$ .</p>
<p>Let $\Phi_s’=\{\psi|\exists \psi_1,\psi_2,\psi=\psi_1+\psi_2,\psi_1\ge f,\psi_2\ge g\}$ , so $\Phi_s’\subseteq \{\psi|\psi\ge f+g\}$ .</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int(f+g)d\mu&=\inf_{\psi\ge f+g} \psi d\mu\\
&\le \inf_{\psi=\psi_1+\psi_2\in \Phi_s'} (\psi_1+\psi_2)d\mu\\\
&=\inf_{\psi_1\ge f,\psi_2\ge g} \int \psi_1d\mu+\int \psi_2d\mu\\
&=\inf_{\psi_1\ge f} \int \psi_1d\mu+\inf_{\psi_2\ge g}\int \psi_2d\mu\\
&=\int fd\mu+\int gd\mu
\end{aligned}</script><p>Use (ii) , let $a=-1$ , so $\int -(f+g)d\mu\le \int (-f)d\mu+\int (-g)d\mu$ , so $\int (f+g)d\mu\ge \int fd\mu+\int gd\mu$ .</p>
</li>
</ol>
</li>
<li><p>Non-Negative Function</p>
<ol>
<li><p>Compare with bounded function</p>
<ul>
<li>can exists $x$ , $f(x)\to\infty$</li>
<li>the smallest $E$ s.t. $f(E^c)=0$ , may be $\mu(E)\to\infty$ </li>
</ul>
</li>
<li><p>Notation : </p>
<ul>
<li><script type="math/tex; mode=display">
\int_E fd\mu:=\int f\cdot \mathbb 1_{E}d\mu</script></li>
<li><script type="math/tex; mode=display">
a\land b:=\min\{a,b\}</script></li>
<li><p>Def [ <strong><em>Integration for non-negative function</em></strong> ] For non-negative function $f$ , </p>
<script type="math/tex; mode=display">
\int fd\mu=\sup\left\{\small\int hd\mu\mid 0\le h\le f,h\text{ bounded function}\right\}</script><blockquote>
<p>$h$ is bounded function , so $\int hd\mu$ is bounded , but $\sup \int hd\mu$ can be unbounded .</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Lemma :</p>
<p>If $E_n\uparrow \Omega , \mu(E_n)&lt;\infty$ , then </p>
<script type="math/tex; mode=display">
\int_{E_n}f\land nd\mu\uparrow \int fd\mu</script><blockquote>
<p>This lemma needs $\mu$ be a $\sigma$-finite measure.</p>
<p>$(f\land n)\cdot \mathbb 1_E\in \{h|0\le h\le f,h\text{ bounded function}\}$</p>
</blockquote>
<p><strong>Proof</strong></p>
<p>It’s easy to find that $\lim\limits_{n\to\infty}\int_{E_n}f\land n d\mu=\sup\limits_{n\ge 1}\int_{E_n}f\land n d\mu\le \int fd\mu$ .</p>
<p>We only need to prove that $\forall h,|h|&lt;M,0\le h\le f$ ,  $\lim\limits_{n\to\infty}\int_{E_n}f\land n d\mu\ge \int fd\mu$ .</p>
<p>$\forall n&gt;M$ , $\int_{E_n} f\land n d\mu\ge \int_{E_n}hd\mu=\int hd\mu-\int_{E_n^c}hd\mu$ .</p>
<p>And $\int_{E_n^c} hd\mu\le M\mu(E_n^c\cap \{x|h(x)&gt;0\})$ .</p>
<p>Since $n\to \infty , \mu(E_n^c\cap \{x|h(x)&gt;0\})\to 0$ $(*)$ .</p>
<p>Therefore , $\liminf\limits_{n\to \infty} \int_{E_n} f\land n d\mu\ge \int hd\mu$ .</p>
<p><strong>To prove $(*)$</strong></p>
<p>Let $a_n=\mu(E_n^c\cap \{x|h(x)&gt;0\})$ , we only need to prove that $\lim_{n\to \infty} a_n=0$ .</p>
<p>Way 1 : $a_n$ non-increasing , $a_n\ge 0$ , so $\lim_{n\to\infty} a_n=c$ </p>
<pre><code>If $c&gt;0$ , then $\exists \tilde \Omega\subseteq \Omega , \mu(\tilde\Omega)=c$ , $\forall n,\tilde\Omega\cap E_n=\varnothing$ .

Therefore , $\tilde \Omega\cap \bigcup_&#123;n=1&#125;^&#123;\infty&#125;E_n=\varnothing$ , so $\tilde \Omega\cap\Omega=\varnothing$ , contradicts .
</code></pre><p>Way 2 : By definition , $E_1^c\supset E_2^c \supset\cdots$ , so $E_n^c=\bigcap_{m=1}^n E_m^c$ .</p>
<pre><code>$\lim_&#123;n\to\infty&#125;a_n=\mu(\bigcap_&#123;m=1&#125;^&#123;\infty&#125;E_m^c\cap\&#123;x|h(x)&gt;0\&#125;)=\mu(\Omega^c\cap\&#123;x|h(x)&gt;0\&#125;)=0$
</code></pre></li>
<li><p>Proof of (i)(ii)(iii)</p>
<p>(i) trivial (non-negative)</p>
<p>(ii) $a&gt;0$ , so $ah\le af\iff h\le f$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int af\ d\mu&=\sup\{\int ah\ d\mu|0\le ah\le af,ah\text{ bounded function}\}\\
&=\sup\{a\int h\ d\mu|0\le h\le f,h\text{ bounded function}\}\\
&=a\int fd\mu
\end{aligned}</script><p>(iii) Firstly prove $\int (f+g)d\mu\ge \int fd\mu+\int gd\mu$ </p>
<p>Consider bounded function $k,h$ , $f\ge h,g\ge k$ , so ,</p>
<p>$\int (f+g)d\mu\ge \sup_{0\le h\le f}\int hd\mu+\sup_{0\le k\le g}\int gd\mu=\int fd\mu+\int gd\mu$</p>
<p>Secondly prove $\int(f+g)d\mu\le \int fd\mu+\int gd\mu$</p>
<p>Use Lemma , $(f+g)\land n\le (f\land n)+(g\land n)$ .</p>
<p>Therefore , $\int_{E_n}(f+g)\land n d\mu\le \int_{E_n}f\land n d\mu+\int_{E_n} g\land n d\mu$ .</p>
<p>As $n\to\infty$ , $\int (f+g)d\mu\le \int fd\mu+\int gd\mu$</p>
</li>
</ol>
</li>
<li><p>General Function</p>
<ol>
<li><p>Def [ <strong><em>Integralable</em></strong> ] : $f$ is <strong><em>integralable</em></strong> if $\int |f|d\mu&lt;\infty$</p>
</li>
<li><p>Def [ <strong><em>Integration for General Function</em></strong> ] : If $f$ is integralable , define </p>
<p>$f^+(x):=f(x)\cdot\mathbb 1(f(x)\ge 0)$ , $f^-(x):=|f(x)|\cdot \mathbb 1(f(x)&lt;0)$</p>
<p>Therefore , $f(x)=f^+(x)-f^-(x)$ , and $f^+,f^-$ non-negative </p>
<script type="math/tex; mode=display">
\int fd\mu:=\int f^+d\mu-\int f^-d\mu</script><blockquote>
<p>Note : If $f$ is not integralable , then $\int f^+d\mu$ , $\int f^-d\mu$ can both be $\infty$ , $\infty-\infty$ not well-defined.</p>
</blockquote>
</li>
</ol>
</li>
<li><p>Lebesgue Integration</p>
<ol>
<li><p>When $\mu$ is Lebesgue measure : $\mu((a,b])=b-a$ .</p>
<p>$\int fd\mu$ is Lebesgue Integration of $f$ .</p>
</li>
<li><p>Compare with Riemann Integration</p>
<p><img src="/images/posts/PS3_fig2.png" alt="Riemann Integration and Lebesgue Integration"></p>
<p>Lebesgue is more powerful</p>
<p>E.g. Dirichlet function $\mathbb 1_{\mathbb Q}$ , $\int_{[0,1]}\mathbb 1_{\mathbb Q}$ </p>
<ul>
<li>Riemann integration : not integralable , $\mu(\{x:\mathbb 1_{\mathbb Q}\text{ incontinuous at }x\})&gt;0$</li>
<li>Lebesgue integration : integralable : $\int_{[0,1]}\mathbb 1_{\mathbb Q}=\mu(\mathbb Q\cap [0,1])=0$</li>
</ul>
</li>
</ol>
</li>
<li><p>Formal Definition of PDF</p>
<ol>
<li><p>Def [ <strong><em>Absolutely Continuous</em></strong> ] : $\nu$ is <strong><em>absolutely continuous</em></strong> w.r.t $\mu$ ( denote as $\nu\ll \mu$ ) if</p>
<p>$\forall A\in \mathcal F$ , $\mu(A)=0\Rightarrow \nu(A)=0$</p>
</li>
<li><p>Thm [ Radon-Nikodym ] : measurable space $(\Omega,\mathcal F)$ with $\sigma$-finite measure $\nu,\mu$ , $\nu\ll\mu$ </p>
<p>Then $\exists g\ge 0,\forall E\in \mathcal F,\int_Egd\mu=\nu(E)$</p>
<p>$g$ is called Radon-Nikodym derivative</p>
</li>
<li><p>Prop : If $\exists g,h$ , $\int_Egd\mu=\int_Ehd\mu=\nu(E)$ , then $g\equiv h$ a.e.</p>
</li>
<li><p>Def [ <strong><em>PDF</em></strong> ] : When $\nu$ is a distribution measure , $\mu$ is Lebesgue measure , $g$ is the <strong><em>probability density function</em></strong> for $\nu$ . </p>
<blockquote>
<p>Observation : $\nu(E)=P(X\in E)=\int gd\mu$</p>
</blockquote>
</li>
<li><p>Prop : $g$ is $\mathcal F$-measurable , i.e. $g$ is a measurable map $(\Omega,\mathcal F)\to(\mathbb R,\mathcal R)$ .</p>
</li>
</ol>
</li>
</ol>
<h3 id="1-5-Properties-of-Integration"><a href="#1-5-Properties-of-Integration" class="headerlink" title="1.5 Properties of Integration"></a>1.5 Properties of Integration</h3><ol>
<li><p>Jenson’s Inequality</p>
<ol>
<li><p>Def [ <strong><em>convex</em></strong> ] : $\varphi$ is a convex , if $\forall \lambda\in (0,1) , \lambda\varphi(x)+(1-\lambda)\varphi(y)\ge \varphi(\lambda x+(1-\lambda)y)$</p>
</li>
<li><p>THM [ <strong><em>Jenson’s Inequality</em></strong> ] : If $\varphi$ is a convex , $\varphi\circ f$ is integralable , $\mu$ is a probability measure ,</p>
<script type="math/tex; mode=display">
\varphi\left(\int fd\mu\right)\le \int (\varphi \circ f)d\mu</script></li>
<li><p>Proof : </p>
<p>For convex $\varphi$ , we can find a line $L(x)=ax+b$ , s.t. $\varphi(x)\ge ax+b$ , and equal iff $x=\int fd\mu$ .</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int (\varphi \circ f)d\mu&\ge \int (ax+b)\circ fd\mu\\
&=a\int fd\mu+\int bd\mu\\
&=a\int fd\mu+b\mu(\Omega)\\
&=a\int fd\mu+b\\
&=L\left(\int fd\mu\right)\\
&=\varphi\left(\int fd\mu\right)
\end{aligned}</script></li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>概率与统计</category>
      </categories>
      <tags>
        <tag>概率论-概率密度函数</tag>
        <tag>实分析-Lebesgue积分</tag>
        <tag>实分析-积分性质</tag>
      </tags>
  </entry>
  <entry>
    <title>ZKP and MPC 3</title>
    <url>/2023/10/16/ZKP-and-MPC-3/</url>
    <content><![CDATA[<h2 id="Lec02-Zero-Knowledge-Proof-under-Composition"><a href="#Lec02-Zero-Knowledge-Proof-under-Composition" class="headerlink" title="Lec02  Zero-Knowledge Proof under Composition"></a>Lec02  Zero-Knowledge Proof under Composition</h2><h3 id="2-3-Sequential-Composition"><a href="#2-3-Sequential-Composition" class="headerlink" title="2.3 Sequential Composition"></a>2.3 Sequential Composition</h3><blockquote>
<p>$V^*$ : should be viewed as a black box , input $(input,randomness,messages)$ , output response</p>
<p>If guess wrong at $m_i$ , we can rewind $V^*$ to the time when it receives $m_i$ , using the same input , randomness , first $i-1$ messages</p>
</blockquote>
<ol>
<li><p>Def [ <strong><em>Zero-Knowledge Proof with Auxiliary Input</em></strong> ] : </p>
<p>$(P,V)$ is an <strong><em>interactive zero-knowledge proof with auxiliary input</em></strong> for $L$ if </p>
<ul>
<li><p>$V$ is an efficient algorithm ( poly-time )</p>
</li>
<li><p><strong><em>Completeness</em></strong> : $\forall x\in L,\forall y,z\in \{0,1\}^*$</p>
<script type="math/tex; mode=display">
\Pr\{\braket{P(y),V(z)}(x)=1\}=1</script></li>
<li><p><strong><em>Soundness with error $\epsilon$</em></strong> : $\forall x\notin L,\forall y,z\in \{0,1\}^*$ </p>
<script type="math/tex; mode=display">
\forall P^*,\Pr\{\braket{P^*(y),V(z)}(x)=1\}<\epsilon</script></li>
<li><p><strong><em>Zero-Knowledge</em></strong> : $\forall V^<em>$ in P.P.T. , $\exists$ expected poly-time algorithm $M^</em>$ , $\forall x\in L,\forall y,z\in \{0,1\}^*$ ,</p>
<script type="math/tex; mode=display">
M^*(x,z)\sim View_{V^*(z)}^{P(y)}(x)</script></li>
</ul>
<blockquote>
<p>$y$ is indeed redundant since $P$ is not computationally bounded</p>
<p>Alternating Form of Zero-Knowledge : $\forall V^<em>,\exists M^</em>,\forall x\in L,\forall y,z\in \{0,1\}^*$ </p>
<script type="math/tex; mode=display">
M^*(x,z)\sim \braket{P(y),V^*(z)}(x)</script></blockquote>
</li>
<li><p>Lemma [ <strong><em>Composition Lemma</em></strong> ] :</p>
<p>$(P,V)$ : zero-knowledge proof with auxiliary input for $L$ , $q$ is a polynomial of $\kappa$ </p>
<p>$(P_q,V_q)$ : An IP runs $(P,V)$ $q$ times</p>
<p>Claim : $(P_q,V_q)$ are still zero-knowledge .</p>
</li>
<li><p>Proof of Composition Lemma </p>
<blockquote>
<p>Only Proof for perfect zero-knowledge</p>
<p>Using Alternating Form of Zero-Knowledge</p>
</blockquote>
<h4 id="Construction"><a href="#Construction" class="headerlink" title="Construction"></a>Construction</h4><ol>
<li><p>$M^*$ inputs $(x,z)$ , $st_0=\varnothing$ . </p>
</li>
<li><p>For $i=1,2,\cdots q$ :</p>
<ol>
<li><p>Construct $V_i^<em>$ : $V_i^</em>(x,st_{i-1},z)=\begin{cases}V^<em>(x,z)&amp;i=1\\V^</em>(st_{i-1})&amp;i\ge 2\end{cases}$ , output state of $V^*$ as $st_i$ </p>
<blockquote>
<p>Note : for $i\ge 2$ , $x,z$ are contained in $st_{i-1}$ , so we do not need to input them.</p>
</blockquote>
</li>
<li><p>By zero-knowledge , $\exists M_i^<em>(x,st_{i-1}|z)$ that can simulate the output of $V_i^</em>$.</p>
</li>
<li><p>$M^<em>$ runs $M_i^</em>(x,st_{i-1}|z)$ to obtain $st_i$.</p>
</li>
</ol>
</li>
<li><p>$M^<em>$ invokes $V^</em>$ with $st_q$ to obtain final output.</p>
</li>
</ol>
<h4 id="Proof-of-M-x-z-equiv-braket-P-y-V-z-x"><a href="#Proof-of-M-x-z-equiv-braket-P-y-V-z-x" class="headerlink" title="Proof of $M^(x,z)\equiv \braket{P(y),V^(z)}(x)$"></a>Proof of $M^<em>(x,z)\equiv \braket{P(y),V^</em>(z)}(x)$</h4><ol>
<li><p>$Hyb_0$ : Real Execution $\braket{P(y),V^*(z)}(x)$</p>
</li>
<li><p>$Hyb_1$ : </p>
<ol>
<li>Run $P(y)$ and $V^*(z)$ until the last phase .</li>
<li>Record the state of $V^*$ as $st_{q-1}$ .</li>
<li>Invoke $M_q^*(x,st_{q-1}|z)$ to obtain $st_q$ .</li>
<li>Invoke $V^*$ with $st_q$ to obtain final output .</li>
</ol>
<p><strong>Proof of $Hyb_0\equiv Hyb_1$</strong> : </p>
<p>Fix state $st_{q-1}$ . For $Hyb_0$ , its distribution is the same as </p>
<ol>
<li>Run $V_q^*(x,st_{q-1},z)$ to obtain $st_q$</li>
<li>Run $V^*$ with $st_q$ to obtain final output</li>
</ol>
<p>By zk , given $x,st_{q-1},z$ , $V_q^<em>(x,st_{q-1},z)\equiv M_q^</em>(x,st_{q-1}|z)$.</p>
<p>Therefore , $Hyb_0\equiv Hyb_1$.</p>
</li>
<li><p>$Hyb_i$ : Change to $M^*$ after the first $q-i$ phases</p>
<p>$Hyb_{i-1}$ : using $V^<em>_{q-i+1}$  ,  $Hyb_i$ : using $M^</em>_{q-i+1}$</p>
<p>Therefore , $Hyb_{i-1}\equiv Hyb_i$</p>
</li>
<li><p>$Hyb_{q+1}$ : The execution of $M^*$ .</p>
</li>
</ol>
</li>
</ol>
<h2 id="Lec03-Commitment-Scheme-zk-proof-for-general-NP"><a href="#Lec03-Commitment-Scheme-zk-proof-for-general-NP" class="headerlink" title="Lec03  Commitment Scheme , zk-proof for general NP"></a>Lec03  Commitment Scheme , zk-proof for general NP</h2><h3 id="3-0-Important"><a href="#3-0-Important" class="headerlink" title="3.0 Important"></a>3.0 Important</h3><p><strong><em>In the next lecture , we re-defined commitment in a more simple way .</em></strong></p>
<h3 id="3-1-Commitment"><a href="#3-1-Commitment" class="headerlink" title="3.1 Commitment"></a>3.1 Commitment</h3><blockquote>
<p>Intuition : </p>
<p>Binding : make some choice , then won’t change it . (change will be rejected)</p>
<p>Hiding : like “safe” , until open it , not knowing the choice . (uniform distribution)</p>
</blockquote>
<ol>
<li><p>Def [ <strong><em>Commitment</em></strong> ] : Three P.P.T. algorithms</p>
<ul>
<li>$Gen(1^\kappa)\to pp$ ( public parameter )</li>
<li>$Commit_{pp}(m,r)\to (c,aux)$</li>
<li>$Verify_{pp}(m,c,aux)\to\{Accept,Reject\}$ </li>
</ul>
<p>Satisfying properties :</p>
<ul>
<li><strong><em>Hiding</em></strong> : $\forall m,m’$ , the following distributions are identical/statistically indistinguishable/computationally indistinguishable</li>
</ul>
<script type="math/tex; mode=display">
Dist\{pp\gets Gen(1^\kappa),(c,aux)\gets Commit_{pp}(m,r) : (pp,c)\}</script><script type="math/tex; mode=display">
Dist\{pp\gets Gen(1^\kappa),(c,aux)\gets Commit_{pp}(m',r) : (pp,c)\}</script><ul>
<li><p><strong><em>Binding</em></strong> : $\forall m\neq m’$ , $\forall pp\gets Gen(1^\kappa)$ , </p>
<p><strong><em>Perfect Binding</em></strong> :</p>
<script type="math/tex; mode=display">
\{c|(c,aux)\gets Commit_{pp}(m,r)\}\cap\{c|(c,aux)\gets Commit_{pp}(m',r)\}=\varnothing</script><p><strong><em>Computationally Binding</em></strong> : $\forall P^*$ in P.P.T.  </p>
</li>
</ul>
</li>
</ol>
<script type="math/tex; mode=display">
\small{\Pr\{pp\gets Gen(1^\kappa) , (c,m,aux,m',aux')\gets P^*(pp) : Verify_{pp}(c,m,aux)=Verify(c,m',aux')=Accept\}<\epsilon}</script><ol>
<li><p>Open Commitment ( implement $Verify_{pp}$ )</p>
<p>One general way : let $aux=r$ </p>
<p>Verify checks whether $(c,aux)=Commit_{pp}(m,r=aux)$ </p>
<blockquote>
<p>There exists faster Verify for some specific problems.</p>
</blockquote>
</li>
<li><p>Note : hiding and binding are incompatible</p>
<p>hiding : $c$ can from different $m$</p>
<p>binding : $c$ cannot from different $m$</p>
</li>
</ol>
<h3 id="3-2-Perfect-Binding-amp-Computationally-Hiding-Commitment"><a href="#3-2-Perfect-Binding-amp-Computationally-Hiding-Commitment" class="headerlink" title="3.2 Perfect Binding &amp; Computationally Hiding Commitment"></a>3.2 Perfect Binding &amp; Computationally Hiding Commitment</h3><ol>
<li><p>DDH assumption</p>
<p>$G=\braket g$ , $|g|=p$ , $p\sim \kappa$ , The following distributions are computationally indistinguishable</p>
<script type="math/tex; mode=display">
Dist\{a,b\gets \mathbb Z_p :(g,g^a,g^b,g^{ab})\}</script><script type="math/tex; mode=display">
Dist\{a,b,c\gets\mathbb Z_p:(g,g^a,g^b,g^c)\}</script></li>
<li><p>Construction</p>
<p>Suppose that $m\in G$ </p>
<script type="math/tex; mode=display">
Gen(1^\kappa)\to(G,p,g)</script><script type="math/tex; mode=display">
Commit_{pp}(m,(a,b))\to c=(g^a,g^b,mg^{ab}),aux=(a,b)</script><script type="math/tex; mode=display">
Verify_{pp}(c,m,aux):\text{check }c_1=g^a\land c_2=g^b\land c_3=g^{ab}</script></li>
<li><p>Perfect Binding</p>
<p>For $c=(c_1,c_2,c_3)$ , $\exists  $ unique $a,b$ , s.t. $c_1=g^a,c_2=g^b$ . </p>
<p>Therefore , $m=\frac{c_3}{g^{ab}}$ , which is uniquely determined</p>
</li>
<li><p>Computationally Hiding  [Prove by Hybrid Proof]</p>
<p>We want to prove that </p>
<script type="math/tex; mode=display">
\{a,b\gets \mathbb Z_p:(g^a,g^b,mg^{ab})\}\sim \{a,b\gets \mathbb Z_p:(g^a,g^b,m'g^{ab})\}</script><ol>
<li>$Hyb_0$ : $a,b\gets \mathbb Z_p :(g^a,g^b,mg^{ab})$</li>
<li>$Hyb_1$ : $a,b,c\gets \mathbb Z_p:(g^a,g^b,mg^c)$</li>
<li>$Hyb_2$ : $a,b,c\gets \mathbb Z_p:(g^a,g^b,m’g^c)$\\</li>
<li>$Hyb_3$ : $a,b,c\gets \mathbb Z_p:(g^a,g^b,m’g^{ab})$</li>
</ol>
<script type="math/tex; mode=display">
Hyb_0\sim Hyb_1\equiv Hyb_2\sim Hyb_3</script></li>
</ol>
<h3 id="3-3-Computationally-Binding-amp-Perfect-Hiding-Commitment"><a href="#3-3-Computationally-Binding-amp-Perfect-Hiding-Commitment" class="headerlink" title="3.3 Computationally Binding &amp; Perfect Hiding Commitment"></a>3.3 Computationally Binding &amp; Perfect Hiding Commitment</h3><ol>
<li><p>Discrete Log Assumption </p>
<p>$G=\braket{g}$ , $|g|=p$ . $\forall A$ in P.P.T. , </p>
<script type="math/tex; mode=display">
\Pr\{a\gets \mathbb Z_p:A(G,p,g,g^a)=a\}<\epsilon</script></li>
<li><p>Construction</p>
<p>Suppose that $m\in \mathbb Z_p$ </p>
<script type="math/tex; mode=display">
Gen(1^\kappa)\to (G,p,g,h=g^a)</script><p>$a\gets \mathbb Z_p$ , and we need to “forget” $a$ .</p>
<script type="math/tex; mode=display">
Commit_{pp}(m,r):r\gets \mathbb Z_p,c=g^rh^m,aux=r</script><script type="math/tex; mode=display">
Verify_{pp}(c,m,aux):\text{ check } c=g^{aux}h^m</script></li>
<li><p>Perfect Hiding</p>
<p>$c=g^rh^m$ , which is the same distribution as $g^r$ , since $r$ is random .</p>
</li>
<li><p>Computationally Binding</p>
<p>Suppose that we have P.P.T. $A$ that $A(G,p,g,h)\to (c,m,aux,m’,aux’)$ , s.t. </p>
<script type="math/tex; mode=display">
c=g^{aux}h^m=g^{aux'}h^{m'}</script><p>Therefore : </p>
<script type="math/tex; mode=display">
aux+am=aux'+am'</script><p>Therefore :</p>
<script type="math/tex; mode=display">
a=\frac{aux-aux'}{m'-m}</script><p>which contradicts with Discrete Log assumption</p>
</li>
<li><p>How to generate $h=g^a$</p>
<ul>
<li><p>Generate by Sender ? NO</p>
<p>knows $a$ , then $c=g^{r+am}$ . Sender can choose proper $r$ to reconstruct $m$ . </p>
</li>
<li><p>Generate by Receiver ? YES</p>
<p>knows $a$ not affect hiding</p>
</li>
</ul>
<blockquote>
<p>perfect binding + computationally hiding : Sender runs Gen</p>
<p>computationally binding + perfect hiding : Receiver runs Gen</p>
</blockquote>
</li>
</ol>
<h3 id="3-4-General-NP-Problem"><a href="#3-4-General-NP-Problem" class="headerlink" title="3.4 General NP Problem"></a>3.4 General NP Problem</h3><ol>
<li><p>NP , NPC</p>
<ol>
<li><p>Def [ <strong><em>NP</em></strong> ] : $L\in NP$ if $\exists $ poly-time algorithm $D$ , </p>
<p>$x\in L\Leftrightarrow \exists w , D(x,w)=1$</p>
<p>$w$ is called the proof .</p>
</li>
<li><p>Def [ <strong><em>NP-Complete</em></strong> ] : $L’\in NP-Complete$ if $\forall L\in NP$ , $\exists $ poly-time algorithm $Q$ , s.t.</p>
<p>$Q(x,w)=(x’,w’)$ , and $x\in L\equiv x’\in L’$ .</p>
<p>i.e. all NP problems can be poly-reducible to NP-Complete Problem</p>
</li>
</ol>
</li>
<li><p>Graph Hamiltonicity</p>
<ol>
<li><p>Def [ <strong><em>Hamiltonian</em></strong> ] : A graph $G$ is <strong><em>Hamiltonian</em></strong> , if there exists a cycle visiting each node exactly once.</p>
</li>
<li><p>Construct zk-proof for Graph Hamiltonicity</p>
<ol>
<li><p>Like GI :</p>
<p>Step 1 : send $\tilde G$ , where $G\sim \tilde G$</p>
<p>Step 2 : give a cycle of $\tilde G$ to prove its Hamiltonian</p>
<p>Traditional ways : we cannot realize both</p>
</li>
<li><p>Protocol with commitment</p>
</li>
</ol>
<p>Commitment of a graph $G$ : $\forall i,j\in [n]$ ,</p>
<script type="math/tex; mode=display">
(c_{i,j},aux_{i,j})\gets\begin{cases}
Commit_{pp}(1,r)&(i,j)\in E\\
Commit_{pp}(0,r)&(i,j)\notin E
\end{cases}</script><p>|               Prover                |                                                            |             Verifier             |<br>| :————————————————-: | :————————————————————————————: | :———————————————: |<br>| $\pi\gets S_n$ , $\tilde G:=\pi(G)$ |                                                            |                                  |<br>|         Commit $\tilde G$ :         |                      —-$c_{i,j}$—&gt;                       |                                  |<br>|                                     |                         &lt;—$b$—-                          |         $b\gets \{0,1\}$         |<br>|              If $b=0$               |               —-$\pi,m_{i,j},aux_{i,j}$—&gt;                |     Checks $\tilde G=\pi(G)$     |<br>|              If $b=1$               | —-cycle for $\tilde G$ , $m_{i,j},aux_{i,j}$ in cycle—-&gt; | Checks cycle and edges existence |</p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>零知识证明和多方安全计算</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>密码学-证明复合</tag>
        <tag>密码学-证明复合引理</tag>
        <tag>密码学-承诺</tag>
        <tag>密码学-密码学假设-DDH假设</tag>
        <tag>密码学-密码学假设-离散对数假设</tag>
        <tag>密码学-承诺实现零知识证明</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 5</title>
    <url>/2023/10/16/Algorithm-Design-5/</url>
    <content><![CDATA[<h2 id="Chapter-3-Dynamic-Programming"><a href="#Chapter-3-Dynamic-Programming" class="headerlink" title="Chapter 3 Dynamic Programming"></a>Chapter 3 Dynamic Programming</h2><h3 id="3-4-2-Treewidth-cond"><a href="#3-4-2-Treewidth-cond" class="headerlink" title="3.4.2 Treewidth cond."></a>3.4.2 Treewidth cond.</h3><p>Extension : [ Roberson , Seymour  ] [ <strong><em>Graph Minor THM</em></strong> ]</p>
<blockquote>
<p>1983 - 2004 , 20 papers , 500 pages</p>
</blockquote>
<ol>
<li><p>Def [ <strong><em>Minor</em></strong> ] : a minor of $G$ can be obtained by contracting edges and deletion of nodes/edges from $G$ </p>
<p>contract edge : $e=\{u,v\}$ , contract it : then merge ${u,v}$ as a new vertex .</p>
</li>
<li><p>Def [ <strong><em>Minor-closed</em></strong> ] : A graph family $\mathcal F$ of graph is <strong><em>minor-closed</em></strong> if $\forall G\in \mathcal F$ , the minor of $G\in \mathcal F$</p>
<p>E.g. : $\mathcal F=\{\text{forests}\}$ , $\mathcal F=\{\text{planar graph}\}$ , $\mathcal F=\{\text{graphs with }tw\le k\}$ are all minor-closed</p>
</li>
<li><p>Thm [ <strong><em>Wagner-Kuratowski THM</em></strong> ] : planar graph $\Leftrightarrow$ no $K_{3,3} , K_5$ as minors </p>
<p>Likewise : forests $\Leftrightarrow$ no $K_3$ as minors</p>
</li>
<li><p>Thm [ <strong><em>Graph Minor THM</em></strong> ] : a minor-closed family $\Leftrightarrow$ no $H$ as minors , $H$ is a finite graph set</p>
</li>
<li><p>Application : Fixed Parameter Tractable </p>
</li>
</ol>
<h3 id="3-5-Shortest-Path-Bellman-Ford"><a href="#3-5-Shortest-Path-Bellman-Ford" class="headerlink" title="3.5 Shortest Path (Bellman-Ford)"></a>3.5 Shortest Path (Bellman-Ford)</h3><ol>
<li><p>Description</p>
<p>directed graph $G=(V,E)$ , $w_e\in \mathbb R$ , $G$ has no negative cycle . ( not necessarily $w_e\ge 0$ )</p>
<p>Find shortest path for all vertices to $t$ . </p>
</li>
<li><p>Algorithm</p>
<p>$opt(i,v)$ : using at most $i$ edges , the shortest path from $v$ to $t$ .</p>
<script type="math/tex; mode=display">
opt(i,v)=\min\begin{cases}
opt(i-1,v)\\
\min_u\{opt(i-1,u)+w_{v\to u}\}
\end{cases}</script><p>Time Complexity : $\mathcal O(nm)$ .</p>
<p>Memory Complexity : $\mathcal O(n^2)$ . ( at most using $n-1$ edges )</p>
</li>
<li><p>Optimize Memory Complexity </p>
<script type="math/tex; mode=display">
opt(v)=\min\begin{cases}
opt(v)\\
\min_{(v,u)\in E}\{opt(u)+w_{v\to u}\}
\end{cases}</script><p>Iterate the update $n$ times .</p>
<blockquote>
<p>Note : may using $opt(i,u)$ , but we do not care about the really number of edges in the shortest path .</p>
<p>Observation : After $i$ iterations , $opt(v)$ is not larger than $opt(i,v)$ </p>
</blockquote>
</li>
<li><p>Find the Shortest Path</p>
<p>Construct a pointer graph $P=\{(v,first[v])|v\in V\backslash\{t\}\}$ :</p>
<p>$\forall v\in V$ , $first[v]$ : the first node on the $v\to t$ path after $v$ .</p>
<p>Compute $first[v]$ : When $opt(v)$ is updated by $opt(u)+w_{v\to u}$ , let $first[v]=u$ .</p>
<blockquote>
<p>Observation : After all iterations , $\{(v,first[v])|v\in V\backslash \{t\}\}$ forms a shortest path tree .</p>
</blockquote>
</li>
<li><p>Lemma 1 : Suppose $G$ has no negative cycle . At the termination of the algorithm , $P$ is the a shortest path tree rooted at $t$ .</p>
<p>Proof : [ Induction like ]</p>
<p>If $first[v]=u$ , then $opt(v)=opt(u)+w_{v\to u}$ </p>
<p>$opt(u)$ is the shortest $u\to t$ path . Therefore , the cost of $u\to t$ path on $P$ is smallest . Therefore , the cost of $v\to t$ path on $P$ is smallest .</p>
</li>
<li><p>Lemma 2 : If $P$ contains a cycle $C$ at any stage , then $cost(C)&lt;0$</p>
<p>Proof : Let $C=\{v_1,v_2,\cdots,v_k\}$ .</p>
<p>If $first[v]=u$ , then $opt(v)\ge w_{v\to u} +opt(u)$ .</p>
<blockquote>
<p>If $opt(v)&lt;w_{v\to u}+opt(u)$ , then $v$ is updated by other $u’$ , so $first[v]\neq u$ .</p>
</blockquote>
<p>Therefore , $opt(v_1)\ge w_{v_1\to v_2}+opt(v_2)$ , $opt(v_2)\ge w_{v_2\to v_3}+opt(v_3)$ , $\cdots$ , $opt(v_k)\ge w_{v_k \to v_1} +opt(v_1)$ .</p>
<p>Therefore , $cost(C)=\sum_{i=1}^k w_{v_i\to v_{i+1}}\le 0$ .</p>
<p>Consider the time before we update the last cycle edge , suppose that it is $v_k\to v_1$ .</p>
<p>Therefore , exactly before the update , $opt(v_k)&gt;w_{v_k\to v_1}+opt(v_1)$ , so at this time the inequality is strict .</p>
</li>
<li><p>Note : If exists negative cycle , there may still exist finite shortest-path vertices and $+\infty$ shortest-path vertices . </p>
</li>
</ol>
<h2 id="Chapter-4-NP-Completeness"><a href="#Chapter-4-NP-Completeness" class="headerlink" title="Chapter 4 NP Completeness"></a>Chapter 4 NP Completeness</h2><ol>
<li><p>[ Cook , Karp ]</p>
<p>$NPC\subset NP$ , poly-time reduction</p>
<p>$P\neq NP$ conjecture</p>
</li>
</ol>
<h3 id="4-1-Polynomial-Time-Reduction"><a href="#4-1-Polynomial-Time-Reduction" class="headerlink" title="4.1 Polynomial Time Reduction"></a>4.1 Polynomial Time Reduction</h3><ol>
<li><p>using poly-time as measure ?</p>
<p>different computation model can have different power constant ( $\mathcal O(n^2,n^3,\cdots)$ )</p>
</li>
<li><p>Def [ <strong><em>polynomial-time reduction</em></strong> ] : If a problem $Y$ can be solved in poly-time plus an oracle that solves problem $X$ , then $Y$ is <strong><em>poly-time reducible</em></strong> to $X$ , denote as $Y\le_P X$ .</p>
<ol>
<li><p>Def [ <strong><em>oracle</em></strong> ] : an oracle for $X$ is a “black-box” , input an instance of $X$ and can output the answer in $\mathcal O(1)$ time .</p>
<blockquote>
<p>Even if $X$ itself cannot be solved in $\mathcal O(1)$ .</p>
</blockquote>
</li>
<li><p>$Y\le _P X$ : $X$ is more powerful than $Y$ ( i.e. $Y$ is not harder than $X$ )</p>
</li>
</ol>
</li>
<li><p>Properties</p>
<ol>
<li>If $Y\le_P X$ , $X$ is poly-solvable , then $Y$ is poly-solvable</li>
<li>If $Y\le_P X$ , $Y$ cannot be solved in poly-time , then $X$ cannot be solved in poly-time </li>
</ol>
</li>
<li><p>[ Cook , Karp ] : </p>
<p>Intuitive : If $Y\le_P X$ , then connect a directed edge $Y\to X$ . Then there exists a class of Problems $\mathcal C$ . $\forall C\in \mathcal C,\forall X\in \mathcal P$ , there exists a directed edge $X\to C$ . </p>
</li>
</ol>
<h3 id="4-2-Examples-of-Poly-reduction"><a href="#4-2-Examples-of-Poly-reduction" class="headerlink" title="4.2 Examples of Poly-reduction"></a>4.2 Examples of Poly-reduction</h3><ol>
<li><p>Independent Set Problem</p>
<p>$IS=(G(V,E),k)$ . </p>
<p>Independent set :  $I\subseteq V$ , s.t. $\forall u,v\in I,(u,v)\notin E$ .</p>
<p>Ask whether there exists an Independent Set of size at least $k$ . </p>
</li>
<li><p>Vertex Cover Problem</p>
<p>$VC=(G(V,E),h)$ .</p>
<p>Vertex Cover : $C\subseteq V$ , s.t. $\forall (u,v)\in E$ , either $u\in C$ or $v\in C$ .</p>
<p>Ask whether there exists a Vertex Cover of size at most $h$ .</p>
</li>
<li><p>$IS\le_P VC$</p>
<p>Lemma : $I$ is an Independent Set $\Leftrightarrow$ $V\backslash I$ is a Vertex Cover</p>
</li>
<li><p>Set Cover Problem</p>
<p>$SC=(U,\{S_1,\cdots,S_m\},h)$ .</p>
<p>$U$ : universe . $S_1,\cdots,S_m\subseteq U$ .</p>
<p>Set Cover : $I\subseteq [m]$ , s.t. $\bigcup_{i\in I}S_i=U$ .</p>
<p>Ask whether there exists a Set Cover of size at most $h$ .</p>
</li>
<li><p>$VC\le_P SC$</p>
<p>Let $S_i=\{e\in E|i\in e\}$ , $U=E$ .</p>
</li>
</ol>
<h3 id="4-3-NP-Complete-Problem"><a href="#4-3-NP-Complete-Problem" class="headerlink" title="4.3 NP-Complete Problem"></a>4.3 NP-Complete Problem</h3><ol>
<li><p>Def [ <strong><em>NP</em></strong> ] : Only consider decision problems ( output Y/N )</p>
<p>A decision problem $X$ , we can consider $X$ as a collection of YES instances.</p>
<ol>
<li><p>Def [ <strong><em>Efficient Verifier</em></strong> ] : $V(x,\pi)$ is an <strong><em>efficient verifier</em></strong> for problem $X$ if</p>
<ul>
<li><p>$V$ is a poly-time algorithm with $x$ and $\pi$</p>
</li>
<li><p>$x\in X$ $\Leftrightarrow$ $\exists \pi , |\pi|\le poly(|x|) , V(x,\pi)=1$ </p>
</li>
</ul>
<p>$\pi$ : certificate / proof</p>
</li>
<li><p>Def [ <strong><em>P</em></strong> ] : The class of all problems $X$ , s.t. there exists a poly-time algorithm that solves $X$ .</p>
</li>
<li><p>Def [ <strong><em>NP</em></strong> ] : The class of all problems $X$ , s.t. there exists an efficient verifier for $X$ .</p>
</li>
</ol>
</li>
<li><p>Def [ <strong><em>NP-Complete</em></strong> ] : A problem $X$ is <strong><em>NPC</em></strong> if</p>
<ul>
<li>$X\in NP$</li>
<li>$\forall Y\in NP$ , $Y\le_P X$ </li>
</ul>
</li>
<li><p>Properties of NPC</p>
<ol>
<li>$\forall X\in NPC$ , if $X\in P$ , then $P=NP$</li>
<li>$\forall X,Y\in NPC$ , $X\le_P Y$ and $Y\le_P X$ </li>
</ol>
</li>
<li><p>Thm [ Cook-Levin 1971 , the first NPC problem ] : SAT is NPC</p>
<ol>
<li><p>Def [ <strong><em>SAT</em></strong> ] : a formula $F$ of $0/1$ variables $\{x_n\}$ and $\land,\lor,\lnot$ . </p>
<p>Fix some variables , determine whether there exists an assignment of $\{x_n\}$ s.t. $F=true$ .</p>
</li>
<li><p>Proof ( high-level sketch )</p>
<p>SAT $\in$ NP : given an assignment , can verify </p>
<p>$\forall X\in NP,X\le_P SAT$ :</p>
<p>efficient verifier for $X$ : can be write as logic-circuit form </p>
<p>$V(x,\pi)$ : $x$ as fixed , $\pi$ as assignment .</p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-图论-treewidth</tag>
        <tag>算法-图论-minor graph</tag>
        <tag>算法-图论-最短路(Bellman Ford)</tag>
        <tag>计算理论-多项式规约</tag>
        <tag>计算理论-P,NP,NPC</tag>
        <tag>计算理论-SAT</tag>
      </tags>
  </entry>
  <entry>
    <title>ZKP and MPC 2</title>
    <url>/2023/10/15/ZKP-and-MPC-2/</url>
    <content><![CDATA[<h2 id="Lec02-Zero-Knowledge-Proof-under-Composition"><a href="#Lec02-Zero-Knowledge-Proof-under-Composition" class="headerlink" title="Lec02  Zero-Knowledge Proof under Composition"></a>Lec02  Zero-Knowledge Proof under Composition</h2><h3 id="2-1-Review-of-Interactive-Proof"><a href="#2-1-Review-of-Interactive-Proof" class="headerlink" title="2.1 Review of Interactive Proof"></a>2.1 Review of Interactive Proof</h3><ol>
<li><p>Interactive Proof</p>
<ol>
<li><p>Framework</p>
<p>Prover $P$, Verifier $V$ . $P,V$ knows $x$ .</p>
<p>$P$ wants to convince $V$ that $x\in L$ with interaction .</p>
<p>$P,V$ sends messages $m_1,m_2,\cdots,m_k$ one by one , then $V$ accept/reject whether $x\in L$ .</p>
<p>$\braket{P,V}(x)=y\in \{0,1\}$ , indicating whether $x\in L$ .</p>
<p><strong><em>transcript</em></strong> : $\tau=\{m_1,m_2,\cdots,m_k\}$ .</p>
</li>
<li><p>Formal Definition</p>
<p>$(P,V)$ is an <strong><em>interactive proof</em></strong> for $L$ if </p>
<ul>
<li>$V$ is an efficient algorithm ( poly-time )</li>
<li><strong><em>Completeness</em></strong> : $\forall x\in L,\Pr\{\braket{P,V}(x)=1\}=1$</li>
<li><strong><em>(Almost) Soundness</em></strong> : $\forall x\notin L,\forall P^<em>,\Pr\{\braket{P^</em>,V}(x)=1\}&lt;\epsilon$</li>
</ul>
</li>
</ol>
</li>
<li><p>Error Toleration</p>
<ol>
<li><p>Security parameter : $\kappa$</p>
</li>
<li><p>For Efficiency : $|x|=poly(\kappa)$</p>
</li>
<li><p>For Security</p>
<ul>
<li><p>Def [<strong><em>Negligible Function</em></strong>] $\epsilon:\mathbb N\to \mathbb R^<em>$ is **</em>negligible<em>*</em> if for all positive polynomial $p$ , $\exists c$ , s.t. $\forall k\ge c,\epsilon(k)&lt;\frac{1}{p(k)}$</p>
</li>
<li><p>Def [<strong><em>Statistically Indistinguishable</em></strong>] : two distribution $f,g$ are <strong><em>statistically indistinguishable</em></strong> if there exists a negligible function $\epsilon$ that : </p>
<script type="math/tex; mode=display">
SD(f(x),g(x))=\frac{1}{2}\sum_{s}\left|\Pr\{f(x)=s\}-\Pr\{g(x)=s\}\right|<\epsilon</script></li>
<li><p>Def [<strong><em>Computationally Indistinguishable</em></strong>] : two distribution $f,g$ are <strong><em>computationally indistinguishable</em></strong> if $\forall D$ (distinguisher) in P.P.T , there exists a negligible function $\epsilon$ that : </p>
<script type="math/tex; mode=display">
\left|\Pr\left\{D\big(f(x)\big)=1\right\}-\Pr\left\{D\big(g(x)\big)=1\right\}\right|<\epsilon</script></li>
</ul>
</li>
</ol>
</li>
<li><p>Zero Knowledge</p>
<ol>
<li><p>Intuition : not gain of <strong>Knowledge</strong></p>
<blockquote>
<p>Everything can be <strong>computed locally</strong> is not a gain of knowledge</p>
</blockquote>
</li>
<li><p>Def [<strong><em>View</em></strong>] : </p>
<script type="math/tex; mode=display">
View_V^P(x)=(x,r,\tau)</script></li>
<li><p>Def [<strong><em>dishonest-verifier zero-knowledge</em></strong>] : $(P,V)$ is an IP for $L$ . If $(P,V)$ achieves perfect/statistical/computational <strong><em>dishonest-verifier zero-knowledge</em></strong> , it satisfies that :</p>
<p>$\forall V^<em>$ in P.P.T. , $\exists  $ expected poly-time randomized algorithm $M^</em>$ , s.t.</p>
<script type="math/tex; mode=display">
\forall x\in L,M^*(x)\sim View_{V^*}^P(x)</script><p>$\sim$ refers to equivalent / statistically indistinguishable / computationally indistinguishable</p>
</li>
<li><p>Alternative definition of zero-knowledge : </p>
<p>$\forall V^<em>$ in P.P.T. , $\exists $ expected poly-time randomized algorithm $M^</em>$ , s.t. </p>
<script type="math/tex; mode=display">
\forall x\in L,\braket{P,V^*}(x)\sim M^*(x)</script><blockquote>
<p><em>Proof of Equivalence</em></p>
<p>Original $\to$ Alternative : The output of $V^<em>$ only depends on its view . If $M^</em>$ can simulate the view of $V^<em>$ , then it can run $V^</em>$ with the view , and get the output of $V^*$ .</p>
<p>Alternative $\to$ Original : The proposition holds for all $V^<em>$ ( which can violate the protocol ) , so if $V^</em>$ just output its entire view , we still need to be able to find the corresponding $M^<em>$ , this means the ability to simulate the view of $V^</em>$ . </p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h3 id="2-2-Interactive-Proof-for-Graph-Isomorphism"><a href="#2-2-Interactive-Proof-for-Graph-Isomorphism" class="headerlink" title="2.2 Interactive Proof for Graph Isomorphism"></a>2.2 Interactive Proof for Graph Isomorphism</h3><ol>
<li><p>Protocol</p>
<p>|                  Prover                   |                |                           Verifier                           |<br>| :———————————————————-: | :——————: | :—————————————————————————————: |<br>|           $\pi_r\leftarrow S_n$           |                |                                                              |<br>|          $\tilde G:=\pi_r(G_0)$           | —$\tilde G$-&gt; |                                                              |<br>|                                           |    &lt;-$b$—     |                    $b\leftarrow \{0,1\}$                     |<br>| find $\pi_b$ , s.t. $\tilde G=\pi_b(G_b)$ |  —$\pi_b$-&gt;   |                                                              |<br>|                                           |                | $\begin{cases}1&amp;\tilde G=\pi_b(G_b)\\0&amp;\text{otherwise}\end{cases}$ |</p>
<blockquote>
<p>Note : If Prover knows $\pi$ that $\pi(G_0)=G_1$ , then $\pi_b$ can be constructed :</p>
<script type="math/tex; mode=display">
\pi_b=\begin{cases}\pi_r&b=0\\\pi_r\circ\pi^{-1}&b=1\end{cases}</script></blockquote>
</li>
<li><p>Completeness : By construction of $\pi_b$ above , $\Pr\{V\text{ accepts}\}=1$ .</p>
</li>
<li><p>Soundness : $\forall \tilde G$ , if $G_0\not\sim G_1$ , then either $\tilde G\not\sim G_0$ , or $\tilde G\not\sim G_1$ ,</p>
<p>If $\tilde G\not\sim G_{b^<em>}$ , then $\Pr\{V\text{ rejects}\}\ge \Pr\{b=b^</em>\}=\frac{1}{2}$</p>
<blockquote>
<p>Note : we cannot let Verifier just choose $b=1$ , since a malicious Prover can violate the protocol , $\pi_r$ may not be a permutation , and $\tilde G$ may not isomorphic to $G_0$ .</p>
</blockquote>
<p>Repeat $k$ times , $\Pr\{V\text{ rejects}\}\ge 1-\frac{1}{2^k}$ .</p>
</li>
<li><p>Honest-Verifier Zero-Knowledge : $P,V$ follows the protocol .</p>
<p>In $View_V^P$ , the additional info is $\tilde G,\pi_b$ , which are both randomly picked s.t. $\pi_b(G_b)=\tilde G$ .</p>
<p>$M$ : samples $b\gets\{0,1\}$ , samples $\pi_b\gets S_n$ , computes $\tilde G=\pi_b(G_b)$ .</p>
</li>
<li><p>Dishonest-Verifier Zero-Knowledge : $V^*$ may not follow the protocol : $b$ may be chosen depends on $\tilde G$ .</p>
<p>$M^*$ : perform as $P$ (but without knowing $\pi$) </p>
<ol>
<li><p><em>Strategy</em> :</p>
<ol>
<li><p>$M^<em>$ samples $b^</em>\gets \{0,1\}$ ,samples $\pi_{b^<em>}\gets S_n$ , computes $\tilde G=\pi_{b^</em>}(G_{b^*})$</p>
</li>
<li><p>$M^<em>$ runs with $V^</em>$ and obtains $b$ .</p>
</li>
<li><p>If $b^<em>=b$ , reply $\pi_{b^</em>}$ and find out the view .</p>
<p>If $b^*\neq b$ , go back to (1).</p>
</li>
</ol>
</li>
<li><p>Prove $M^<em>(x)\equiv View_{V^</em>}^P(x)$ :</p>
<p><em>Proof Strategy</em> : <strong><em>Hybrid Arguments</em></strong> : make sure that every two adjacent $Hyb$ are so near .</p>
<ol>
<li><p>$Hyb_0$ : $P$ runs with $V^<em>$ , output $View_{V^</em>}^P(x)$</p>
</li>
<li><p>$Hyb_1$ : $\tilde P$ runs with $V^<em>$ , receives $b$ . Then $\tilde P$ samples $b^</em>\gets \{0,1\}$ </p>
<p>If $b^<em>=b$ , $\tilde P$ continues with $V^</em>$ . Otherwise , $\tilde P$ reruns with $V^*$ .</p>
<ul>
<li>$\mathbb E\{\text{repitition time}\}=2$</li>
<li>$View_{V^<em>}^P(x)\equiv View_{V^</em>}^{\tilde P}(x)$</li>
</ul>
<blockquote>
<p>Proof : $\forall v$ , compute $\Pr\{View_{V^*}^{\tilde P}(x)=v\}$</p>
<p>Define $v_i$ : view of $V^*$ in the $i$-th iteration </p>
<p>If $\tilde P$ exactly finishes at $i$-th iteration , then $v_i\equiv View_{V^*}^{P}(x)$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Pr\{View_{V^*}^{\tilde P}(x)=v\}&=\sum_{l=1}^{+\infty} \Pr\{v_l=v\land b_l^*=b_l\land \forall i<l,b_i^*\neq b_i\}\\
&=\sum_{l=1}^{+\infty}\Pr\{v_l=v\}\Pr\{b_l^*=b_l\land \forall i<l,b_i^*\neq b_i\}\\
&=\Pr\{View_{V^*}^P(x)=v\}\sum_{l=1}^{+\infty}\Pr\{b_l^*=b_l\land \forall i<l,b_i^*\neq b_i\}\\
&=\Pr\{View_{V^*}^P(x)=v\}
\end{aligned}</script></blockquote>
</li>
<li><p>$Hyb_2$ : $\tilde P$ samples $b^<em>\gets \{0,1\}$ . Then $\tilde P$ runs with $V^</em>$ , receives $b$ .</p>
<p>If $b^<em>=b$ , $\tilde P$ continues with $V^</em>$ . Otherwise , $\tilde P$ reruns with $V^*$ .</p>
<ul>
<li>$\mathbb E\{\text{repitition time}\}=2$</li>
<li>$Hyb_1\equiv Hyb_2$ ( Since we only swap two independent operations )</li>
</ul>
</li>
<li><p>$Hyb_3$ : $\tilde P$ samples $b^<em>\gets\{0,1\}$ . Then $\tilde P$ samples $\pi_{b^</em>}$ and computes $\tilde G=\pi_{b^<em>}(G_{b^</em>})$ . Then $\tilde P$ runs with $V^*$ , receives $b$ .</p>
<p>If $b^<em>=b$ , $\tilde P$ continues with $V^</em>$ . Otherwise , $\tilde P$ reruns with $V^*$ .</p>
<ul>
<li>$\mathbb E\{\text{repitition time}\}=2$</li>
<li>$Hyb_2\equiv Hyb_3$ ( Since the distribution of $(\pi_{b^*},\tilde G)$ is the same )</li>
<li>$Hyb_3$ is exactly $M^*$ </li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
<li><p>Zero-knowledge under Repetition (for soundness)</p>
<ol>
<li><p>Def [<strong><em>parallel composition</em></strong>] : </p>
<p>|                         Prover                          |                                      |                           Verifier                           |<br>| :——————————————————————————-: | :—————————————————: | :—————————————————————————————: |<br>|       $\pi_r^i\leftarrow S_n$ for $i\in [\kappa]$       |                                      |                                                              |<br>|    $\tilde {G^i}:=\pi_r^i(G_0)$ for $i\in [\kappa]$     | —$\{\tilde {G^i}|i\in [\kappa]\}$-&gt; |                                                              |<br>|                                                         |               &lt;-$b$—                |                 $b\leftarrow \{0,1\}^\kappa$                 |<br>| find $\pi_b^i$ , s.t. $\tilde {G^i}=\pi_{b}^i(G_{b_i})$ |   —$\{\pi_b^i|i\in [\kappa]\}$-&gt;    |                                                              |<br>|                                                         |                                      | $\begin{cases}1&amp;\forall i\in [\kappa],\tilde {G^i}=\pi_b^i(G_{b_i})\\0&amp;\text{otherwise}\end{cases}$ |</p>
</li>
<li><p>Brute force simulator :</p>
<p>randomly sample $b^<em>\gets\{0,1\}^\kappa$ , if received $b\neq b^</em>$ , rerun .</p>
<p>Good : $M^<em>(x)\equiv View_{V^</em>}^P(x)$</p>
<p>Bad : $\mathbb E\{\text{repitition time}\}=2^{\kappa}$</p>
</li>
</ol>
<blockquote>
<p>For most ZKP , we don’t know how to prove <em>dishonest-verifier zero-knowledge</em> under parallel composition .</p>
</blockquote>
</li>
</ol>
<h3 id="2-3-Sequential-Composition"><a href="#2-3-Sequential-Composition" class="headerlink" title="2.3 Sequential Composition"></a>2.3 Sequential Composition</h3><ol>
<li><p>Def [<strong><em>sequential composition</em></strong>] : $(P,V)$ is an IP for $L$ , Let $(P_q,V_q)$ be $(P,V)$ repeat $q$ times .</p>
</li>
<li><p>Brute force simulator for Graph Isomorphism :</p>
<p>For $i=1,2,\cdots,\kappa$ :</p>
<ol>
<li><p>$M^<em>$ samples $b^</em>\gets\{0,1\}$ , samples $\pi_{b^<em>}\gets S_n$ , computes $\tilde G=\pi_{b^</em>}(G_{b^*})$</p>
</li>
<li><p>$M^<em>$ runs with $V^</em>$ and receives $b^*$</p>
</li>
<li><p>If $b=b^<em>$ , $M^</em>$ continues working as $P$</p>
<p>If $b\neq b^<em>$ , $M^</em>$ restarts from $i=1$</p>
</li>
</ol>
<blockquote>
<p>Bad : $\mathbb E\{\text{repitition time}\}=\mathcal O(2^{\kappa})$</p>
</blockquote>
</li>
<li><p>Idea : Not restarts from $i=1$ ?</p>
<p>Store the state of $V^*$ : $(input,memory,output,randomness)$</p>
<blockquote>
<p>We can store randomness of $V^<em>$ since it can be provided by $M^</em>$</p>
<p>The randomness of $V^*$ can be pre-determined</p>
</blockquote>
<p>Simulator $M^*$ :</p>
<p>For $i=1,2,\cdots,\kappa$ :</p>
<ol>
<li><p>$M^<em>$ saves the state $st_{i-1}$ of $V^</em>$</p>
</li>
<li><p>$M^<em>$ samples $b_i^</em>\gets\{0,1\}$ , samples $\pi_{b_i^<em>}\gets S_n$ , computes $\tilde G_i=\pi_{b_i^</em>}(G_{b_i^*})$</p>
</li>
<li><p>$M^<em>$ runs with $V^</em>$ and receives $b_i^*$</p>
</li>
<li><p>If $b_i=b_i^<em>$ , $M^</em>$ continues working as $P$ </p>
<p>If $b_i\neq b_i^<em>$ , $M^</em>$ reloads $st_{i-1}$ for $V^*$ and goes back to (2). </p>
</li>
</ol>
</li>
<li><p>Def [<strong><em>Interactive Proof with Auxiliary Input</em></strong>] : </p>
<p>$(P,V)$ is an <strong><em>interactive zk-proof with auxiliary input</em></strong> for $L$ if </p>
<ul>
<li><p>$V$ is an efficient algorithm ( poly-time )</p>
</li>
<li><p><strong><em>Completeness</em></strong> : $\forall x\in L,\forall y,z\in \{0,1\}^*,\Pr\{\braket{P(y),V(z)}(x)=1\}=1$</p>
</li>
<li><p><strong><em>(Almost) Soundness</em></strong> : $\forall x\notin L,\forall y,z\in \{0,1\}^<em>,\forall P^</em>,\Pr\{\braket{P^*(y),V(z)}(x)=1\}&lt;\epsilon$</p>
</li>
<li><p><strong><em>Zero-Knowledge</em></strong> : $\forall V^<em>$ in P.P.T. , $\exists$ expected poly-time algorithm $M^</em>$ , $\forall x\in L,\forall y,z\in \{0,1\}^*$ ,</p>
<script type="math/tex; mode=display">
M^*(x,z)\sim View_{V^*(z)}^{P(y)}(x)</script></li>
</ul>
</li>
<li><p>Lemma [<strong><em>Composition Lemma</em></strong>] :</p>
<p>$(P,V)$ : zero-knowledge IP for $L$ , $q$ is a polynomial of $\kappa$ </p>
<p>$(P_q,V_q)$ : An IP runs $(P,V)$ $q$ times</p>
<p>Claim : $(P_q,V_q)$ are still zero-knowledge .</p>
<blockquote>
<p>Proof Intuition </p>
<p>In the $i$-th iteration : $V$ runs like $V_i^<em>(x,st_{i-1})$ , construct corresponding $M_i^</em>(x,st_{i-1})$ </p>
<p>Prove $M_i^<em>(x,st_{i-1})\sim View_{V_i^</em>}^P(x)$</p>
</blockquote>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>零知识证明和多方安全计算</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>密码学-零知识证明</tag>
        <tag>密码学-交互式证明</tag>
        <tag>密码学-证明复合</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 4</title>
    <url>/2023/10/09/Algorithm-Design-4/</url>
    <content><![CDATA[<h2 id="Chapter-2-Greedy-Algorithm"><a href="#Chapter-2-Greedy-Algorithm" class="headerlink" title="Chapter 2 Greedy Algorithm"></a>Chapter 2 Greedy Algorithm</h2><h3 id="2-7-Optimal-Caching"><a href="#2-7-Optimal-Caching" class="headerlink" title="2.7 Optimal Caching"></a>2.7 Optimal Caching</h3><ol>
<li><p>Description</p>
<ul>
<li><p>U : $n$ pieces of distinct elements stored in main memory</p>
</li>
<li><p>cache : can hold $k&lt;n$ pieces</p>
<p>init : cache holds $k$ elements </p>
</li>
<li><p>need to process a sequence of elements $d_1,\cdots,d_m$ ( $m$ can $\ge n$ , not distinct)</p>
</li>
<li><p>when processing $d_i$ : </p>
<ul>
<li>If $d_i$ already in cache : cache hit , do nothing to the cache</li>
<li>If $d_i$ not in cache : cache miss , evict some other element from the cache</li>
</ul>
</li>
<li><p>Goal : minimize # of cache miss .</p>
</li>
</ul>
</li>
<li><p>Greedy Algorithm</p>
<ol>
<li><p>FF Strategy : evict the element that will be used the furthest in the future .</p>
</li>
<li><p>Proof [Exchange Argument] :</p>
<p>See BOOK</p>
<blockquote>
<p>Suppose our solution $S_{FF}$ differs with the optimal solution $S$ firstly at $d_i$ , which is a cache miss , and our solution evicts $a$ and the optimal solution evicts $b$ , where the future used time : $t(a)&gt;t(b)$ . Let’s prove that we can construct a new solution $S’$ that is same with our solution at $d_i$ as well , and is not worse than the optimal solution $S$.</p>
<p>We do not need to care about other caches and other elements . We only need to care about $a,b$ . $S=S’$ except the following cases :</p>
<p>Case 0 : At $d_i$ , $S’$ should evict $a$ rather than $b$ </p>
<p>Case 1 : $d_j$ let $a$ evicted in $S$ before the first future used time $t(a)$ . Then let $S’$ evicts $b$ at the same time , $S$ is the same as $S’$ .</p>
<p>Case 2 : At $t(b)$ . If at this time , $S$ evicts $a$ , then $S’$ can do nothing and save one cache miss . If at this time , $S$ evicts $c\neq a$ , then we can let $S’$ evicts $c$ as well , and then let $a$ “waiting” until it is used at $t(a)$ or evicted later .</p>
</blockquote>
</li>
</ol>
</li>
<li><p>In reality : we cannot know the sequence ahead</p>
<p>LRU : least recently used </p>
<p>using locality of reference</p>
<p>[ Slater , Tarjan ] : LRU is the earliest online solution (elements come one by one)</p>
<p>$LRU\le k(FF+1)$ , $k$ is the cache size .</p>
</li>
</ol>
<h3 id="2-8-Minimum-Cost-Arborescence"><a href="#2-8-Minimum-Cost-Arborescence" class="headerlink" title="2.8 Minimum Cost Arborescence"></a>2.8 Minimum Cost Arborescence</h3><p>a.k.a 最小树形图</p>
<ol>
<li><p>Description</p>
<p>Find a min-cost directed spanning tree rooted at $r$ in a directed graph $G$ .</p>
<p>Assumption : $\forall e\in E,w(e)\ge 0$</p>
</li>
<li><p>Algorithm</p>
<ul>
<li><p>For $v\in V\backslash \{r\}$ , choose the in-edge with minimum weight $e_v$ .</p>
<p>Let $F^<em>$ be the set of chosen edge , so $cost(F^</em>)\le OPT$ .</p>
<p>If $F^*$ is already a tree , we get the optimal solution .</p>
</li>
<li><p>Otherwise , for all $v\in V\backslash\{r\}$ , let $w’(e=(u,v))=w(u,v)-w(e_v)$ , and then contract zero-cost cycles to get a new graph $G’$ .</p>
</li>
<li>Process $G’$ as above .</li>
</ul>
</li>
<li><p>Extension : matrix-tree theorem</p>
<p>count # of spanning tree in a graph (either directed or undirected)</p>
<p>$Det(L_{00})$</p>
<p>variant : count # of spanning tree in a graph with given total weight</p>
</li>
</ol>
<h2 id="Chapter-3-Dynamic-Programming"><a href="#Chapter-3-Dynamic-Programming" class="headerlink" title="Chapter 3 Dynamic Programming"></a>Chapter 3 Dynamic Programming</h2><h3 id="3-1-weighted-interval-scheduling-problem"><a href="#3-1-weighted-interval-scheduling-problem" class="headerlink" title="3.1 weighted interval scheduling problem"></a>3.1 weighted interval scheduling problem</h3><ol>
<li><p>Description</p>
<p>Input : $n$ intervals $[l_i,r_i]$ with weight $w_i$</p>
<p>Goal : maximize the total weight s.t. the chosen intervals are distinct </p>
</li>
<li><p>key : Dynamic Programming recursion</p>
<ol>
<li>sort the jobs according to $r_i$ (like greedy algorithm)</li>
<li>define $opt(j)$ : the optimal value for the first $j$ intervals </li>
</ol>
<script type="math/tex; mode=display">
opt(i)=\max\{opt(i-1),opt(p(i))+w_i\}</script><p>where $p(i)=\max\{j|r_j&lt;l_i\}$ .</p>
<ol>
<li><p>If we don’t use interval $i$ , it is $opt(i-1)$ .</p>
<p>If we use interval $i$ , since $\{r_i\}$ non-decreasing , $opt(p(i))$ contains all valid optimal solutions before .</p>
</li>
</ol>
</li>
<li><p>Implementation</p>
<p>naive :</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">compute_opt</span><span class="params">(<span class="type">int</span> i)</span></span>&#123;</span><br><span class="line">   <span class="keyword">if</span>(i==<span class="number">0</span>)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">else</span> <span class="keyword">return</span> <span class="built_in">max</span>(<span class="built_in">compute_opt</span>(i<span class="number">-1</span>),w[i]+<span class="built_in">compute_opt</span>(<span class="built_in">p</span>(i)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Problem : can be exponential time !</p>
<p>Key : many redundant computation $\to$ store them</p>
</blockquote>
<p>memorization / filling out the DP table : </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> opt[]=&#123;<span class="number">-1</span>&#125;;</span><br><span class="line"><span class="comment">// recursive implementation </span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">compute_opt</span><span class="params">(<span class="type">int</span> i)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(i==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(opt[i]!=<span class="number">-1</span>)<span class="keyword">return</span> opt[i];</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> opt[i]=<span class="built_in">max</span>(<span class="built_in">compute_opt</span>(i<span class="number">-1</span>),w[i]+<span class="built_in">compute_opt</span>(<span class="built_in">p</span>(i)));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// non-recursive implementation</span></span><br><span class="line">&#123;</span><br><span class="line">    opt[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;++i)&#123;</span><br><span class="line">        opt[i]=<span class="built_in">max</span>(opt[i<span class="number">-1</span>],w[i]+opt[<span class="built_in">p</span>(i)]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="3-2-Subset-Sum-Knapsack"><a href="#3-2-Subset-Sum-Knapsack" class="headerlink" title="3.2 Subset Sum / Knapsack"></a>3.2 Subset Sum / Knapsack</h3><ol>
<li><p>Description</p>
<p>Input : $n$ items , item $i$ has weight $w_i$ and value $v_i$ ; Weight limit $W$ </p>
<p>Goal : find a subset of items $S$ s.t.  $\sum_{i\in S}w_i\le W$ and maximize $\sum_{i\in S}v_i$ .</p>
</li>
<li><p>DP Algorithm</p>
<ol>
<li><p>pseudo-poly time / poly time</p>
<p>pseudo-poly time : $poly(n,W)$</p>
<p>poly time : $poly(n,\log W)$ ( $\log W$ is the least number of bits to input $W$ )</p>
<blockquote>
<p>Knapsack : NP-Hard : hard to find poly time algorithm</p>
<p>But finding a pseudo-poly time algorithm is easy !</p>
</blockquote>
</li>
<li><p>$opt(i,w)$ : the optimal solution for first $i$ items and weight $= w$ </p>
<script type="math/tex; mode=display">
opt(i,w)=\max\begin{cases}opt(i-1,w)&\text{not using i-th item}\\
opt(i-1,w-w_i)+v_i&\text{using i-th item}
\end{cases}</script></li>
<li><p>Time Complexity : $\mathcal O(nW)$ .</p>
</li>
</ol>
</li>
<li><p>Implementation</p>
<p>$opt(w)$ : at $i$-th iteration : the optimal solution for first $i$ items and weight $= w$ </p>
<script type="math/tex; mode=display">
opt(w)=\max\{opt(w),opt(w-w_i)+v_i\}</script><p>iterate $w$ decreasingly </p>
<p><img src="\images\posts\AD4_fig1.jpg" alt=""></p>
<p>Memory Complexity : $\mathcal O(n+W)$ </p>
</li>
</ol>
<h3 id="3-3-RNA-secondary-structure"><a href="#3-3-RNA-secondary-structure" class="headerlink" title="3.3 RNA secondary structure"></a>3.3 RNA secondary structure</h3><ol>
<li><p>Description</p>
<p>Input : $\{A,C,G,U\}^n$</p>
<p>Constraints :</p>
<ul>
<li>no sharp turns : If pairing $(i,j)\in S$ , then $i&lt;j-4$</li>
<li>pairing : $A-G$ , $C-U$</li>
<li>one base can only appear in $\le 1$ pairs</li>
<li>no crossing pairing : $\not\exist (i,j),(l,r)\in S,i&lt;l&lt;j&lt;r$ </li>
</ul>
<p>Goal : maximize the size of pairing set $S$ </p>
</li>
<li><p>Algorithm</p>
<p>$opt(l,r)$ : the maximum number of pairs in subsequence $[l,r]$ </p>
<script type="math/tex; mode=display">
opt(l,r)=\max\begin{cases}
opt(l+1,r-1)&a[l]\text{ and }a[r] \text{ can pairing}\\
\max_{l\le i\le r-1}\{opt(l,i)+opt(i+1,r)\} 
\end{cases}</script><p>Order : $r-l$ increasing</p>
<p>Time Complexity : $\mathcal O(n^3)$</p>
<p>Memory Complexity : $\mathcal O(n^2)$</p>
</li>
</ol>
<h3 id="3-4-DP-on-tree-and-tree-like-graph"><a href="#3-4-DP-on-tree-and-tree-like-graph" class="headerlink" title="3.4 DP on tree and tree-like graph"></a>3.4 DP on tree and tree-like graph</h3><h4 id="3-4-1-maximum-independent-set-on-tree"><a href="#3-4-1-maximum-independent-set-on-tree" class="headerlink" title="3.4.1 maximum independent set on tree"></a>3.4.1 maximum independent set on tree</h4><ol>
<li><p>Description</p>
<p>Input : A tree , vertex-weighted </p>
<p>Goal : Find an Independent Set of maximum weight</p>
</li>
<li><p>Algorithm</p>
<p>$opt_{0/1}(x)$ : the maximum-weighted independent set of the subtree $x$ , $opt_1$ means we choose $x$ , $opt_0$ means we don’t choose $x$ .</p>
<script type="math/tex; mode=display">
opt_1(x)=w_x+\sum_{y\in child(x)}opt_0(y)\\
opt_0(x)=\sum_{y\in child(x)}\max\{opt_0(y),opt_1(y)\}</script></li>
<li><p>Tree DP View : </p>
<ul>
<li>subtree is the natural subinstance</li>
<li>delete $x$ , the tree will be separated into several parts , no connection between $subtree(y)$ ($y\in child(x)$) , $G\backslash subtree(x)$ </li>
</ul>
</li>
</ol>
<h4 id="3-4-2-treewidth"><a href="#3-4-2-treewidth" class="headerlink" title="3.4.2 treewidth"></a>3.4.2 treewidth</h4><ol>
<li><p>Tree Decomposition ( clique tree , junction trees )</p>
<ol>
<li><p>Def [separator] : For connected graph $G$ , a separator is a vertex set $S\subset V$ , s.t. $G\backslash S$ is not connected . </p>
</li>
<li><p>Def [tree decomposition] : $T(G)$ is a tree decomposition if :</p>
<p>Each vertex in $T(G)$ is called a bag $(V_T(x),E_T(x))$ : containing some vertices in $G$ and their edges</p>
<ul>
<li>$\cup_x V_T(x)=V$</li>
<li>$\forall e\in E,\exists x\in T(G) , e\in E_T(x)$</li>
<li>$\forall v\in V,\{x|v\in V_T(x)\}$ form a connected subtree</li>
</ul>
</li>
<li><p>Def [treewidth for tree decomposition] : $tw(T(G)):=\max_x \{|V_T(x)|\}-1$</p>
</li>
<li><p>Lemma : If $G$ is a tree , $tw(T(G))=1$</p>
</li>
<li><p>Def [treewidth for graph] : </p>
<script type="math/tex; mode=display">
tw(G)=\min\limits_{T(G)\text{ is a tree decomposition}}tw(T(G))</script></li>
</ol>
</li>
<li><p>Lemma [ bags and separators ] : $T(G)$ is a tree decomposition of $G$ , $A,B$ are adjacent bags in $T(G)$ , then </p>
<p>$V_T(A)$ is a separator of $G$ , $V_T(A\cap B)$ is a separator of $G$</p>
</li>
<li><p>Independent Set on a graph with a tree decomposition</p>
<p>Input : given $G$ and a tree decomposition $T(G)$</p>
<p>Goal : Find maximum Independent Set in Time Complexity $\mathcal O(n2^{tw(T(G))})$</p>
</li>
<li><p>Algorithm</p>
<ul>
<li><p>$opt(x,S)$ : consider the subtree $x$ in the tree decomposition , choosing exactly $S$ in $V_T(x)$ , the maximum independent set </p>
</li>
<li><script type="math/tex; mode=display">
opt(x,S)=w(S)+\sum_{y\in child(x)}\max_{S' , S\text{ consists with } S'\text{ on }S\cap S'}\{opt(y,S')-w(S\cap S')\}</script></li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-贪心</tag>
        <tag>算法-贪心-最优缓存</tag>
        <tag>算法-图论-最小树形图</tag>
        <tag>算法-动态规划</tag>
        <tag>算法-动态规划-线性DP</tag>
        <tag>算法-动态规划-背包</tag>
        <tag>算法-动态规划-区间DP</tag>
        <tag>算法-动态规划-树形DP</tag>
        <tag>算法-图论-树分解</tag>
      </tags>
  </entry>
  <entry>
    <title>Probability and Statistics 2</title>
    <url>/2023/10/08/Probability-and-Statistics-2/</url>
    <content><![CDATA[<h2 id="Chapter-1-Background-in-Probability"><a href="#Chapter-1-Background-in-Probability" class="headerlink" title="Chapter 1 Background in Probability"></a>Chapter 1 Background in Probability</h2><h3 id="1-2-Random-Variables"><a href="#1-2-Random-Variables" class="headerlink" title="1.2 Random Variables"></a>1.2 Random Variables</h3><ol>
<li><p>measurable map</p>
<ol>
<li><p>Remark of Thm [a sufficient condition for measurable map]</p>
<blockquote>
<p>If $\mathcal S$ is a $\sigma$-field , then $\{X^{-1}(B)|B\in \mathcal S\}$ is a $\sigma$-field</p>
<p>Def : [generation of measurable map] $\sigma(X)$ is the $\sigma$-field generated by $X$ </p>
<script type="math/tex; mode=display">
\sigma(X)=\{X^{-1}(B)|B\in \mathcal S\}</script><p>$\sigma(X)$ is the smallest $\sigma$-field on $\Omega$ that makes $X$ a measurable map to $(S,\mathcal S)$ .</p>
</blockquote>
</li>
<li><p>Thm [measurable map composition] : If $X:(\Omega,\mathcal F)\to(S,\mathcal S)$ , $f:(S,\mathcal S)\to (T,\mathcal T)$ are both measurable maps , then $f\circ X$ is a measurable map $(\Omega,\mathcal S)\to(T,\mathcal T)$ .</p>
<blockquote>
<p>Proof : $(f\circ X)^{-1}(B)=X^{-1}(f^{-1}(B))$ , $C:=f^{-1}(B)\in \mathcal S$ , $X^{-1}(C)\in \mathcal F$ </p>
</blockquote>
</li>
<li><p>Cor : If $X_1,\cdots,X_n$ are random variables , $f:(\mathbb R^n,\mathcal R^n)\to(\mathbb R,\mathcal R)$ is measurable , then $f(X_1,\cdots,X_n)$ is a random variable .</p>
</li>
<li><p>(*) Cor : $X_1,\cdots,X_n$ are random variables , then $X_1+\cdots+X_n$ is a random variable .</p>
</li>
<li><p>(*) Thm : $X_1,\cdots$ are random variables , then $\inf_n X_n$ , $\sup_n X_n$ , $\liminf_n X_n$ , $\limsup_n X_n$ are random variables</p>
<blockquote>
<p>Proof : $\{\inf_n X_n&lt;a\}=\cup_n\{X_n&lt;a\}\in\mathcal F$ .</p>
<script type="math/tex; mode=display">
\liminf_{n\to \infty}X_n=\sup_n\left(\inf_{m\ge n}X_m\right)</script><p>$Y_n=\inf_{m\ge n}X_m$ is a random variable .</p>
</blockquote>
</li>
</ol>
</li>
<li><p>(*) Generalization of random variable </p>
<ol>
<li><p>Def : [Converges almost surely] </p>
<p>From Thm above , the following set is a measurable set</p>
<script type="math/tex; mode=display">
\Omega_o:=\{w:\lim_{n\to\infty}X_n\text{ exists}\}=\{w:\limsup_{n\to\infty}X_n-\liminf_{n\to\infty}X_n=0\}</script><p>If $P(\Omega_o)=1$ , then $X_n$ converges almost surely ( a.s. ) .</p>
<blockquote>
<p>即一列 $X_n$ 是几乎处处收敛的 当且仅当 $X_n$ 不收敛的点集测度为 $0$ </p>
</blockquote>
<p>Def : $X_\infty:=\limsup_{n\to \infty} X_n$ </p>
<p>Problem : $X_{\infty}$ can have value $\pm\infty$ .</p>
</li>
<li><p>Def : [Generalized random variable] </p>
<p>A function whose domain is $D\in \mathcal F$ and range is $\mathbb R^<em>:=[-\infty,+\infty]$ is a random variable if $\forall B\in \mathcal R^</em> , X^{-1}(B)\in \mathcal F$ .</p>
<p>Def : [Extended Borel set ] : $\mathcal R^*$ is generated by intervals of form $[-\infty,a),(a,b),(b,+\infty]$ , $a,b\in \mathbb R$ .</p>
<blockquote>
<p>extended real line $(\mathbb R^<em>,\mathcal R^</em>)$ is a measurable space </p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h3 id="1-3-Distribution"><a href="#1-3-Distribution" class="headerlink" title="1.3 Distribution"></a>1.3 Distribution</h3><ol>
<li><p>Definition of Distribution</p>
<ol>
<li><p>Def : [distribution] $X$ is a random variable , then let $\mu=P\circ X^{-1}$ ( $\mu(A)=P(X\in A)$ ) , $\mu$ is a probability measure called distribution .</p>
</li>
<li><p>Check $\mu$ is a probability measure $(\mathbb R,\mathcal R)\to (\mathbb R,\mathcal R)$ .</p>
<ul>
<li><p>$\mu(A)=P(\{w\in \Omega:X(w)\in A\})\ge 0$ </p>
</li>
<li><p>If $A_i\in \mathcal R$ are disjoint countable sequence , then $X^{-1}(A_i)$ are also disjoint </p>
<script type="math/tex; mode=display">
\begin{aligned}
\mu(\cup_i A_i)&=P(X^{-1}(\cup_i A_i))\\
&=P(\cup_i X^{-1}(A_i))\\
&=\sum_{i} P(X^{-1}(A_i))\\
&=\sum_{i} \mu(A_i)
\end{aligned}</script></li>
</ul>
</li>
</ol>
</li>
<li><p>distribution function</p>
<ol>
<li><p>Def : [distribution function]  Let $F(x)=P(X\le x)$ , $F(x)$ is the distribution function .</p>
<blockquote>
<p>$F(x)=P(X\le x)$ : Let $A_x=(-\infty,x]$ , $F(x)=P(X^{-1}(A_x))=\mu(A_x)$</p>
<p>$F(x)$ can be viewed as the CDF of $\mu$ / the Stieltjes measure function </p>
</blockquote>
</li>
<li><p>Thm [Props of distribution function] : Let $F$ be a distribution function</p>
<ul>
<li><p>$F$ : non-decreasing</p>
</li>
<li><p>$\lim\limits_{x\to -\infty }F(x)=0$ , $\lim\limits_{x\to+\infty}F(x)=1$</p>
</li>
</ul>
<blockquote>
<p>$x\to -\infty , \{X\le x\}\downarrow \varnothing$ , $x\to+\infty , \{X\le x\}\uparrow \Omega$</p>
</blockquote>
<ul>
<li>$F$ : right continuous , $F(x^+)=\lim_{y\downarrow x}F(y)=F(x)$</li>
</ul>
<blockquote>
<p>$y=x+\epsilon$ , $\{X\le y\}=\{X\le x+\epsilon\}\downarrow \{X\le x\}$</p>
</blockquote>
<ul>
<li>$F(x^-)=P(X&lt;x)$</li>
</ul>
<blockquote>
<p>$y=x-\epsilon$ , $\{X\le y\}=\{X\le x-\epsilon\}\uparrow \{X&lt;x\}$</p>
</blockquote>
<ul>
<li>$P(X=x)=F(x)-F(x^-)$</li>
</ul>
</li>
<li><p>Thm [Judgement of distribution function] : $F$ satisfies (i) , (ii) , (iii) is a distribution function</p>
<blockquote>
<p>Proof : [construction]</p>
<p>Let $\Omega=(0,1)$ , $\mathcal F$ is the corresponding Borel set , $P$ is Lebesgue measure , so $P((a,b])=b-a$ .</p>
<p>Let $X(w)=\sup\{y:F(y)&lt;w\}$ , we want to prove that $P(X\le x)=F(x)$ .</p>
<p>Since $F(x)=P(\{w:w\le F(x)\})$ , we want to prove that :</p>
<script type="math/tex; mode=display">
\{w:X(w)\le x\}=\{w:w\le F(x)\} \quad\quad \quad (*)</script><p>$R\subset L$ : $\forall w,w\le F(x)$ , and $X(w)=\sup\{y:F(y)&lt;w\}$ , so $F(X(w))\le w\le F(x)$ , so $X(w)\le x$ .</p>
<p>$R^c\subset L^c$ : $\forall w,w&gt;F(x)$ , and $X(w)=\sup\{y:F(y)&lt;w\}$ . </p>
<p>$F$ : right continuous , so $\exists \epsilon&gt;0$ , $F(x+\epsilon)<w$ , so $X(w)\ge x+\epsilon>x$ .</p>
</blockquote>
<p>Equation (*) means :</p>
<p><img src="/images/posts/PS2_fig1.png" alt=""></p>
</li>
<li><p>Remark </p>
<p>Each distribution function $F$ corresponds to a unique distribution measure $\mu$ </p>
<p>One distribution function $F$ can correspond to many different random variables</p>
<p>Def [equal in distribution] : If $X,Y$ have same distribution measure/function , then $X$ and $Y$ are equal in distribution , denote as $X\overset{d}{=}Y$ or $X=_d Y$ .</p>
</li>
</ol>
</li>
<li><p>Density function</p>
<ol>
<li><p>Def [density function] : when a distribution function $F(x)=P(X\le x)$ has the form </p>
<script type="math/tex; mode=display">
F(x)=\int_{-\infty}^x f(y)\ dy</script><p>then $X$ has the density function $f$ , denote as $f_X(x)$ .</p>
<blockquote>
<script type="math/tex; mode=display">
P(X=x)=\lim_{\epsilon\to0}\int_{x-\epsilon}^{x+\epsilon} f(y)dy=0</script></blockquote>
</li>
<li><p>Prop : (necessary and sufficient)</p>
<ul>
<li>$f(x)\ge 0$ </li>
<li>$\int_{-\infty}^{+\infty}f(x)dx=1$</li>
</ul>
</li>
</ol>
</li>
<li><p>Discrete / Continuous</p>
<p>A probability measure $P$ is discrete if there exists a countable set $S$ that $P(S^c)=0$ ( only non-zero on countable set) .</p>
<p>Discrete : usually $[a_i,b_i)$ segments like .</p>
</li>
</ol>
<h3 id="1-4-Integration"><a href="#1-4-Integration" class="headerlink" title="1.4 Integration"></a>1.4 Integration</h3><p>Intuition : Expectation needs Integration .</p>
<ol>
<li><p>Notations </p>
<ol>
<li><p>Def : $(\Omega,\mathcal F)$ , with measure $\mu$ , $f:(\Omega,\mathcal F)\to(\mathbb R,\mathcal R)$ . Denote the integration as $\int fd\mu$ .</p>
</li>
<li><p>Restriction for $\mu$ : should be $\sigma$-finite measure</p>
<p>e.g. Lebesgue measure is $\sigma$-finite : $A_i=[-i,i]$ , so $\mu(A_i)&lt;\infty$ and $\cup_i{A_i}=\mathbb R$ .</p>
</li>
<li><p>Restriction for $\int fd\mu$ :</p>
<p>(i) $\varphi\ge 0$ $\mu$-a.e. , then $\int\varphi d\mu\ge 0$</p>
<p>Def [almost everywhere] : $\mu$-a.e. : $\mu(\{w:\varphi(w)&lt;0\})=0$ .</p>
<p>(ii) $\int a\varphi d\mu=a\int \varphi d\mu$</p>
<p>(iii) $\int (\varphi+\psi)d\mu=\int \varphi d\mu+\int \psi d\mu$</p>
<p>(iv) $\varphi\le \psi$ $\mu$-a.e. , then $\int \varphi d\mu\le \int \psi d\mu$</p>
<p>(v) $\varphi=\psi$ $\mu$-a.e. , then $\int \varphi d\mu=\int \psi d\mu$</p>
<p>(vi) $\left|\int \varphi d\mu\right|\le \int |\varphi|d\mu$ </p>
</li>
<li><p>Thm : (i),(ii),(iii) can derive (iv),(v),(vi)</p>
</li>
</ol>
</li>
<li><p>Simple Function</p>
<ol>
<li><p>Def [simple function] : If $\varphi(\omega)=\sum_{i=1}^n a_i \mathbb 1_{A_i}$ , and $A_i$ are disjoint sets , $\mu(A_i)&lt;\infty$ </p>
</li>
<li><p>Def [simple function integration] : If $\varphi(\omega)=\sum_{i=1}^n a_i \mathbb 1_{A_i}$ , define </p>
<script type="math/tex; mode=display">
\int \varphi d\mu=\sum_{i=1}^n a_i \mu(A_i)</script></li>
<li><p>Check Props</p>
<blockquote>
<p>(i) : $\varphi\ge 0$ $\mu$-a.e. , so for all $A_i$ with $\mu(A_i)&gt;0$ , $a_i\ge 0$ .</p>
<p>(ii) : trivial</p>
<p>(iii) : Suppose $\varphi=\sum_{i=1}^m a_i \mathbb 1_{A_i}$ , $\psi=\sum_{j=1}^n b_j \mathbb 1_{B_j}$ </p>
<p>Define $A_0=\cup_{j} B_j-\cup_i A_i$ , $B_0=\cup_i A_i-\cup_j B_j$ . Let $a_0=b_0=0$ .</p>
<p>Therefore $\cup_{j=1}^n B_j\subset \cup_{i=0}^n A_i$ and $\cup_{i=1}^m A_i\subset \cup_{j=1}^n B_j$ .</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int(\varphi+\psi)d\mu&=\sum_{i=0}^m \sum_{j=0}^n (a_i+b_j)\mu(A_i\cap B_j)\\
&=\sum_{i=0}^m a_i\sum_{j=0}^n \mu(A_i\cap B_j)+\sum_{j=0}^nb_j\sum_{i=0}^m \mu(A_i\cup B_j)\\
&=\sum_{i=0}^m a_i\mu(A_i)+\sum_{j=0}^n b_j\mu(B_j)\\
&=\int \varphi d\mu +\int \psi d\mu
\end{aligned}</script></blockquote>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>概率与统计</category>
      </categories>
      <tags>
        <tag>实分析-测度-可测映射</tag>
        <tag>概率论-随机变量</tag>
        <tag>概率论-分布函数</tag>
        <tag>概率论-概率密度函数</tag>
        <tag>实分析-Lebesgue积分</tag>
      </tags>
  </entry>
  <entry>
    <title>Probability and Statistics 1</title>
    <url>/2023/10/08/Probability-and-Statistics-1/</url>
    <content><![CDATA[<h2 id="Chapter-0"><a href="#Chapter-0" class="headerlink" title="Chapter 0"></a>Chapter 0</h2><ol>
<li><p>Categories</p>
<ol>
<li><p>Background in Probability</p>
<ul>
<li><p>what is probability $\to$ measure theory</p>
</li>
<li><p>what is integration $\to $ Riemann / Lebesgue Integration</p>
</li>
<li><p>Expectation &amp; its properties</p>
</li>
</ul>
</li>
<li><p>Probability foundations of Asymptotic Statistics</p>
<ul>
<li>weak law of large numbers</li>
<li>strong law of large numbers (proof by Kolmoguor)</li>
<li>central limit theorem </li>
<li>characteristic function</li>
</ul>
</li>
<li><p>Estimation inference &amp; testing</p>
<ul>
<li>hypothesis testing</li>
<li>regression analysis</li>
<li>frontiers of statistical research (e.g. distribution of free test)</li>
</ul>
</li>
</ol>
</li>
<li><p>Textbook</p>
<p>[Durrett] Probability Theory &amp; Examples (or PTE)</p>
</li>
</ol>
<h2 id="Chapter-1-Background-in-Probability"><a href="#Chapter-1-Background-in-Probability" class="headerlink" title="Chapter 1 Background in Probability"></a>Chapter 1 Background in Probability</h2><h3 id="1-1-Probability-Space"><a href="#1-1-Probability-Space" class="headerlink" title="1.1 Probability Space"></a>1.1 Probability Space</h3><ol>
<li><p>Probability Space</p>
<ol>
<li><p>Def : $(\Omega,\mathcal F,P)$</p>
<p>$\Omega$ : set of “outcomes”</p>
<p>$\mathcal F$ : set of “events” , like subset of $\Omega$</p>
<p>$P$ : function : $\mathcal F\to [0,1]$</p>
</li>
<li><p>$\mathcal F$ should be a $\sigma$-field</p>
<ol>
<li><p>Def : [$\sigma$-field] A nonempty collection of subsets of $\Omega$ that</p>
<ul>
<li>补封闭：If $A\in \mathcal F$ , then $A^c\in \mathcal F$</li>
<li>可数无穷并封闭：If $A_i\in \mathcal F$ , $A_i$ is a countable sequence , then $\cup_i A_i\in \mathcal F$</li>
</ul>
</li>
<li><p>Prop : </p>
<ul>
<li>$\varnothing \in \mathcal F,\Omega\in \mathcal F$</li>
</ul>
<blockquote>
<p>Proof : $A\in\mathcal F$ $\to $ $A^c\in \mathcal F$ $\to$ $A\cup A^c\in \mathcal F$ $\to$ $\Omega\in \mathcal F$ $\to$ $\varnothing\in \mathcal F$ </p>
</blockquote>
<ul>
<li>可数无穷交封闭：If $A_i\in \mathcal F$ , , $A_i$ is a countable sequence , then $\cap_i A_i\in \mathcal F$</li>
</ul>
<blockquote>
<p>Proof : $A\cap B=(A^c\cup B^c)^c$</p>
</blockquote>
</li>
</ol>
</li>
</ol>
</li>
<li><p>Measurable Space</p>
<ol>
<li><p>Def : [measure] : non-negative , countably additive , set function</p>
<p>A function $\mu:\mathcal F\to \mathbb R$ with :</p>
<ul>
<li><p>$\forall A\in \mathcal F$ , $\mu(A)\ge \mu(\varnothing)=0$</p>
</li>
<li><p>If $A_i\in \mathcal F$ , $A_i$ countable <strong>disjoint</strong> sequence , then</p>
<script type="math/tex; mode=display">
\mu(\cup_i A_i)=\sum_{i}\mu(A_i)</script></li>
</ul>
</li>
<li><p>Def : [probability measure] : a measure $\mu$ with $\mu(\Omega)=1$</p>
</li>
<li><p>Thm : a measure $\mu$ on $(\Omega,\mathcal F)$ satisfies</p>
<ul>
<li><p>monotonicity : $A\subset B\Rightarrow \mu(A)\le \mu(B)$</p>
</li>
<li><p>subadditivity : $A\subset \cup_{i=1}^{\infty} A_i\Rightarrow \mu(A)\le \sum_{i=1}^{\infty}A_i$</p>
</li>
<li><p>continuity : </p>
<blockquote>
<p>Def : [$A_i\uparrow A$ ]</p>
<p>For set $A$ : $A_1\subset A_2\subset\cdots , \cup_i A_i=A$ </p>
<p>For real number $A$ : $A_1\le A_2\le \cdots,\lim_{n\to\infty}A_n=A$</p>
</blockquote>
<p>If $A_i\uparrow A$ , then $\mu(A_i)\uparrow \mu(A)$</p>
<p>If $A_i\downarrow A$ , then $\mu(A_i)\downarrow \mu(A)$</p>
</li>
</ul>
<blockquote>
<p>Proof :</p>
<p>(i) : Let $B-A=B\cap A^c$ , so if $A\subset B$ , then $B=A+(B-A)$ , and $A,B-A$ are disjoint</p>
<script type="math/tex; mode=display">
\mu(B)=\mu(A+(B-A))=\mu(A)+\mu(B-A)\ge \mu(A)</script><p>(ii) : Let $A_n’:=A_n\cap A$ , so $A=\cup_{i=1}^{\infty} A’_i$ . Let $B_n=\begin{cases}A_1’&amp;n=1\\A_n’-\cup_{i=1}^{n-1} A_i’&amp;n\ge 2\end{cases}$</p>
<p>Therefore , $B_n$ are disjoint , and $\cup_{i=1}^{\infty} B_i=\cup_{i=1}^{\infty} A_i’=A$</p>
<script type="math/tex; mode=display">
\mu(A)=\mu(\cup_{i} B_i)=\sum_{i}\mu(B_i)\le \sum_{i} \mu(A_i)</script><p>(iii) : Let $B_n=A_n-A_{n-1}$ , so $B_n$ are disjoint , $\cup_{i=1}^{n} B_i=A_n$ , $\cup_{i=1}^{\infty}B_i=A$</p>
<script type="math/tex; mode=display">
\mu(A)=\sum_{i=1}^{\infty} \mu(B_i)=\lim_{n\to\infty}\sum_{i=1}^n \mu(B_i)=\lim_{n\to \infty}\mu(A_n)</script></blockquote>
</li>
<li><p>E.g.1 Discrete Probability Space</p>
<p>$\Omega$ : countable set , $\mathcal F$ : the set of all subsets of $\Omega$ , $p: \Omega\to[0,1]$ , where $\sum_{\omega\in \Omega}p(\omega)=1$  .</p>
<script type="math/tex; mode=display">
P(A):=\sum_{\omega\in A}p(\omega)</script></li>
</ol>
</li>
<li><p>Measure on real line</p>
<ol>
<li><p>Def : [generate] $\mathcal A$ is a set of some subsets of $\Omega$. A $\sigma$-field is generated by $\mathcal A$ if it is the smallest $\sigma$-field containing $\mathcal A$ :</p>
<script type="math/tex; mode=display">
\sigma(\mathcal A):=\bigcap_{\mathcal A\subset\mathcal F,\mathcal F\text{ is }\sigma\text{-field}}\mathcal F</script></li>
<li><p>Def : [Borel Set]</p>
<p>Let $\mathcal A$ be the open subsets of $\mathbb R^d$ , Borel set is $\sigma(\mathcal A)$ , denoted as $\mathcal R^d$ .</p>
</li>
<li><p>measure for $d=1$</p>
<ol>
<li><p>Def : [Stieltjes measure function] $F:\mathbb R\to\mathbb R$ satisfies :</p>
<ul>
<li>non-decreasing : $\forall x\ge y , F(x)\ge F(y)$</li>
<li>right-continuous : $\lim_{y\downarrow x}F(y)=\lim_{y\to x^+}F(y)=F(x)$</li>
</ul>
</li>
<li><p>Thm : For all Stieltjes measure function $F$ , there is a unique measure $\mu$ on $(\mathbb R,\mathcal R)$ , with</p>
<script type="math/tex; mode=display">
\mu((a,b])=F(b)-F(a)</script></li>
<li><blockquote>
<p>When $F(x)=x$ , $\mu$ is Lebesgue measure</p>
<p>right-continuous : If $b_n\downarrow b$ , then $\cup_{n}(a,b_n]=(a,b_n]$ （可以保持右闭）</p>
</blockquote>
</li>
<li><p>Def [CDF] : For probability measure : $\lim\limits_{x\to -\infty}F(x)=0,\lim\limits_{x\to+\infty}F(x)=1$</p>
<p>$F$ : Cumulative Distribution Function [CDF] .</p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>(*)  semi-algebra , algebra , $\sigma$-algebra</p>
<ol>
<li>Def : [semi-algebra , algebra , $\sigma$-algebra]</li>
</ol>
</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">structure</th>
<th style="text-align:center">complement</th>
<th style="text-align:center">intersection/union</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">semi-algebra</td>
<td style="text-align:center">$S^c$ is a finite , disjoint union of sets in $\mathcal S$</td>
<td style="text-align:center">$S,T\in \mathcal S$ , then $S\cap T\in\mathcal S$</td>
</tr>
<tr>
<td style="text-align:center">algebra</td>
<td style="text-align:center">$A\in \mathcal A$ , then $A^c\in \mathcal A$</td>
<td style="text-align:center">$S,T\in \mathcal A$ , then $S\cap T,S\cup T\in \mathcal A$</td>
</tr>
<tr>
<td style="text-align:center">$\sigma$-algebra</td>
<td style="text-align:center">$A\in \mathcal F$ , then $A^c\in \mathcal F$</td>
<td style="text-align:center">$A_i\in \mathcal F$ , countable sequence , then $\cup_i A_i\in \mathcal F$</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><p>E.g. [algebra but not $\sigma$-algebra]</p>
<p>$\Omega=\mathbb Z$ , $\mathcal A=\{A\subset \Omega | A\text{ or }A^c\text{ is finite}\}$ </p>
<p>$\mathcal A$ is obviously algebra but not $\sigma$-algebra</p>
</li>
<li><p>Lemma</p>
<p>If $\mathcal S$ is a semi-algebra , then $\bar{\mathcal S}=\{\text{finite disjoint union of sets in }\mathcal S\}$ is algebra .</p>
<p>$\bar{\mathcal S}$ is called the algebra generated by $\mathcal S$ .</p>
<blockquote>
<p>Proof : easy to check two properties of algebra</p>
<p>Question : Is this generation the smallest generation ? </p>
</blockquote>
</li>
<li><p>Def : [measure for algebra] a measure $\mu$ on an algebra $\mathcal A$ satisfies :</p>
<ul>
<li><p>$\forall A\in \mathcal A , \mu(A)\ge \mu(\varnothing)=0$</p>
</li>
<li><p>If $A_i\in \mathcal A$ is a disjoint sequence , and $\cup_i A_i\in \mathcal A$ , then</p>
<script type="math/tex; mode=display">
\mu(\cup_i A_i)=\sum_{i}\mu(A_i)</script></li>
</ul>
<p>Def : [$\sigma$-finite] If there exists a sequence of sets $A_n\in \mathcal A $ , $\mu(A_n)&lt;\infty $ , $\cup_n A_n=\Omega$ .</p>
<blockquote>
<p>We can let $A_n’=\cup_{i=1}^n A_i$ , then $A_n’\uparrow \Omega$ .</p>
<p>We can let $A_n’=A_n\cap(\cap_{i=1}^{n-1}A_i^c)$ , then $A_n’$ are disjoint .</p>
<p>即，在构造这样的 $A_n$ 的时候，我们可以直接考虑 $A_n\uparrow \Omega$ 或 $A_n$ 不交</p>
</blockquote>
</li>
<li><p>Thm : $\mathcal S$ is a semi-algebra , $\mu$ defined on $\mathcal S$ with $\mu(\varnothing)=0$</p>
<p>(i) . If $\mu$ satisfies :</p>
<ul>
<li>If $S\in \mathcal S$ is a finite disjoint union of sets $S_i\in \mathcal S$ , then $\mu(S)=\sum_{i}\mu(S_i)$ </li>
<li>If $S_i,S\in \mathcal S$ , $S=+_{i\ge 1} S_i$ , then $\mu(S)\le \sum_{i\ge 1} \mu(S_i)$</li>
</ul>
<p>Then $\mu$ has a unique extension $\bar \mu$ that is a measure on $\bar{\mathcal S}$ .</p>
<p>(ii) . If $\bar\mu$ is $\sigma$-finite , then there is a unique extension $\hat \mu$ that is a measure on $\sigma(\mathcal S)$ .</p>
</li>
<li><p>Lemma : If $\mathcal S$ is a semi-algebra ,  $\mu$ defined on $\mathcal S$ with $\mu(\varnothing)=0$ . If  $S\in \mathcal S$ is a finite disjoint union of sets $S_i\in \mathcal S$ , then $\mu(S)=\sum_{i}\mu(S_i)$ . Then ,</p>
<ul>
<li>If $A,B_i\in \bar{\mathcal S}$ , $A=+_{i=1}^n B_i$ , then $\bar \mu(A)=\sum_{i=1}^n \bar\mu(B_i)$ </li>
<li>If $A,B_i\in \bar{\mathcal S}$ , $A\subset \cup_{i=1}^n B_i$ , then $\bar \mu(A)\le \sum_{i=1}^n \bar\mu(B_i)$ </li>
</ul>
<blockquote>
<p>相当于，上面 (i) 中如果第一个条件成立，对于有限情况下的 第二个条件 一定成立，并可以直接扩展到 $\bar{\mathcal S}$ 和 $\bar \mu$ 上 。</p>
</blockquote>
</li>
<li><p>可以借助 Thm , 证明 Stieltjes measure function 对应的 measure 存在，且证明过程需要左开。</p>
</li>
</ol>
<ol>
<li><p>(*) measure on $\mathbb R^d$</p>
<ol>
<li><p>直接采用类似 Stieltjes measure function 的条件构造 measure 是不够的</p>
<p>Restrictions : </p>
<ul>
<li>non-decreasing : If $\vec x\le \vec y$ ( $\forall i\in [d] , x_i\le y_i$) , then $F(\vec x)\le F(\vec y)$</li>
<li>right-continuous : Define $\vec y\downarrow \vec x$ as $\forall i\in [d] , y_i\downarrow x_i$ , then $\lim_{\vec y\downarrow \vec x}F(\vec y)=F(\vec x)$</li>
<li>(probability measure) $\lim\limits_{\vec x\downarrow -\infty}F(\vec x)=0$ , $\lim_{\vec x\uparrow +\infty}F(\vec x)=1$ </li>
</ul>
<p>Problem : </p>
<script type="math/tex; mode=display">
F(x_1,x_2)=\begin{cases}
1&x_1\ge 1,x_2\ge 1\\
\frac{2}{3}&x_1\ge 1,x_2\in [0,1)\\
\frac{2}{3}&x_1\in [0,1),x_2\ge 1\\
0&\text{otherwise}
\end{cases}</script><script type="math/tex; mode=display">
\mu((a_1,b_1]\times(a_2,b_2])=F(b_1,b_2)-F(a_1,b_2)-F(b_1,a_2)+F(a_1,a_2)</script><p>Let $a_1,a_2=1-\epsilon $ , $b_1,b_2=1$ , $\epsilon \to 0$ , then</p>
<script type="math/tex; mode=display">
\mu(\{1\}\times\{1\})=-\frac{1}{3}<0</script></li>
<li><p>Def : [ $\mathbb R^d$ measure ] </p>
<p>Consider finite rectangles $A=(a_1,b_1]\times\cdots\times(a_d,b_d]$ , $V=\{a_1,b_1\}\times\cdots\times\{a_d,b_d\}$ </p>
<p>If $v\in V$ , define </p>
<script type="math/tex; mode=display">
sgn(v)=(-1)^{|\{i\in [d]|v_i=a_i\}|}\\
\Delta_A F:=\sum_{v\in V}sgn(v)F(v)</script><p>let $\mu(A)=\Delta_A F$ .</p>
<blockquote>
<p>此处相当于 $d$ 维前缀和与差分，$V$ 相当于 $d$ 维矩形 $A$ 的所有顶点，$sgn(v)$ 相当于顶点 $v$ 有多少维是左顶点，然后容斥求差分。</p>
</blockquote>
</li>
<li><p>Thm : [$\mathbb R^d$ measure ]  If $F:\mathbb R^d\to [0,1]$ , satisfies the $3$ restrictions above , and for all rectangles $A$ , $\Delta_A F\ge 0$ . Then there is a unique probability measure $\mu$ on $(\mathbb R^d,\mathcal R^d)$ that $\mu(A)=\Delta_A F$ for all finite rectangles .</p>
</li>
<li><blockquote>
<p>If $F(\vec x)=\prod_{i=1}^d F_i(x_i)$ , $F_i$ are all Stieltjes measure function , then </p>
<script type="math/tex; mode=display">
\Delta_A F=\prod_{i=1}^d (F_i(b_i)-F_i(a_i))</script><p>When $F_i(x)=x$ for all $i\in [d]$ , $F$ is Lebesgue measure on $\mathbb R^d$ .</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h3 id="1-2-Random-Variables"><a href="#1-2-Random-Variables" class="headerlink" title="1.2 Random Variables"></a>1.2 Random Variables</h3><ol>
<li><p>measurable map</p>
<ol>
<li><p>Def : [measurable map] $X:\Omega\to S$ is a measurable map from $(\Omega,\mathcal F)$ to $(S,\mathcal S)$ if </p>
<script type="math/tex; mode=display">
\forall B\in \mathcal S , X^{-1}(B):=\{w\in \Omega|X(w)\in B\}\in \mathcal F</script><p>Def : [random vector] When $(S,\mathcal S)=(\mathbb R^d,\mathcal R^d)$ , $X$ is random vector .</p>
<p>Def : [random variable] When $(S,\mathcal S)=(\mathbb R,\mathcal R)$ , $X$ is a random variable .</p>
</li>
<li><blockquote>
<p>虽然 measurable map 写作 from $(\Omega,\mathcal F)$ to $(S,\mathcal S)$ ，但 $X$ 本身并不实现 $\mathcal F\to\mathcal S$ 的映射，只有 $\Omega\to S$ 的映射。 $\mathcal F,\mathcal S$ 是表明 measurable 的”范围” </p>
<p>Random variable is not a variable but a (measurable) map</p>
<p>这也很好解释了 $E(X^2)$ 这种类型的记号的实际含义</p>
</blockquote>
</li>
<li><p>Thm [a sufficient condition for measurable map]</p>
<p>$X:\Omega\to S$ , $\mathcal A $ : a collection of some subsets of $S$ , If</p>
<ul>
<li>$\forall A\in \mathcal A , X^{-1}(A)\in \mathcal F$</li>
<li>$\mathcal A$ generates $\mathcal S$</li>
</ul>
<p>Then $X$ is a measurable map from $(\Omega,\mathcal F)$ to $(S,\mathcal S)$ .</p>
<blockquote>
<p>Proof : Prove $\mathcal B=\{B\subset S|X^{-1}(B)\in \mathcal F\}$ is a $\sigma$-field , and obviously $\mathcal A\subset \mathcal B$ . Consider generation is the smallest , $\mathcal S\subset \mathcal B$ .</p>
</blockquote>
</li>
<li><p>E.g. $f:\mathbb R^d\to \mathbb R$ : $f(x_1,\cdots,x_d)=\sum_{i=1}^d x_i$ is a measurable map from $(\mathbb R^d,\mathcal R^d)$ to $(\mathbb R,\mathcal R)$ .</p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>概率与统计</category>
      </categories>
      <tags>
        <tag>概率论-概率空间</tag>
        <tag>实分析-测度</tag>
        <tag>实分析-代数空间</tag>
        <tag>实分析-测度-可测映射</tag>
        <tag>概率论-随机变量</tag>
      </tags>
  </entry>
  <entry>
    <title>答疑坊 线性代数 趣题若干</title>
    <url>/2023/10/07/%E7%AD%94%E7%96%91%E5%9D%8A-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0-%E8%B6%A3%E9%A2%98%E8%8B%A5%E5%B9%B2/</url>
    <content><![CDATA[<h3 id="T1"><a href="#T1" class="headerlink" title="T1"></a>T1</h3><p>类型：行列式计算，加行列技巧</p>
<h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>求矩阵 $A$ 的行列式，其中 $A$ 定义为：</p>
<script type="math/tex; mode=display">
A_{ij}=\begin{cases}a_i+b_j+1&i=j\\a_i+b_j\end{cases}</script><h4 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h4><p>（自己不会做，求助大佬的）</p>
<p><img src="/images/posts/DYF_Linear_Algebra_fig1.jpg" alt=""></p>
]]></content>
      <categories>
        <category>答疑坊</category>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>答疑坊</tag>
        <tag>线性代数</tag>
        <tag>线性方程组</tag>
        <tag>矩阵</tag>
        <tag>行列式</tag>
        <tag>对角化</tag>
      </tags>
  </entry>
  <entry>
    <title>答疑坊 微积分 趣题若干</title>
    <url>/2023/10/07/%E7%AD%94%E7%96%91%E5%9D%8A-%E5%BE%AE%E7%A7%AF%E5%88%86-%E8%B6%A3%E9%A2%98%E8%8B%A5%E5%B9%B2/</url>
    <content><![CDATA[<h3 id="P1"><a href="#P1" class="headerlink" title="P1"></a>P1</h3><p>类型：极限存在性</p>
<h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p>证明下面的极限存在：</p>
<script type="math/tex; mode=display">
\lim_{n\to +\infty}(1+\frac{1}{2^2})(1+\frac{1}{3^2})\cdots(1+\frac{1}{n^2})</script><h4 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h4><p>显然上述数列单调增，因此只需要证明有上界</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad \prod_{k=2}^n (1+\frac{1}{k^2})\\
&=\exp\left(\sum_{k=2}^n \ln (1+\frac{1}{k^2})\right)\\
&\le \exp\left(\sum_{k=2}^n \frac{1}{k^2}\right)\\
&\le \exp\left(\sum_{k=2}^n \frac{1}{k(k-1)}\right)\\
&=\exp\left(\sum_{k=1}^n \frac{1}{k-1}-\frac{1}{k}\right)\\
&=\exp(1-\frac{1}{n})\\
&\le e
\end{aligned}</script><p>其中第 $3$ 行使用常见不等式 $\ln(1+x)\le x$ ，第 $4\sim 6$ 行为裂项。</p>
<hr>
<h3 id="P2"><a href="#P2" class="headerlink" title="P2"></a>P2</h3><p>类型：$e$ 相关极限不等式</p>
<h4 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a>题目</h4><p>证明，对任意正整数 $n\ge 2$ ，</p>
<script type="math/tex; mode=display">
\sum_{k=0}^n \frac{1}{k!}-\frac{3}{2n}<(1+\frac{1}{n})^n<\sum_{k=0}^n \frac{1}{k!}</script><h4 id="解答-1"><a href="#解答-1" class="headerlink" title="解答"></a>解答</h4><p>使用二项式定理</p>
<script type="math/tex; mode=display">
\begin{aligned}
(1+\frac{1}{n})^n&=\sum_{k=0}^n \frac{n!}{k!(n-k)!}\frac{1}{n^k}\\
&=\sum_{k=0}^n \frac{1}{k!}\prod_{j=0}^{k-1}(1-\frac{j}{n})
\end{aligned}</script><p>显然对 $n\ge 2$ ,  $\prod_{j=0}^{k-1}(1-\frac{j}{n})&lt; 1$ ，故右侧不等式成立。</p>
<p>考虑 Bernoulli 不等式：</p>
<script type="math/tex; mode=display">
\forall a_1,\cdots,a_n>-1,且a_i 同号，则\prod_{k=1}^n (1+a_k)>1+\sum_{k=1}^n a_k</script><p>故</p>
<script type="math/tex; mode=display">
\begin{aligned}
(1+\frac{1}{n})^n&>\sum_{k=0}^n\left( \frac{1}{k!}-\frac{1}{k!}\sum_{j=1}^{k-1}\frac{j}{n}\right)\\
&=\sum_{k=0}^n \frac{1}{k!}-\sum_{k=0}^n \frac{1}{k!}\frac{k(k-1)}{2n}\\
&=\sum_{k=0}^n \frac{1}{k!}-\sum_{k=2}^n \frac{1}{(k-2)!}\frac{1}{2n}\\
&\ge \sum_{k=0}^n \frac{1}{k!}-\frac{1}{2n}\left(2+\sum_{k=2}^{n-2} \frac{1}{k(k-1)}\right)\\
&=\sum_{k=0}^n \frac{1}{k!}-\frac{1}{2n}(3-\frac{1}{n-2})\\
&>\sum_{k=0}^n \frac{1}{k!}-\frac{3}{2n}
\end{aligned}</script><hr>
]]></content>
      <categories>
        <category>答疑坊</category>
        <category>微积分</category>
      </categories>
      <tags>
        <tag>答疑坊</tag>
        <tag>微积分</tag>
        <tag>多元微积分</tag>
        <tag>级数</tag>
        <tag>微分方程</tag>
      </tags>
  </entry>
  <entry>
    <title>答疑坊 程设/离散/DSA 趣题若干</title>
    <url>/2023/10/07/%E7%AD%94%E7%96%91%E5%9D%8A-%E7%A8%8B%E8%AE%BE-DSA-%E8%B6%A3%E9%A2%98%E8%8B%A5%E5%B9%B2/</url>
    <content><![CDATA[<h3 id="P1"><a href="#P1" class="headerlink" title="P1"></a>P1</h3><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p>输入 $3n-1$ 个数，其中 $n-1$ 个数出现恰好 $3$ 次，$1$ 个数恰好出现 $2$ 次。求这个出现 $2$ 次的数。</p>
<p>要求，时间复杂度 $\tilde {\mathcal O}(n)$ , 空间复杂度 $\mathcal O(1)$ .</p>
<h4 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h4><p>考虑使用三进制不进位加法，显然满足交换律、结合律，且 $a\oplus a\oplus a=0$ ，出现 $3$ 次的数都抵消掉，只剩下恰好出现 $2$ 次的那个数 $a$ 的 “2倍”。</p>
<p>对最后结果 $sum$ 求 $sum\oplus sum$ 。因 $sum=a\oplus a$ , 故 $sum\oplus sum=a\oplus a\oplus a\oplus a=a$ 。</p>
]]></content>
      <categories>
        <category>答疑坊</category>
        <category>程设/离散/DSA</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>答疑坊</tag>
        <tag>程序设计</tag>
        <tag>离散数学</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 3</title>
    <url>/2023/10/07/Algorithm-Design-3/</url>
    <content><![CDATA[<h2 id="Chapter-2-Greedy-Algorithm"><a href="#Chapter-2-Greedy-Algorithm" class="headerlink" title="Chapter 2 Greedy Algorithm"></a>Chapter 2 Greedy Algorithm</h2><h3 id="2-3-Minimum-Spanning-Tree-undirected"><a href="#2-3-Minimum-Spanning-Tree-undirected" class="headerlink" title="2.3 Minimum Spanning Tree (undirected)"></a>2.3 Minimum Spanning Tree (undirected)</h3><ol>
<li><p>Definition</p>
<ol>
<li><p>Input : undirected , edge-weighted graph $G$</p>
</li>
<li><p>Output : Minimum Spanning Tree of $G$</p>
<p>Spanning Tree : a subgraph of $G$ that is a tree containing all vertices</p>
<p>Minimum : the sum of weights on tree’s all edges is minimized</p>
</li>
</ol>
</li>
<li><p>Properties</p>
<ol>
<li><p>Spanning Tree</p>
<ol>
<li>Connectivity $\leftrightarrow$ check connectivity $\to$ <strong>cut</strong></li>
<li>Acyclic</li>
</ol>
</li>
<li><p>Cut</p>
<ol>
<li><p>Def : Graph $G=(V,E),V=A\cup B,A\cap B=\varnothing$</p>
<p>$(A,B)$-cut : $E(A,B)=\{(u,v)\in E|u\in A,v\in B\}$</p>
</li>
<li><p>Observation : If $T$ is a Spanning Tree , $E(A,B)$ is any cut , then $T\cap E(A,B)\neq \varnothing$</p>
<blockquote>
<p>Otherwise , not connected</p>
</blockquote>
</li>
</ol>
</li>
<li><p>Lemma : Assumption : weights are distinct </p>
<p>Suppose $e=(u,v),u\in A,v\in B$ is the edge with minimum weight in $(A,B)$-cut , then every MST must contain $e$ .</p>
<p>Proof : [Exchange Argument , Prove by contradiction]</p>
<p>Suppose $T$ is a MST but $e\notin T$ . By the observation , there exists $e’\in T,e’\in E(A,B)$ .</p>
<p>Let $T’=T-e’+e$ , then $T$ is still connected , with smaller weight .</p>
</li>
</ol>
</li>
</ol>
<div class="note warning"><p>Problem : $T$ can have cycle !</p>
</div>
<div class="note success"><p>Correction :<br>Choose one specific $e’$ : $e\notin T$ , then $e$ and some edges in $T$ can form a cycle ( the path from $u$ to $v$ on $T$ ). This cycle must contain an edge $e’\in E(A,B)$ .</p>
</div>
<ol>
<li><p>Kruskal’s Algorithm</p>
<ol>
<li><p>Algorithm</p>
<p>Successively inserting edges from $E$ in increasing order of weight .</p>
<p>If edge $e$ would create a cycle . discard it .</p>
<p>(Using Union-Find Set)</p>
</li>
<li><p>Proof of Correctness</p>
<p>Consider every added edge $e$ , $e$ is the min-weight edge in some cut </p>
<p>$e=(u,v)$ , consider the connected component $C$ containing $u$ , then $e$ is the minimum weight edge in cut $E(C,V\backslash C)$ </p>
</li>
</ol>
</li>
<li><p>Prim’s Algorithm</p>
<ol>
<li><p>Algorithm</p>
<p>At each step , add the node that can be attached as cheaply as possible to the partial tree we already have .</p>
</li>
<li><p>Naive Implementation</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// d[x] : current minimum edge in cut E(&#123;x&#125;,S)</span></span><br><span class="line"><span class="comment">// Init </span></span><br><span class="line"><span class="keyword">for</span> (v in V)&#123;</span><br><span class="line">    d[v]=+inf;</span><br><span class="line">&#125;</span><br><span class="line">d[<span class="number">1</span>]=<span class="number">0</span>;S.<span class="built_in">insert</span>(<span class="number">1</span>);</span><br><span class="line"><span class="keyword">while</span>(S!=V)&#123;</span><br><span class="line">    <span class="type">int</span> x=<span class="built_in">argmin</span>(d[v] | v in V\S);</span><br><span class="line">    S.<span class="built_in">insert</span>(x); <span class="comment">// using the edge d[x]</span></span><br><span class="line">    <span class="keyword">for</span>(y in <span class="built_in">Neighbour</span>(x))&#123;</span><br><span class="line">        d[y]=<span class="built_in">min</span>(d[y],<span class="built_in">w</span>(x,y));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Improvement : Priority Queue</p>
</li>
</ol>
</li>
<li><p>Reverse Deletion</p>
<p>Successively removing edges from $E$ in decreasing order of weight .</p>
<p>If removing edge $e$ would cause disconnectedness , discard it  .</p>
<blockquote>
<p>The reverse version of Kruskal’s Algorithm</p>
</blockquote>
</li>
<li><p>Faster algorithms</p>
<p>[Chazelle] Deterministic Algorithm $\mathcal O(|E|\alpha(|E|))$</p>
<blockquote>
<p>This means MST is weaker than sorting </p>
</blockquote>
<p>[Karger , Klem , Tarjan] Randomized Algorithm $\mathcal O(|E|+|V|)$ </p>
<p><strong>Open Question</strong> : Deterministic Linear Algorithm </p>
</li>
</ol>
<h3 id="2-4-Union-Find-Set"><a href="#2-4-Union-Find-Set" class="headerlink" title="2.4 Union-Find Set"></a>2.4 Union-Find Set</h3><ol>
<li><p>Definition</p>
<p>Maintain sets of elements , support :</p>
<ol>
<li><code>find(element e)</code> , return the set that contains $e$</li>
<li><code>union(set Si,set Sj)</code> , combine set $S_i$ and $S_j$ , return the union set $S_i\cup S_j$</li>
</ol>
</li>
<li><p>Implementation</p>
<p>The sets can be viewed as trees .</p>
<p>The set can be represented by the root of the corresponding tree .</p>
<p><code>find</code> : find the root of the tree (keep finding father)</p>
<p><code>union</code> : $fa(root(S_1))\leftarrow root(S_2)$ </p>
<p>Problem : tree can be very deep</p>
</li>
<li><p>Improve</p>
<ol>
<li><p>启发式合并 / Heuristic merging</p>
<p>To make the tree shallow , suppose $|S_1|\le |S_2|$ </p>
<p>小集合合并到大集合上</p>
</li>
<li><p>路径压缩 / Path Compression</p>
<p>Suppose one find : $e,v_1,\cdots,v_k,root$ , after the find , let $fa(e)=fa(v_1)=\cdots=fa(v_k)=root$</p>
</li>
</ol>
</li>
<li><p>Time Complexity</p>
<p>Amortized Analysis / 均摊分析   ( using Path Compression and Heuristic merging)</p>
<p>Suppose we have a sequence of <code>find</code> or <code>union</code> operations , with length $m$ . </p>
<p>Total running time of union-find is  $\mathcal O(m\alpha(m,n))$</p>
<p>$\alpha$ : inverse Ackermann’s function , grows very slow </p>
<blockquote>
<p>Another slow function : $\log^*(n)$ , number of logs to make $n$ to small constant (i.e. $\log(\log\cdots\log(n))\le 5$)</p>
</blockquote>
<p>See [CLRS] for detailed proof .</p>
</li>
<li><p>Kruskal’s Algorithm Implementation</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sort</span>(E); <span class="comment">// weight-increasing</span></span><br><span class="line"><span class="keyword">for</span>(edge e=(u,v):E)&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">find</span>(u)!=<span class="built_in">find</span>(v))&#123;</span><br><span class="line">        T.<span class="built_in">insert</span>(e);</span><br><span class="line">        <span class="built_in">union</span>(<span class="built_in">find</span>(u),<span class="built_in">find</span>(v));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Total time complexity : $\mathcal O(|E|\log|E|)$ .</p>
</li>
</ol>
<h3 id="2-5-Priority-Queue"><a href="#2-5-Priority-Queue" class="headerlink" title="2.5 Priority Queue"></a>2.5 Priority Queue</h3><ol>
<li><p>Definition</p>
<p>Maintain a set of elements $S$ , support :</p>
<p><code>insert(S,x)</code> , insert $x$ into $S$ .</p>
<p><code>extract_max(S)</code> , return the maximum element in $S$ , and then remove this element .</p>
<p><code>max(S)</code> , return the maximum element in $S$</p>
<p><code>increase_key(S,x,k)</code> , increase value of $x$ by $k$</p>
</li>
<li><p>(Basic) (max) Heap</p>
<ol>
<li><p>Property : Complete binary tree , parent $\ge $ both children </p>
</li>
<li><p>Can be implemented in an array</p>
<p><code>fa[x]=x/2</code> , <code>left_child[x]=2x</code> , <code>right_child[x]=2x+1</code> </p>
</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">insert</span>(S,x)&#123; <span class="comment">// flow down , O(log n)</span></span><br><span class="line">    val[|S|]=x;|S|++;</span><br><span class="line">    <span class="type">int</span> p=|S|<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">while</span>(p!=root&amp;&amp;val[p]&gt;val[fa[p]])&#123;</span><br><span class="line">        <span class="built_in">swap</span>(val[p],val[fa[p]]);</span><br><span class="line">        p=fa[p];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Note : build a heap : O(n)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">extract_max</span>(S)&#123; <span class="comment">// flow up , O(log n)</span></span><br><span class="line">    <span class="type">int</span> ret_val=val[root];</span><br><span class="line">    <span class="built_in">swap</span>(val[root],val[|S|<span class="number">-1</span>]); <span class="comment">// swap the root and the last element</span></span><br><span class="line">    |S|--; <span class="comment">// delete original root</span></span><br><span class="line">    <span class="type">int</span> p=root;</span><br><span class="line">    <span class="keyword">while</span>(val[p]&lt;val[left_child[p]]||val[p]&lt;val[right_child[p]])&#123;</span><br><span class="line">        <span class="keyword">if</span>(val[left_child[p]]&gt;val[right_child[p]])&#123;</span><br><span class="line">            <span class="built_in">swap</span>(val[p],val[left_child[p]]);</span><br><span class="line">            p=left_child[p];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="built_in">swap</span>(val[p],val[right_child[p]]);</span><br><span class="line">            p=right_child[p];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret_val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">max</span>(S)&#123; <span class="comment">// O(1)</span></span><br><span class="line">    <span class="keyword">return</span> val[root];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">increase_key</span>(S,p,k)&#123; <span class="comment">// flow down , O(log n)</span></span><br><span class="line">    val[p]=val[p]+k;</span><br><span class="line">    <span class="keyword">while</span>(p!=root&amp;&amp;val[p]&gt;val[fa[p]])&#123;</span><br><span class="line">        <span class="built_in">swap</span>(val[p],val[fa[p]]);</span><br><span class="line">        p=fa[p];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Advanced : Fibonacci Heap</p>
<p>Time complexity (amortized-time)</p>
<p>| operations     | binary heap          | Fibonacci heap       |<br>| ——————— | —————————— | —————————— |<br>| <code>insert</code>       | $\mathcal O(\log n)$ | $\mathcal O(1)$      |<br>| <code>extract_max</code>  | $\mathcal O(\log n)$ | $\mathcal O(\log n)$ |<br>| <code>max</code>          | $\mathcal O(1)$      | $\mathcal O(1)$      |<br>| <code>increase_key</code> | $\mathcal O(\log n)$ | $\mathcal O(1)$      |</p>
<p>See [CLRS] for detailed Fibonacci heap .</p>
</li>
<li><p>Prim’s Algorithm with Priority Queue</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// using min priority queue</span></span><br><span class="line"><span class="keyword">for</span> (v in V\&#123;<span class="number">1</span>&#125;)&#123;</span><br><span class="line">    d[v]=+inf;</span><br><span class="line">    inS[v]=<span class="literal">false</span>;</span><br><span class="line">    PQ.<span class="built_in">insert</span>((v,+inf));</span><br><span class="line">&#125;</span><br><span class="line">d[<span class="number">1</span>]=<span class="number">0</span>;inS[<span class="number">1</span>]=<span class="literal">true</span>;</span><br><span class="line">PQ.<span class="built_in">insert</span>((<span class="number">1</span>,<span class="number">0</span>));</span><br><span class="line"><span class="comment">// index 1 , value 0</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;n;++i)&#123; <span class="comment">// n-1 rounds</span></span><br><span class="line">    <span class="type">int</span> x=PQ.<span class="built_in">extract_min</span>(); <span class="comment">// argmin value</span></span><br><span class="line">    inS[x]=<span class="literal">true</span>;</span><br><span class="line">    <span class="comment">// using edge d[x]</span></span><br><span class="line">    <span class="keyword">for</span>(y in <span class="built_in">Neighbour</span>(x))&#123;</span><br><span class="line">        <span class="keyword">if</span>(!inS[y] &amp;&amp; <span class="built_in">w</span>(x,y)&lt;d[y])&#123;</span><br><span class="line">            PQ.<span class="built_in">decrease_key</span>(y,d[y]-<span class="built_in">w</span>(x,y));</span><br><span class="line">            d[y]=<span class="built_in">w</span>(x,y);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Time Complexity : $\mathcal O(|E|\log |V|)$ .</p>
</li>
<li><p>Dijkstra’s Algorithm with Priority Queue</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// using min priority queue</span></span><br><span class="line"><span class="keyword">for</span>(v in V\&#123;s&#125;)&#123;</span><br><span class="line">    d[v]=+inf;</span><br><span class="line">    inS[v]=<span class="literal">false</span>;</span><br><span class="line">    PQ.<span class="built_in">insert</span>((v,+inf))</span><br><span class="line">&#125;</span><br><span class="line">d[s]=<span class="number">0</span>;inS[s]=<span class="literal">true</span>;</span><br><span class="line">PQ.<span class="built_in">insert</span>((v,+inf));</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;n;++i)&#123; <span class="comment">// n-1 rounds</span></span><br><span class="line">    <span class="type">int</span> x=PQ.<span class="built_in">extract_min</span>();</span><br><span class="line">    inS[x]=<span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span>(y in <span class="built_in">Neighbour</span>(x))&#123;</span><br><span class="line">        <span class="keyword">if</span>(!inS[y] &amp;&amp; d[y] &gt; d[x]+<span class="built_in">w</span>(x,y) )&#123;</span><br><span class="line">            PQ.<span class="built_in">decrease_key</span>(d[y]-d[x]-<span class="built_in">w</span>(x,y));</span><br><span class="line">            d[y]=d[x]+<span class="built_in">w</span>(x,y);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Time Complexity : $\mathcal O(|E|\log |V|)$ .</p>
</li>
</ol>
<h3 id="2-6-Huffman-Code"><a href="#2-6-Huffman-Code" class="headerlink" title="2.6 Huffman Code"></a>2.6 Huffman Code</h3><ol>
<li><p>Code Definition : a set $S$ of letters encode to $\{0,1\}^<em>$ , $r:S\to \{0,1\}^</em>$</p>
<p>Properties</p>
<ol>
<li>$r$ is one-to-one / injection</li>
<li>encoding / decoding efficiently</li>
<li>minimize average length of the code</li>
<li>[optional] robust to errors , Error Correction Code</li>
</ol>
</li>
<li><p>Prefix Code</p>
<ol>
<li><p>Def : $\forall x,y\in S$ , $r(x)$ is not a prefix of $r(y)$</p>
</li>
<li><p>Property : useful for decoding (unique decode) , can decode greedily</p>
</li>
<li><p>e.g. $S=\{a,b,c,d,e\}$</p>
<p>Prefix Code </p>
<script type="math/tex; mode=display">
r(a)=11,r(b)=01,r(c)=001,r(d)=10,r(e)=000</script><script type="math/tex; mode=display">
decode(0000011101)=decode(000\ 001\ 11\ 01)=ecab</script><p>non-Prefix Code</p>
<script type="math/tex; mode=display">
r(a)=110,r(b)=11,r(c)=01</script><script type="math/tex; mode=display">
decode(11011)=decode(110\ 11)=decode(11\ 01\ 1)</script></li>
<li><p>Prefix Code can be represented using a binary tree</p>
<p>Leafy tree : each leaf corresponds to a letter</p>
</li>
</ol>
</li>
<li><p>Minimize the length</p>
<ol>
<li><p>Average encoding length</p>
<p>Suppose each letter has a frequency $p(x)$ , $\sum_{x\in S}p(x)=1$ .</p>
<p>Average encoding length :</p>
<script type="math/tex; mode=display">
AEL(r)=\sum_{x\in S}p(x)|r(x)|</script></li>
<li><p>[Shannon] Source Coding Theorem</p>
<script type="math/tex; mode=display">
\forall r,AEL(r)\ge \sum_{x\in S}-p(x)\log p(x) =:H</script><p>See : [Thomas Cover] Information Theory</p>
</li>
</ol>
</li>
<li><p>Lemma</p>
<p>There is an optimal code ( or an optimal binary tree ) in which two lowest-frequency letter are assigned to leaves that are as deep as possible , and are siblings .</p>
<blockquote>
<p>Proof :  同层换：对 AEL 无影响；跨层： exchange argument </p>
</blockquote>
</li>
<li><p>Huffman’s Code</p>
<p>Initially , construct a set $S$ containing all letters.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(|S|&gt;<span class="number">1</span>)&#123;</span><br><span class="line">    elem x,y;</span><br><span class="line">    x=<span class="built_in">extract_max</span>(S) , y=<span class="built_in">extract_max</span>(S);</span><br><span class="line">    elem new_elem=(id,<span class="built_in">p</span>(x)+<span class="built_in">p</span>(y));</span><br><span class="line">    <span class="built_in">insert</span>(S,new_elem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-图论</tag>
        <tag>算法-图论-最小生成树</tag>
        <tag>算法-图论-最短路</tag>
        <tag>算法-数据结构-并查集</tag>
        <tag>算法-数据结构-堆/优先队列</tag>
        <tag>编码-Huffman编码</tag>
      </tags>
  </entry>
  <entry>
    <title>ZKP and MPC 1</title>
    <url>/2023/10/06/ZKP-and_MPC-1/</url>
    <content><![CDATA[<h2 id="Lec01-Basic-Definitions-and-Examples-of-ZKP"><a href="#Lec01-Basic-Definitions-and-Examples-of-ZKP" class="headerlink" title="Lec01 Basic Definitions and Examples of ZKP"></a>Lec01 Basic Definitions and Examples of ZKP</h2><h3 id="1-Basic-Notations"><a href="#1-Basic-Notations" class="headerlink" title="1 Basic Notations"></a>1 Basic Notations</h3><ol>
<li><p>Complexity</p>
<ol>
<li><p>Efficient Algorithm : poly-time algorithm .</p>
</li>
<li><p>NP class : class of problems that are easy to verify a solution (but may be hard to solve it)</p>
<p>Formal def : Suppose $L\subset \{0,1\}^*$ is a language . $L\in NP$ if there exists a poly-time algorithm $\mathcal A$ :</p>
<ul>
<li>If $x\in L$ , $\exists w\in \{0,1\}^{poly(|x|)}$ , $\mathcal A(x,w)=1$</li>
<li>If $x\notin L$ , $\forall w\in \{0,1\}^{poly(|x|)}$ , $\mathcal A(x,w)=0$</li>
</ul>
<p>$w$ that makes $\mathcal A(x,w)=1$ is called the proof of $x\in L$ , and $\mathcal A$ is called the verification algorithm .</p>
</li>
</ol>
</li>
<li><p>Proof</p>
<ol>
<li><p>Def : a static sequence of rules </p>
</li>
<li><p>E.g.1 : Prove that $f(x,y)=x^2y^3-5xy+4=0$ has a solution .</p>
<p>Proof 1 [Explicit Proof] : $f(1,1)=0$</p>
<p>Proof 2 [Implicit Proof] : $f(2,1)<0,f(2,2)>0$ .</p>
<p>( $f(2,x)$ is continuous , so $\exists x_0\in (1,2) , f(2,x_0)=0$ )</p>
</li>
</ol>
</li>
</ol>
<h3 id="2-Interactive-Proof"><a href="#2-Interactive-Proof" class="headerlink" title="2 Interactive Proof"></a>2 Interactive Proof</h3><ol>
<li><p>Interactive Proof </p>
<ol>
<li><p>A Prover and a Verifier , Prover needs to convince Verifier some proposition .</p>
<p>Usually , Prover needs to convince Verifier that some proposition is <strong>true</strong> . $(x\in L)$</p>
</li>
<li><p>Prover : with unbounded computation sources </p>
<p>Verifier : Only efficient algorithm </p>
</li>
<li><p>Complexity Class : IP : class of problems that are easy to determine with an interactive proof</p>
<ul>
<li>$NP\subsetneq IP$</li>
</ul>
</li>
<li><p>Transcript</p>
<p>Prover sends $m_1$ to Verifier , Verifier receives $m_1$ and sends $m_2$ to Prover , …</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(x,r_p)\to m_1\quad \quad &\\
V(x,r_v,m_1,\cdots,m_{i-1})\to m_i\quad& \text{for odd }i\\
P(x,r_p,m_1,\cdots,m_{i-1})\to m_i\quad& \text{for even }i\\
V(x,r_v,m_1,\cdots,m_k)\to y\in \{0,1\}&
\end{aligned}</script><ol>
<li>Def : transcript : $\tau=\{m_1,\cdots,m_k\}$</li>
<li>Def : $\braket{P,V}(x):=y$</li>
</ol>
</li>
</ol>
</li>
<li><p>IP Criteria</p>
<ol>
<li><p>Completeness : If Prover is honest , Verifier accepts the proof .</p>
<p>$\forall x\in L , \Pr\{\left<P,V\right>(x)=1\}=1$</p>
</li>
<li><p>Soundness : If Prover is dishonest , Verifier rejects the proof (with high probability) .</p>
</li>
<li><p>Zero-Knowledge : If Prover is honest , Verifier cannot know more than “knowing the proposition is true”</p>
</li>
</ol>
</li>
<li><p>Definition (somewhat formal (?) )</p>
<p>A pair of randomized algorithms $(P,V)$ is an interactive proof for $L$ </p>
<ol>
<li><p>$V$ runs in poly-time</p>
</li>
<li><p>Completeness : $\forall x\in L , \Pr\{\braket{P,V}(x)=1\}=1$</p>
</li>
<li><p>Soundness : $\forall x\notin L,\forall P^<em>,\Pr\{\braket{P^</em>,V}(x)=1\}&lt; \epsilon$</p>
</li>
<li><p>Zero-Knowledge (Optimal) </p>
<p>$\forall x\in L$ , $V$ can generate everything itself with bounded computation time without interaction</p>
<p><strong>gain sth. cannot gain by PPT $\Rightarrow$ gain knowledge</strong></p>
</li>
</ol>
</li>
</ol>
<h3 id="3-IP-Examples"><a href="#3-IP-Examples" class="headerlink" title="3 IP Examples"></a>3 IP Examples</h3><ol>
<li><p>Graph non-Isomorphism</p>
<ol>
<li><p>Description</p>
<p>Prover and Verifier know graphs $G_0,G_1$ .</p>
<p>Prover wants to convince verifier that $G_0$ is not isomorphic with $G_1$ .</p>
<script type="math/tex; mode=display">
\begin{aligned}
&G_0=(V,E_0) , G_1=(V,E_1)\\
\not\exists \pi\in S_{|V|}&\ ,\ \{(\pi(u),\pi(v)):(u,v)\in E_0\}=E_1
\end{aligned}</script></li>
<li><p>Protocol</p>
<p>Global : Prover and Verifier already know $G_0,G_1$ .</p>
<p>|                     Prover                     |                  | Verifier                                                     |<br>| :——————————————————————: | :———————: | —————————————————————————————— |<br>|                                                |                  | $b\leftarrow \{0,1\}$ , $\pi\leftarrow S_{|V|}$              |<br>|                                                | &lt;- $\tilde G$ — | $\tilde G:=\pi(G_b)$                                         |<br>| Find $\tilde b $ , $G_{\tilde b}\sim \tilde G$ | — $\tilde b$ -&gt; |                                                              |<br>|                                                |                  | $y=\begin{cases}1&amp; \text{if }b=\tilde b\\0&amp;\text{otherwise}\end{cases}$ |</p>
</li>
<li><p>Analysis</p>
<ol>
<li><p>Completeness : When $G_0\not\sim G_1$ , $G_{\tilde b}\sim \tilde G\sim G_b$ , so $b=\tilde b$ . Always accept .</p>
</li>
<li><p>Soundness : When $G_0\sim G_1$ , any prover can only guess $\tilde b$ randomly since $G_0\sim G_1\sim \tilde G$ . $\Pr\{V\text{ accept}\}\le \frac{1}{2}$ .</p>
<p>Proof : $P^*$ knows $\tilde G$ and wants to guess $b$ , we need to prove that</p>
<script type="math/tex; mode=display">
\forall P^* , \Pr\{P^*(\tilde G)=b\}\le \frac{1}{2}</script><p>Since $\forall \tilde G\sim G_0\sim G_1$ , </p>
<script type="math/tex; mode=display">
\begin{aligned}
\Pr\{\pi(G_0)=\tilde G\}&=\Pr\{\pi(G_1)=\tilde G\}\\
\Rightarrow \Pr\{\pi(G_b)=\tilde G|b=0\}&=\Pr\{\pi(G_b)=\tilde G|b=1\}\\
\Rightarrow \Pr\{b=0|\pi(G_b)=\tilde G\}&=\Pr\{b=1|\pi(G_b)=\tilde G\}\\
\end{aligned}</script><p>Therefore ,</p>
<script type="math/tex; mode=display">
\forall P^* , \Pr\{P^*(\tilde G)=b|\pi(G_b)=\tilde G\}\le\frac{1}{2}</script><p>Therefore ,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Pr\{P^*(\tilde G)=b\}&=\sum_{\tilde G}\Pr\{P^*(\tilde G)=b|\pi(G_b)=\tilde G\}\Pr\{\pi(G_b)=\tilde G\}\\
&\le \frac{1}{2}\sum_{\tilde G}\Pr\{\pi(G_b)=\tilde G\}\\
&=\frac{1}{2}
\end{aligned}</script><p>$k$ rounds : $\Pr\{V\text{ accepts}\}\le \frac{1}{2^k}$ .</p>
</li>
<li><p>Zero-Knowledge </p>
<p>Verifier itself knows $b,\pi,\tilde G$ .  Prover tells Verifier $\tilde b$ .</p>
<p>Only consider $G_0\not\sim G_1$ , then $\tilde b$ must be $b$ . Verifier itself can generate the transcript .</p>
<blockquote>
<p>注意：因为 zero-knowledge 是对 Prover 的保护，我们只需要保护诚实 Prover 的隐私，因此只要考虑命题成立的情况（即 $G_0\not\sim G_1$）。</p>
</blockquote>
</li>
</ol>
</li>
<li><p>Notes</p>
<blockquote>
<p>Given $G_0\not\sim G_1$ , Verifier can generate the proof itself </p>
<p>Due to Verifier’s randomness , Prover cannot generate the proof itself</p>
<p>Graph non-isomorphism Problem is not $NP$ , but can be solved by $IP$ . </p>
</blockquote>
</li>
</ol>
</li>
<li><p>Collision Problem</p>
<ol>
<li><p>Definition</p>
<ol>
<li>Given $h:\{0,1\}^n\to\{0,1\}^n$ , either $h$ is a permutation or $|Im(h)|\le 2^{n-1}$ .</li>
<li>Prover wants to convince Verifier that $h$ is a permutation .</li>
</ol>
</li>
<li><p>Protocol 1</p>
<p>Global : Prover and Verifier already know $h$ . $\forall x\in \{0,1\}^n$ , Verifier can get $h(x)$ in poly-time .</p>
<p>|              Prover               |                  |                          Verifier                          |<br>| :———————————————-: | :———————: | :————————————————————————————: |<br>|                                   |                  |                  $x\leftarrow \{0,1\}^n$                   |<br>|                                   |    &lt;- $y$ —     |                         $y:=h(x)$                          |<br>| Find $\tilde x$ , $h(\tilde x)=y$ | — $\tilde x$ -&gt; |                                                            |<br>|                                   |                  | $\begin{cases}1&amp;x=\tilde x\\0&amp;\text{otherwise}\end{cases}$ |</p>
</li>
<li><p>Analysis 1</p>
<ol>
<li><p>Completeness : When $h$ is a permutation , $h$ is a injection , $x=\tilde x$ .</p>
</li>
<li><p>Soundness : When $h$ is not a permutation , then $|Im(h)|\le 2^{n-1}$ .</p>
<script type="math/tex; mode=display">
\forall x , P^*\ ,\ \Pr\{P^*(h(x))=x\}=\frac{1}{|h^{-1}(h(x))|}</script><p>Therefore ,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Pr\{V\text{ accepts}\}&=\sum_{y\in Im(h)}\Pr\{y=f(x)|x\in \{0,1\}^n\}\Pr\{P^*(h(x))=x\}\\
&=\sum_{y\in Im(h)}\frac{|h^{-1}(y)|}{2^n}\frac{1}{|h^{-1}(y)|}\\
&=\frac{|Im(h)|}{2^n}\\
&\le \frac{1}{2}
\end{aligned}</script><p>$k$ rounds : $\Pr\{V \text{ accepts}\}\le\frac{1}{2^k}$ .</p>
</li>
<li><p>Zero-knowledge : </p>
<p>Verifier itself knows $x,y$ . Prover tells Verifier $\tilde x$ .</p>
<p>When $h$ is a permutation , $\tilde x=x$ , so Verifier can generate $\tilde x$ itself .</p>
</li>
</ol>
</li>
<li><p>Protocol 2</p>
<p>Global : Prover and Verifier already know $h$ . $\forall x\in \{0,1\}^n$ , Verifier can get $h(x)$ in poly-time .</p>
<p>|       Prover        |           |                        Verifier                        |<br>| :————————-: | :———-: | :——————————————————————————: |<br>|                     | &lt;- $y$ — |                $y\leftarrow \{0,1\}^n$                 |<br>| Find $x$ , $h(x)=y$ | — $x$ -&gt; |                                                        |<br>|                     |           | $\begin{cases}1&amp;h(x)=y\\0&amp;\text{otherwise}\end{cases}$ |</p>
</li>
<li><p>Analysis 2</p>
<ol>
<li><p>Completeness : When $h$ is a permutation , $h$ is a bijection , $x$ always exists and unique . Always accept .</p>
</li>
<li><p>Soundness : When $h$ is not a permutation , $|Im(h)|\le 2^{n-1}$ .</p>
<script type="math/tex; mode=display">
\Pr\{h^{-1}(y)=\varnothing|y\leftarrow \{0,1\}^n\}=1-\frac{|Im(h)|}{2^n}\ge \frac{1}{2}</script><p>When $h^{-1}(y)=\varnothing$ , any Prover cannot convince verifier , so </p>
<script type="math/tex; mode=display">
\Pr\{V\text{ accepts}\}\le 1-\Pr\{h^{-1}(y)=\varnothing|y\leftarrow \{0,1\}^n\}\le \frac{1}{2}</script><p>$k$ rounds : $\Pr\{V \text{ accepts}\}\le\frac{1}{2^k}$ .</p>
</li>
<li><p>Zero-knowledge : <strong>NOT (dishonest-verifier) zero-knowledge</strong></p>
<p>$V$ knows $h^{-1}(y)$ , which cannot be computed in poly-time by itself .</p>
<blockquote>
<p>This is still <strong>honest-verifier zero-knowledge</strong> , see homework 1</p>
</blockquote>
</li>
</ol>
</li>
<li><p>Notes</p>
<blockquote>
<p>In Protocol 1 , when $h$ is a permutation , Verifier can always generate a valid proof</p>
<p>In both Protocols , Prover must have the ability to solve $h^{-1}$ . This may lead Verifier knowing something more .</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h3 id="4-Formularize-zero-knowledge-IP"><a href="#4-Formularize-zero-knowledge-IP" class="headerlink" title="4 Formularize zero-knowledge IP"></a>4 Formularize zero-knowledge IP</h3><ol>
<li><p>Security Parameter : $\kappa$</p>
<ol>
<li>For efficiency : usually $|x|=poly(\kappa)$  , PPT should run in $poly(\kappa)$ .</li>
<li>For security : negligible function</li>
</ol>
</li>
<li><p>Negligible function : $\epsilon$</p>
<ol>
<li><p>Def : negligible function $\epsilon:\mathbb Z^<em>\to \mathbb R^</em>$</p>
<p>For all polynomial $p$ , $\exists c\in \mathbb Z^* , \forall k&gt;c,\epsilon(k)&lt;\frac{1}{p(k)}$ .</p>
</li>
<li><p>Propositions :</p>
<ol>
<li>$\epsilon , \delta$ negligible $\to$ $\epsilon+\delta , \epsilon\delta$ negligible </li>
<li>$\epsilon$ negligible $\to$ For all polynomial $p$ , $p\epsilon$ negligible</li>
</ol>
</li>
<li><p>Conventions :</p>
<ol>
<li><p>$\epsilon$ noticeable : $\exists$ polynomial $p$ , $\exists c\in \mathbb Z^*,\forall k&gt;c,\epsilon(k)\ge \frac{1}{p(k)}$ </p>
</li>
<li><p>$A$ happens with overwhelming probability : </p>
<script type="math/tex; mode=display">
\Pr\{A\}\ge 1-\epsilon \quad\quad \epsilon \text{ is negligible}</script></li>
</ol>
</li>
</ol>
<blockquote>
<p>There exists function that is neither negligible nor noticeable </p>
</blockquote>
</li>
<li><p>View of Verifier</p>
<script type="math/tex; mode=display">
View_V^P(x):=(x,r,\tau)</script></li>
<li><p>Honest-Verifier Zero-knowledge</p>
<ol>
<li><p>Perfect honest-verifier zero-knowledge</p>
<script type="math/tex; mode=display">
\exists M\in PPT\ ,\ \forall x\in L\ ,\ View_V^P(x)\equiv M(x)</script></li>
<li><p>Statistical honest-verifier zero-knowledge</p>
<p>The statistical distance between $View_V^P(x)$ and $M(x)$ is negligible</p>
<p>$\exists M\in PPT , \forall x\in L$ ,</p>
<script type="math/tex; mode=display">
SD(View_V^P(x),M(x))=\frac{1}{2}\sum_{s}\left|\Pr\{View_V^P(x)=s\}-\Pr\{M(x)=s\}\right|<\epsilon</script></li>
<li><p>Computational honest-verifier zero-knowledge</p>
<p>$\exists M\in PPT , \forall x\in L ,  \forall$ distinguisher $D\in PPT$ ,</p>
<script type="math/tex; mode=display">
\left|\Pr\{D(View_V^P(x))=1\}-\Pr\{D(M(x))=1\}\right|<\epsilon</script></li>
</ol>
</li>
<li><p>Dishonest Verifier</p>
<p>$\braket{P,V}$ achieves perfect/statistical/computational dishonest-verifier zero-knowledge if :</p>
<p>$\forall V^<em>\in PPT ,  \exists$ expected poly-time randomized algorithm $M^</em>$ , $\forall x\in L$ , $View_{V^<em>}^P(x)$ and $M^</em>(x)$ are perfectly/statistically/computationally indistinguishable .</p>
</li>
<li><p>Graph Isomorphism IP with dishonest-verifier zero-knowledge</p>
<ol>
<li><p>Definition</p>
<p>Prover and Verifier know graph $G_0,G_1$ .</p>
<p>Prover wants to convince Verifier that $G_0\sim G_1$ , i.e. $\exists \pi,\pi(G_0)=G_1$ .</p>
</li>
<li><p>Protocol</p>
<p>|                  Prover                   |                |                           Verifier                           |<br>| :———————————————————-: | :——————: | :—————————————————————————————: |<br>|           $\pi_r\leftarrow S_n$           |                |                                                              |<br>|          $\tilde G:=\pi_r(G_0)$           | —$\tilde G$-&gt; |                                                              |<br>|                                           |    &lt;-$b$—     |                    $b\leftarrow \{0,1\}$                     |<br>| find $\pi_b$ , s.t. $\tilde G=\pi_b(G_b)$ |  —$\pi_b$-&gt;   |                                                              |<br>|                                           |                | $\begin{cases}1&amp;\tilde G=\pi_b(G_b)\\0&amp;\text{otherwise}\end{cases}$ |</p>
<blockquote>
<p>Note : If Prover knows $\pi$ that $\pi(G_0)=G_1$ , then $\pi_b$ can be constructed :</p>
<script type="math/tex; mode=display">
\pi_b=\begin{cases}\pi_r&b=0\\\pi_r\circ\pi^{-1}&b=1\end{cases}</script></blockquote>
</li>
<li><p>Analysis</p>
<ol>
<li><p>Completeness : If $G_0\sim G_1$ , $\pi_b$ can be constructed as above . Always Accept .</p>
</li>
<li><p>Soundness : If $G_0\not\sim G_1$ , $\exists b^<em>\in \{0,1\},G_{b^</em>}\not\sim\tilde G$ </p>
<p>$\Pr\{V \text{ rejects}\}\ge \Pr\{V\text{ chooses }b^*\}=\frac{1}{2}$ </p>
<blockquote>
<p>Note : we cannot let Verifier just choose $b=1$ , since a malicious Prover can violate the protocol , $\pi_r$ may not be a permutation , and $\tilde G$ may not isomorphic to $G_0$ .</p>
</blockquote>
</li>
<li><p>honest-verifier zero-knowledge : </p>
<p>Verifier itself knows : $G_0,G_1,b$ , $r$ : randomness generating $b$ . Prover tells verifier $\tilde G,\pi_b$ .</p>
<p>$View_V^P=(G_0,G_1,r,b,\tilde G,\pi_b)$ .</p>
<p>$M_V$ : </p>
<pre><code> 1. sample $b\leftarrow \&#123;0,1\&#125;$
 2. choose $\pi_b\leftarrow S_n$
 3. $\tilde G:=\pi_b(G_b)$
</code></pre><p>Since Prover and Verifier are honest , $b$ is independent of $\tilde G $ , and $\pi_b$ is generated by a uniformly random $\pi_r$ hence is also uniformly random .</p>
</li>
<li><p>dishonest-verifier zero-knowledge :</p>
<p>Malicious Verifier can choose $b$ based on $\tilde G$ to gain more knowledge .</p>
<p>$M^*$ : should perform as Prover :</p>
<pre><code> 1.  guess $b^*\in \&#123;0,1\&#125;$ 
 2.  compute $\pi_r\leftarrow S_n$ , $\tilde G:=\pi_r(G_&#123;b^*&#125;)$
 3.  use Verifier to receive $b$
 4.  If $b\neq b^*$ , go back to 1.
 5.  If $b=b^*$ , then let $\pi_b=\pi_r$ , get the view 
</code></pre></li>
</ol>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>零知识证明和多方安全计算</category>
      </categories>
      <tags>
        <tag>密码学</tag>
        <tag>密码学-零知识证明</tag>
        <tag>密码学-交互式证明</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 2</title>
    <url>/2023/10/06/Algorithm-Design-2/</url>
    <content><![CDATA[<h3 id="1-3-dfs-application"><a href="#1-3-dfs-application" class="headerlink" title="1.3 dfs application"></a>1.3 dfs application</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">dfs</span><span class="params">(u)</span></span>&#123;</span><br><span class="line">    ++time;</span><br><span class="line">    discover[u]=time;</span><br><span class="line">    col[u]=grey;</span><br><span class="line">    <span class="keyword">for</span>(v in <span class="built_in">Neighbour</span>(u) )&#123;</span><br><span class="line">        <span class="keyword">if</span>(col[v]==white)&#123;</span><br><span class="line">            <span class="built_in">dfs</span>(v)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    col[u]=black;</span><br><span class="line">    ++time;</span><br><span class="line">    finish[u]=time;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>THM parenthesis</p>
<p>on dfs tree , if $u$ is one ancestor of $v$ , then $[v.d,v.f]\subset[u.d,u.f]$</p>
<p>on dfs tree , otherwise , then $[u.d,u.f]\cap [v.d,v.f]=\varnothing$</p>
</li>
<li><p>THM white-path</p>
<p>on dfs tree ,  at the time when $u$ is discovered , </p>
<p> $u$ is one ancestor of $v$  $\Leftrightarrow$ $\exists$ white path from $u$ to $v$ </p>
<p>即： $v$ 是 $u$ 的后代$\Leftrightarrow$在 $u$ 刚访问到的时候一定存在一条完全没有被访问过的路径 $u\to v$ .</p>
<ul>
<li>Proof : use parenthesis THM</li>
</ul>
<p>$\Rightarrow$ trivial</p>
<p>$\Leftarrow$  proof by contradiction </p>
<p>Consider a white path $x_1=u,x_2,\cdots,x_m=v$ , and $x_k$ is the last vertex that is a descendent of $u$ (including $u$ itself) .</p>
<p>We need to prove that $x_{k+1}$ is also a descendent of $u$ , leading to contradiction . </p>
<p>Therefore , $u.d&lt;x_k.d&lt;x_k.f&lt;u.f$ , </p>
<p>Case 1 : $x_k.d<x_{k+1}.d<x_{k+1}.f<x_k.f$ -> $x_{k+1}$ is also a descendent of $u$ .</p>
<p>Case 2 : $x_k.d&lt;x_k.f&lt;x_{k+1}.d&lt;x_{k+1}.f$ : Impossible .</p>
</li>
<li><p>Strongly Connected Components (SCC)</p>
<ol>
<li><p>View : any directed graph can be viewed as a DAG of SCC</p>
</li>
<li><p>Find SCC ?  Kosaraju’s Algorithm</p>
<ol>
<li><p>Algorithm</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">dfs1</span>(G);<span class="comment">// compute the finishing time u.f</span></span><br><span class="line">G_=<span class="built_in">reverse_edge</span>(G);</span><br><span class="line"><span class="built_in">dfs2</span>(G_); <span class="comment">// In main loop , consider vertices in decreasing order of u.f</span></span><br><span class="line">Claim : Each dfs-<span class="function">tree in <span class="title">dfs2</span><span class="params">(G_)</span> is a SCC</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Proof</p>
<p>Intuition : find and “delete” sink components in dfs2 , like a-topological order in $G_$ ( i.e. topological order in $G$ ) </p>
<ol>
<li><p>Lemma : If we start DFS at a node in a sink component , we will visit precisely all vertices in this component .</p>
<p>Trivial .</p>
</li>
<li><p>Lemma (key) : node with largest $u.f$ belongs to a start component in $G$ (i.e. a sink component in $G_$)</p>
<p>Only need to prove the following lemma .</p>
</li>
<li><p>Lemma ( for proving key Lemma ) : $C,D$ are two SCC , $D$ is reachable from $C$ , </p>
<p>Then for $v\in C$ , which is the firstly visited vertex in $C$ , then $\forall u\in D,v.f&gt;u.f$</p>
<p>Proof : </p>
<p>Case 1 : $v$ is also the firstly visited vertex in $C\cup D$ , then by white-path THM , all nodes in $C\cup D$ are descendent of $v$ , so $\forall u\in C\cup D\backslash \{v\} , v.f&gt;u.f$ .</p>
<p>Case 2 : $\exists y\in D$ , $y$ is the firstly visited vertex in $C\cup D$ , so $v$ is not a descendent of $y$ since $C$ is not reachable from $D$ .</p>
<p>Therefore , by parenthesis THM , $y.d\le u.d&lt;u.f\le y.f&lt;v.d&lt;v.f$ for all $u\in D$ .</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Chapter-2-Greedy-Algorithm"><a href="#Chapter-2-Greedy-Algorithm" class="headerlink" title="Chapter 2 : Greedy Algorithm"></a>Chapter 2 : Greedy Algorithm</h2><h3 id="2-1-Interval-Scheduling"><a href="#2-1-Interval-Scheduling" class="headerlink" title="2.1 Interval Scheduling"></a>2.1 Interval Scheduling</h3><ol>
<li><p>Description </p>
<p>Input : $n$ jobs $s_i,f_i$</p>
<p>Goal : maximize the #jobs , s.t. at most one job at a time .</p>
</li>
<li><p>Another view</p>
<p>Connect all jobs pairs $[s_i,f_i],[s_j,f_j]$ if $[s_i,f_i]\cap [s_j,f_j]\neq \varnothing$ .</p>
<p>Goal $\Leftrightarrow$ maximum independent set .</p>
<p>In general graph : NP-hard for general graph .</p>
</li>
<li><p>Algorithm</p>
<p>Repeat : Select the available jobs that finishes first .</p>
</li>
<li><p>Proof of optimality :</p>
<p>Method : [Exchange Argument] : Compare our solution and the optimal solution .</p>
<p>SOL : $i_1,i_2,\cdots ,i_m$ , OPT : $j_1,j_2,\cdots,j_k$ .</p>
<ol>
<li>Claim : If $f_{i_1}\le f_{j_1}$ , then $f_{i_r}\le f_{j_r}$ for all $r\ge 1$ .</li>
</ol>
<p>Proof : [Induction]</p>
<p>If the claim is true for $r-1$ , suppose the claim is not true for $r$ , i.e. $f_{i_r}&gt;f_{j_r}$ . Since $f_{i_{r-1}}\le f_{j_{r-1}}$ , and $s_{j_r}&gt;f_{j_{r-1}}$ , then $f_{i_{r-1}}&lt;s_{j_r}$ , so $j_r$ is also available , but has earlier finish time , contradict .</p>
<ol>
<li>If $m&lt;k$ , then $f_{i_m}&lt;f_{j_m}$ . We can use $j_{m+1},\cdots,j_k$ after  $i_m$ .</li>
</ol>
</li>
</ol>
<h3 id="2-2-Scheduling-to-minimize-lateness"><a href="#2-2-Scheduling-to-minimize-lateness" class="headerlink" title="2.2. Scheduling to minimize lateness"></a>2.2. Scheduling to minimize lateness</h3><ol>
<li><p>Description</p>
<p>Input : $n$ jobs , each job $i$ has ddl $d_i$ and length $t_i$ . Def lateness : $l_i:=\max\{0,f_i-d_i\}$</p>
<p>Goal : find a schedule of all $n$ jobs , and minimize the maximal lateness .</p>
</li>
<li><p>Equal formularization</p>
<p>Goal : find a permutation $\{p_i\}\in S_n$ , then $f_i=\sum_{j=1}^i t_{p_j}$ , $l_i:=\max\{0,f_i-d_{p_i}\}$ . Minimize $\max\{l_i\}$ .</p>
<p>$\Leftrightarrow$ $l_i:=f_i-d_{p_i}$ , Minimize $\max\{0,\max\{l_i\}\}$ .</p>
</li>
<li><p>Algorithm</p>
<p>Schedule the job in increasing order of $d_i$ .</p>
</li>
<li><p>Proof of optimality</p>
<ol>
<li><p>Def (Inversion) : Consider a schedule $A’$ , $(i,j)$ is an inversion if $i$ is scheduled before $j$ but $d_i&gt;d_j$ .</p>
</li>
<li><p>If OPT$\neq $ SOL , there must be an inversion , then there must be an adjacent inversion . Suppose $(i,i+1)$ is an inversion , then $d_{p_i}&gt;d_{p_{i+1}}$ .</p>
<p>Let $f=\sum_{j=1}^{i-1}t_{p_j}$ , so $f_i=f+t_{p_i}$ , $f_{i+1}=f+t_{p_i}+t_{p_{i+1}}$ , so $l_i=f+t_{p_i}-d_{p_i}$ , $l_{i+1}=f+t_{p_i}+t_{p_{i+1}}-d_{p_{i+1}}$ .</p>
<p>If swap $(i,i+1)$ , then $f_{i+1}’=f+t_{p_{i+1}}$ , $f_{i}’=f+t_{p_{i+1}}+t_{p_i}$ , so $l_{i+1}’=f+t_{p_{i+1}}-d_{p_{i+1}}$ , $l_i’=f+t_{p_{i+1}}+t_{p_i}-d_{p_i}$ .</p>
<p>Therefore , obviously , $l_{i+1}’<l_{i+1}$ . Since $d_{p_i}>d_{p_{i+1}}$ , then $l_i’&lt;l_i$ . Therefore , swap can lead to better solution , so OPT is not optimal . </p>
</li>
</ol>
</li>
</ol>
<h3 id="2-3-Shortest-Path-without-w-lt-0"><a href="#2-3-Shortest-Path-without-w-lt-0" class="headerlink" title="2.3. Shortest Path (without $w&lt;0$)"></a>2.3. Shortest Path (without $w&lt;0$)</h3><ol>
<li><p>Description</p>
<p>Input : weighted graph $G=(V,E)$ , start vertex $s$ .</p>
<p>Output : $d(u)$ for all $u\in V$ , where $d(u)=\min_{p\text{ is a path }s\to u}\{\sum_{e\in p}l(e)\}$  .</p>
</li>
<li><p>Algorithm [Dijkstra 1959]</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Init</span></span><br><span class="line"><span class="keyword">for</span>(u in V)&#123;</span><br><span class="line">    d[u]=+inf;</span><br><span class="line">&#125;</span><br><span class="line">d[s]=<span class="number">0</span>; S.<span class="built_in">insert</span>(s);</span><br><span class="line"><span class="comment">// Main Algorithm</span></span><br><span class="line"><span class="keyword">while</span>(S != V)&#123;</span><br><span class="line">    v=<span class="built_in">argmin</span>(d[u]+<span class="built_in">l</span>(e) |v: v in V\S , e=(u,v) , u in S );</span><br><span class="line">    d[v]=d[u]+<span class="built_in">l</span>(e);</span><br><span class="line">    S.<span class="built_in">insert</span>(v);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Proof</p>
<p>Prove by induction on $S$ . $\forall v\in S , d(v)=\min dist(s,v)$ ,</p>
<p>Suppose we grow $S$ by adding $v$ , suppose the proposition does not hold . Then $d(u)+l_e$ is not the shortest distance to $v$ . Let $p$ be the shortest path from $s$ to $v$ .</p>
<p>Let $w$ be the last vertex that is in $S$ , then by induction , all vertices on $p$ from $s$ to $w$ are all in $S$ . Let $p’=path(s,u)+e$ .</p>
<p>$p’: s\to w\to u\to v$ , $p:s\to w\to x\to v$ . ($x\notin S$) . Since $d(u)+l_e$ is minimal , $d(u)+l_e\le d(w)+l_{w,x}$ , but $dist(p)=d(w)+l_{w,x}+dist(x,v)$ , where $dist(x,v)\ge 0$ , and $dist(p)&lt;dist(p’)=d(u)+l_e$ , so $d(w)+l_{w,x}\le dist(p)&lt;d(u)+l_e$ , contradict .</p>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-搜索</tag>
        <tag>算法-强连通分量(SCC)</tag>
        <tag>算法-贪心</tag>
      </tags>
  </entry>
  <entry>
    <title>Algorithm Design 1</title>
    <url>/2023/10/05/Algorithm-Design-1/</url>
    <content><![CDATA[<h2 id="Chapter-0-Logistics"><a href="#Chapter-0-Logistics" class="headerlink" title="Chapter 0 Logistics"></a>Chapter 0 Logistics</h2><p><strong>Content</strong> : discrete(combinatorial) algorithm  ,  Theoretical </p>
<ul>
<li>[minority] Complexity , NP-Completeness</li>
<li>Basic Graph Algorithm , DFS / BFS</li>
<li>Greedy</li>
<li>Dynamic Programming</li>
<li>Divide and Conquer</li>
<li>NP-completeness Theory</li>
<li>Approximation Algorithm</li>
<li>Randomized Algorithm (Probability Analysis)</li>
<li><ul>
<li>Computational Geometry</li>
</ul>
</li>
<li><ul>
<li>Streaming Algorithm (online)</li>
</ul>
</li>
</ul>
<p><strong>Textbook</strong> : [Kleinberg&amp;Tardos] Algorithm Design </p>
<p><strong>Reference Book</strong> : [CLRS] Intro to Algorithm</p>
<h2 id="Chapter-1"><a href="#Chapter-1" class="headerlink" title="Chapter 1"></a>Chapter 1</h2><h3 id="1-1-Stable-matching"><a href="#1-1-Stable-matching" class="headerlink" title="1.1 Stable matching"></a>1.1 Stable matching</h3><ol>
<li><p>Def</p>
<ul>
<li><p>Input : $boys=\{B_1,\cdots,B_n\} , girls=\{G_1,\cdots,G_n\}$</p>
<p>Preference List : $BP_i$ : a permutation of $girls$ , $GP_i$ : a permutation of $boys$</p>
</li>
<li><p>output : a stable matching </p>
</li>
<li><p>stable matching : no unstable pairs</p>
</li>
<li><p>unstable pair : $(B_i,G_j)$ s.t.  $M(B_i)$ after $G_j$ in $BP_i$ and $M(G_j)$ after $B_i$ in $GP_j$</p>
</li>
</ul>
</li>
<li><p>Efficient Algorithm : Gale &amp; Shapley Algorithm ( propose-reject )</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>( exists sb. single )&#123;</span><br><span class="line">    A &lt;- an arbitrary single boy</span><br><span class="line">    X &lt;- <span class="function">first girl A has <span class="keyword">not</span> proposed yet</span></span><br><span class="line"><span class="function">    <span class="title">if</span> <span class="params">( X is single )</span></span></span><br><span class="line"><span class="function">        A-X engage</span></span><br><span class="line"><span class="function">    <span class="keyword">else</span></span></span><br><span class="line"><span class="function">        <span class="title">if</span> <span class="params">( A is better than M(X) )</span></span></span><br><span class="line"><span class="function">            A-X engage</span></span><br><span class="line"><span class="function">        <span class="keyword">else</span></span></span><br><span class="line"><span class="function">            X reject A</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Analysis</p>
<ol>
<li><p>Proof of Termination </p>
<ol>
<li>For girl : once engaged , engaged forever </li>
<li>For one boy : If used all preference list : then all girls must be engaged</li>
</ol>
</li>
<li><p>Proof of Correctness</p>
<p>Prove by Contradiction</p>
<p>$(B_i,G_j)$ : an unstable pair</p>
<p>$B_i$ preference list : $\cdots G_j \cdots M(B_i)$</p>
<p>$G_j$ preference list : $\cdots B_i \cdots Z=M(G_j)$</p>
<p>$\therefore$ $G_j$ rejected $B_i$ , but $G_j$ should not reject $B_i$ compared with $Z$</p>
</li>
<li><p>Running Time : $\mathcal O(n^2)$ ( All boys used up their preference list)</p>
</li>
</ol>
</li>
<li><p>Random ver.</p>
<ol>
<li><p>def : $BP_i$ , $GP_i$ are all random permutations (uniformly distributed) </p>
</li>
<li><p>How to get a uniformly distributed random permutation ?</p>
<p>draw-likely process</p>
</li>
<li><p>THM : $\mathbb E[T]\le n\cdot H_n$  ($\mathbb E[T]=\mathcal O(n\log n)$)</p>
</li>
<li><p>Proof </p>
<ol>
<li><p>key observation : </p>
<p>G-S’ : each time a boy propose to a random girl not proposed yet</p>
<p><strong>This is equivalent as generate a uniformly distributed random permutation</strong></p>
<p>G-S’’ : each time a boy propose to a random girl (can be proposed yet)</p>
<p>$\therefore $ $T(G-S)=T(G-S’)\le T(G-S’’)$</p>
</li>
<li><p>Coupon Collector Problem ( Bins-Balls ) </p>
<p>$n$ bins , each time throw a ball to a random bin . </p>
<p>Q : $\mathbb E[\text{balls}]$ s.t. every bin is nonempty .</p>
<p>A : $\mathbb E[\text{balls}]=n\cdot H_n$</p>
<p>Construct Sequence $a_i\in \{0,1\}$ , $a_i=1\Leftrightarrow$ a ball falls in an empty bin</p>
<p>Exactly $n$ number of $1$s . -&gt; $n$ segments like $0\cdots 01$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb E[T]&=\mathbb E\left[\sum_{i=1}^n \text{len of i-th segment}\right]\\
&=\sum_{i=1}^n\mathbb E\left[ \text{len of i-th segment}\right]\\
&=\sum_{i=1}^n \frac{1}{\Pr\{\text{in i-th seg , choosed empty bin}\}}\\
&=\sum_{i=1}^n \frac{n}{n-i+1}\\
&=n\cdot H_n
\end{aligned}</script></li>
<li><p>Consider boy -&gt; ball , girl -&gt; bin</p>
<p>G-S’’ -&gt; Bins-Balls Problem</p>
</li>
<li><ul>
<li><p>Concentration inequality for Coupon Collection Running Time</p>
<p>Same as Chernoff Bound</p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="1-2-BFS-amp-DFS"><a href="#1-2-BFS-amp-DFS" class="headerlink" title="1.2 BFS &amp; DFS"></a>1.2 BFS &amp; DFS</h3><ol>
<li><p>BFS</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// bfs</span></span><br><span class="line">q.<span class="built_in">clear</span>();</span><br><span class="line"><span class="keyword">for</span>(u in Vertices)&#123;</span><br><span class="line">    dep[u]=inf;</span><br><span class="line">    prev[u]=<span class="literal">NULL</span>;</span><br><span class="line">    col[u]=white;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dep[s]=<span class="number">0</span>;</span><br><span class="line">prev[s]=<span class="literal">NULL</span>;</span><br><span class="line">col[s]=grey;</span><br><span class="line">q.<span class="built_in">push</span>(s);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(!q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">    u=q.<span class="built_in">front</span>();q.<span class="built_in">pop</span>();</span><br><span class="line">    <span class="keyword">for</span>(v in <span class="built_in">Neighbour</span>(u) )&#123;</span><br><span class="line">        <span class="keyword">if</span>(col[v]==white)&#123;</span><br><span class="line">            dep[v]=dep[u]+<span class="number">1</span>;</span><br><span class="line">            prev[v]=u;</span><br><span class="line">            col[v]=grey;</span><br><span class="line">            q.<span class="built_in">push</span>(v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    col[v]=black;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>BFS Tree Property : no edges with depth difference $\ge 2$ .</p>
</li>
<li><p>DFS</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">function <span class="title">dfs</span><span class="params">(u)</span></span>&#123;</span><br><span class="line">    ++time;</span><br><span class="line">    discover[u]=time;</span><br><span class="line">    col[u]=grey;</span><br><span class="line">    <span class="keyword">for</span>(v in <span class="built_in">Neighbour</span>(u) )&#123;</span><br><span class="line">        <span class="keyword">if</span>(col[v]==white)&#123;</span><br><span class="line">            <span class="built_in">dfs</span>(v)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    col[u]=black;</span><br><span class="line">    ++time;</span><br><span class="line">    finish[u]=time;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>DFS Tree Properties</p>
<p>Time stamp intervals</p>
<ol>
<li>non-crossing : only non-intersect / totally include</li>
<li>Tree Structure $\Leftrightarrow$ Time stamp intervals Structure </li>
</ol>
</li>
<li><p>Connectivity</p>
<ol>
<li><p>undirected graph : connected component</p>
</li>
<li><p>directed graph : strongly connected component (SCC)</p>
<p>every vertex can reach other vertex</p>
</li>
<li><p>directed acyclic graph (DAG)</p>
<p>i.e. no directed cycle</p>
<p>i.e. no SCC has $\ge 2$ vertices</p>
<ul>
<li>DAG has a topological order</li>
</ul>
</li>
<li><p>A useful view of directed graph : a DAG of SCC</p>
<p>a.k.a. 缩点</p>
</li>
<li><p>DAG has a topological order</p>
<p>get topological order : use bfs/dfs starting from $InDeg=0$ </p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>课程笔记</category>
        <category>算法设计</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法-匹配</tag>
        <tag>算法-搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/10/05/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>测试文档</category>
      </categories>
      <tags>
        <tag>测试文档</tag>
      </tags>
  </entry>
</search>
